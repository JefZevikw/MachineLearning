{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader.data as web\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Zeev\n",
    "## 033363870\n",
    "## Assaf\n",
    "## 204249197\n",
    "### setting and Downloading the tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = pd.read_csv('/Users/bebik/Documents/hotels_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/17/2015 0:00\n",
      "<class 'datetime.timedelta'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "date_time_str = '2018/8/04'\n",
    "print(hotels['Snapshot Date'][0])\n",
    "#for index, row in hotels.iterrows():\n",
    "tabletime = datetime.datetime.strptime( hotels['Snapshot Date'][0] , '%m/%d/%Y %H:%M')\n",
    "now = datetime.datetime.strptime( date_time_str , '%Y/%m/%d')\n",
    "newdt = tabletime - now\n",
    "print (type(newdt))\n",
    "print ( np.timedelta64(1, 'D').astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hotels['Days'] #always 5\n",
    "del hotels['Snapshot ID']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars  \n",
       "count  187848.000000  \n",
       "mean        3.905823  \n",
       "std         0.846496  \n",
       "min         0.000000  \n",
       "25%         3.000000  \n",
       "50%         4.000000  \n",
       "75%         4.000000  \n",
       "max         5.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars  \n",
       "count  187848.000000  \n",
       "mean        3.905823  \n",
       "std         0.846496  \n",
       "min         0.000000  \n",
       "25%         3.000000  \n",
       "50%         4.000000  \n",
       "75%         4.000000  \n",
       "max         5.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels['SnapDate']= pd.to_datetime(hotels['Snapshot Date']) \n",
    "hotels['CheckingDate']= pd.to_datetime(hotels['Checkin Date']) \n",
    "hotels['weekday']= hotels['CheckingDate'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels['DayDiff'] = hotels['CheckingDate']-hotels['SnapDate']\n",
    "hotels['Discount'] = hotels['Original Price']-hotels['Discount Price']\n",
    "hotels['DiscountPerc'] = hotels['Discount']/hotels['Original Price']*100\n",
    "#hotels['delta_days']= (hotels['CheckingDate']- now).astype(int)/8.636207/10**13\n",
    "hotels['delta_days']=(pd.to_datetime(hotels['Checkin Date']) - pd.to_datetime(hotels['Snapshot Date'])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DayDiff</th>\n",
       "      <th>Discount</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "      <td>2.917763</td>\n",
       "      <td>17 days 11:10:35.185894</td>\n",
       "      <td>152.569860</td>\n",
       "      <td>8.877715</td>\n",
       "      <td>17.465685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "      <td>1.840536</td>\n",
       "      <td>10 days 00:57:55.438813</td>\n",
       "      <td>143.316985</td>\n",
       "      <td>6.030248</td>\n",
       "      <td>10.040225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9 days 00:00:00</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>4.844961</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18 days 00:00:00</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>7.056229</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>26 days 00:00:00</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>10.933941</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>34 days 00:00:00</td>\n",
       "      <td>3760.000000</td>\n",
       "      <td>68.425842</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars        weekday                  DayDiff       Discount  \\\n",
       "count  187848.000000  187848.000000                   187848  187848.000000   \n",
       "mean        3.905823       2.917763  17 days 11:10:35.185894     152.569860   \n",
       "std         0.846496       1.840536  10 days 00:57:55.438813     143.316985   \n",
       "min         0.000000       0.000000          1 days 00:00:00      15.000000   \n",
       "25%         3.000000       2.000000          9 days 00:00:00      70.000000   \n",
       "50%         4.000000       3.000000         18 days 00:00:00     103.000000   \n",
       "75%         4.000000       4.000000         26 days 00:00:00     180.000000   \n",
       "max         5.000000       6.000000         34 days 00:00:00    3760.000000   \n",
       "\n",
       "        DiscountPerc     delta_days  \n",
       "count  187848.000000  187848.000000  \n",
       "mean        8.877715      17.465685  \n",
       "std         6.030248      10.040225  \n",
       "min         0.581395       1.000000  \n",
       "25%         4.844961       9.000000  \n",
       "50%         7.056229      18.000000  \n",
       "75%        10.933941      26.000000  \n",
       "max        68.425842      34.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "554 554\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Snapshot Date</th>\n",
       "      <th>Checkin Date</th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Name</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>SnapDate</th>\n",
       "      <th>CheckingDate</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DayDiff</th>\n",
       "      <th>Discount</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/17/2015 0:00</td>\n",
       "      <td>8/12/2015 0:00</td>\n",
       "      <td>1178</td>\n",
       "      <td>1040</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Best Western Plus Seaport Inn Downtown</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>2</td>\n",
       "      <td>26 days</td>\n",
       "      <td>138</td>\n",
       "      <td>11.714771</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/17/2015 0:00</td>\n",
       "      <td>8/19/2015 0:00</td>\n",
       "      <td>1113</td>\n",
       "      <td>982</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Best Western Plus Seaport Inn Downtown</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>2015-08-19</td>\n",
       "      <td>2</td>\n",
       "      <td>33 days</td>\n",
       "      <td>131</td>\n",
       "      <td>11.769991</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/17/2015 0:00</td>\n",
       "      <td>8/13/2015 0:00</td>\n",
       "      <td>4370</td>\n",
       "      <td>4240</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The Peninsula New York</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>2015-08-13</td>\n",
       "      <td>3</td>\n",
       "      <td>27 days</td>\n",
       "      <td>130</td>\n",
       "      <td>2.974828</td>\n",
       "      <td>27</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/17/2015 0:00</td>\n",
       "      <td>7/26/2015 0:00</td>\n",
       "      <td>1739</td>\n",
       "      <td>1667</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>Eventi Hotel a Kimpton Hotel</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>2015-07-26</td>\n",
       "      <td>6</td>\n",
       "      <td>9 days</td>\n",
       "      <td>72</td>\n",
       "      <td>4.140311</td>\n",
       "      <td>9</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/17/2015 0:00</td>\n",
       "      <td>8/12/2015 0:00</td>\n",
       "      <td>1739</td>\n",
       "      <td>1672</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Eventi Hotel a Kimpton Hotel</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>2</td>\n",
       "      <td>26 days</td>\n",
       "      <td>67</td>\n",
       "      <td>3.852789</td>\n",
       "      <td>26</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Snapshot Date    Checkin Date  Original Price  Discount Price  \\\n",
       "0  7/17/2015 0:00  8/12/2015 0:00            1178            1040   \n",
       "1  7/17/2015 0:00  8/19/2015 0:00            1113             982   \n",
       "2  7/17/2015 0:00  8/13/2015 0:00            4370            4240   \n",
       "3  7/17/2015 0:00  7/26/2015 0:00            1739            1667   \n",
       "4  7/17/2015 0:00  8/12/2015 0:00            1739            1672   \n",
       "\n",
       "   Discount Code  Available Rooms                              Hotel Name  \\\n",
       "0              1                6  Best Western Plus Seaport Inn Downtown   \n",
       "1              1                8  Best Western Plus Seaport Inn Downtown   \n",
       "2              1                3                  The Peninsula New York   \n",
       "3              1               18            Eventi Hotel a Kimpton Hotel   \n",
       "4              1                3            Eventi Hotel a Kimpton Hotel   \n",
       "\n",
       "   Hotel Stars   SnapDate CheckingDate  weekday DayDiff  Discount  \\\n",
       "0            3 2015-07-17   2015-08-12        2 26 days       138   \n",
       "1            3 2015-07-17   2015-08-19        2 33 days       131   \n",
       "2            5 2015-07-17   2015-08-13        3 27 days       130   \n",
       "3            4 2015-07-17   2015-07-26        6  9 days        72   \n",
       "4            4 2015-07-17   2015-08-12        2 26 days        67   \n",
       "\n",
       "   DiscountPerc  delta_days  hotel  \n",
       "0     11.714771          26     35  \n",
       "1     11.769991          33     35  \n",
       "2      2.974828          27    482  \n",
       "3      4.140311           9    127  \n",
       "4      3.852789          26    127  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy \n",
    "vals = numpy.unique(hotels['Hotel Name'].values)\n",
    "checkins = numpy.unique(hotels['Checkin Date'].values)\n",
    "hotels_num = numpy.unique(hotels['Hotel Name'].values)\n",
    "print (type(hotels_num[0]))\n",
    "#print(checkins,len(checkins))             \n",
    "#print(hotels_num,len(hotels_num))             \n",
    "numbers = []\n",
    "for i in range(len(vals)) :\n",
    "    numbers.append(i)\n",
    "print (len(vals),len(numbers) )\n",
    "hotels['hotel']=hotels['Hotel Name']\n",
    "hotels['hotel'].replace(to_replace=vals, value=numbers,inplace = True)\n",
    "hotels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Original Price  Discount Price  Available Rooms        weekday  \\\n",
      "count   187848.000000   187848.000000    187848.000000  187848.000000   \n",
      "mean      1825.742143     1673.172283        14.588561       2.917763   \n",
      "std       1042.355712      983.618013        41.575482       1.840536   \n",
      "min        289.000000      260.000000        -1.000000       0.000000   \n",
      "25%       1160.000000     1035.000000        -1.000000       2.000000   \n",
      "50%       1599.000000     1475.000000        -1.000000       3.000000   \n",
      "75%       2160.000000     1963.250000         8.000000       4.000000   \n",
      "max      29975.000000    28675.000000       431.000000       6.000000   \n",
      "\n",
      "        DiscountPerc     delta_days          hotel     days_delta  \n",
      "count  187848.000000  187848.000000  187848.000000  187848.000000  \n",
      "mean        8.877715      17.465685     289.105655      17.465685  \n",
      "std         6.030248      10.040225     146.185570      10.040225  \n",
      "min         0.581395       1.000000       0.000000       1.000000  \n",
      "25%         4.844961       9.000000     176.000000       9.000000  \n",
      "50%         7.056229      18.000000     285.000000      18.000000  \n",
      "75%        10.933941      26.000000     397.000000      26.000000  \n",
      "max        68.425842      34.000000     553.000000      34.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X, y = make_classification(random_state=0)\n",
    "X = hotels.copy()\n",
    "y = X['Discount Code']\n",
    "del X['Hotel Stars']\n",
    "del X['Discount Code']\n",
    "\n",
    "#del X['DiscountPerc']\n",
    "\n",
    "\n",
    "\n",
    "X['days_delta'] = X['DayDiff'].astype('timedelta64[D]')\n",
    "del X['DayDiff']\n",
    "del X['Snapshot Date']\n",
    "del X['Checkin Date']\n",
    "del X['Hotel Name']\n",
    "del X['SnapDate']\n",
    "del X['CheckingDate']\n",
    "#del X['Discount Price']\n",
    "del X['Discount']\n",
    "\n",
    "print(X.describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "#X, y = load_iris(return_X_y=True)\n",
    "#clf2 = tree.DecisionTreeClassifier()\n",
    "#clf2 = clf2.fit(X_train, y_train)\n",
    "#clf2 = clf2.predict(X_test)\n",
    "#tree.plot_tree(clf2) \n",
    "#print(clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.plot_tree(clf2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test=X_test.append([2000,2,3,4,5])\n",
    "#print (len(X_test))\n",
    "#print (clf.predict(X_test[:1]))\n",
    "#print (X_test[:1])\n",
    "\n",
    "#array([1, 0])\n",
    "#clf.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "pic_size = 256\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self,size_list):\n",
    "        super(Simple_MLP,self).__init__()\n",
    "        layers=[]\n",
    "        self.size_list = size_list\n",
    "        for i in range(len(size_list) -2):\n",
    "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "            op = random.randint(0,2) \n",
    "            \n",
    "            layers.append(torch.nn.ReLU())\n",
    "        #layers.append(nn.Linear(size_list[-3],size_list[-2]))\n",
    "        #layers.append(nn.Softmax(dim=1))\n",
    "        layers.append(nn.Linear(size_list[-2],size_list[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelsDataset(data.Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index].float()\n",
    "        Y = self.Y[index].float()\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    #device = get_device()\n",
    "    #return torch.from_numpy(df.values)\n",
    "    return torch.FloatTensor(df.values)\n",
    "def create_set(num,df,target_col):\n",
    "    print(' target column is ',target_col)\n",
    "    df_train_sample = df.copy()\n",
    "    if (num != 0):\n",
    "        df_train_sample = df_train_sample.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[target_col] )\n",
    "    ten_train_target = torch.FloatTensor(target_sample[target_col].values)\n",
    "    #print(\"train_target \",ten_train_target)\n",
    "    del df_train_sample[target_col]\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "    #print (\"target is \" , ten_train_target)\n",
    "    #print(ten_train_target)\n",
    "    train_dataset = HotelsDataset(ten_train_data,ten_train_target)\n",
    "\n",
    "\n",
    "    train_loader_args = dict(shuffle=True,batch_size=pic_size)\n",
    "    train_loader = data.DataLoader(train_dataset,**train_loader_args)\n",
    "    return train_loader\n",
    "\n",
    "def create_set2(num,df,target_col):\n",
    "    df_train_sample = df.copy()\n",
    "    if (num != 0):\n",
    "        df_train_sample = df_train_sample.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[target_col] )\n",
    "    ten_train_target = torch.FloatTensor(target_sample[target_col].values)\n",
    "    \n",
    "    del df_train_sample[target_col]\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "    return ten_train_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model =  [7, 65, 47, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=65, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=65, out_features=47, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=47, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([7, 65, 40, 1], [7, 98, 47, 1], [7, 65, 47, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "#criterion = nn.L1Loss()\n",
    "first_round = 7\n",
    "#first_round = 5\n",
    "\n",
    "\n",
    "end_round  = 1\n",
    "device = torch.device( \"cpu\")\n",
    "def create_ar(lens):\n",
    "    \n",
    "    r1 = 160\n",
    "    r2 = 160\n",
    "    ar = [first_round]\n",
    "    for i in range(random.randint(1,lens)):\n",
    "        r1 = random.randint(4,140)\n",
    "        ar.append( r1  )\n",
    "        r2 = random.randint(4,140)\n",
    "        ar.append(  r2 )\n",
    "    ar.append(end_round)\n",
    "    return ar\n",
    "\n",
    "\n",
    "#create next generations\n",
    "def next_gen(best_ar):\n",
    "    \n",
    "    ar_left = []\n",
    "    ar_right = []\n",
    "    for index,item in enumerate(best_ar):\n",
    "        r1 = random.randint(4,140)\n",
    "        if (index is 0 or index is len(best_ar)-1):\n",
    "            r1=item\n",
    "        if (index < len(best_ar)/2 ):\n",
    "            ar_left.append(item)\n",
    "            ar_right.append(r1)\n",
    "        else:\n",
    "            ar_left.append(r1)\n",
    "            ar_right.append(item)\n",
    "    \n",
    "    return ar_left,ar_right,best_ar\n",
    "\n",
    "\n",
    "\n",
    "def create_model(ar): \n",
    "    print(\"create_model = \",ar)\n",
    "    model = Simple_MLP(ar)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    print(model)\n",
    "    return model,optimizer ,ar\n",
    "ar2=create_ar(10)\n",
    "model,optimizer ,ar2= create_model(ar2)\n",
    "next_gen(ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "def create_data_loader(df,target_col):\n",
    "    target = pd.DataFrame(df[ target_col] )\n",
    "    #print(target)\n",
    "    ten_target = torch.FloatTensor(target[target_col].values)\n",
    "    df = df.drop(target_col ,axis=1 )\n",
    "    ten_data = df_to_tensor(df)\n",
    "    #print (ten_data)\n",
    "\n",
    "    _dataset = HotelsDataset(ten_data,ten_target)\n",
    "\n",
    "\n",
    "\n",
    "#    test_loader_args = dict(shuffle=True,batch_size=pic_size,num_workers=0,pin_memory=True) if cuda\\\n",
    "#    else \n",
    "    test_loader_args = dict(shuffle=True,batch_size=pic_size)\n",
    "    loader = data.DataLoader(_dataset,**test_loader_args)\n",
    "    return ten_target,loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = hotels.copy()\n",
    "del df_train['DayDiff']\n",
    "del df_train['Snapshot Date']\n",
    "#hotels_df['SnapshotUnixDate']  = pd.to_datetime(hotels_df['Snapshot Date'])\n",
    "\n",
    "del df_train['Checkin Date']\n",
    "del df_train['Hotel Name']\n",
    "del df_train['SnapDate']\n",
    "del df_train['CheckingDate']\n",
    "\n",
    "\n",
    "target_columns = 'DiscountPerc'\n",
    "#target_columns = 'Discount Price'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_validate = train_test_split(df_train, test_size=0.2)\n",
    "df_train2 = df_train.copy()\n",
    "#print(df_validate.describe())\n",
    "ten_target,loader = create_data_loader(df_train2,target_columns)\n",
    "#df_validate =  df_train.sample(n = 10000)\n",
    "\n",
    "\n",
    "df_validate = df_validate.drop(\"Discount Price\" ,axis=1 )\n",
    "#df_validate = df_validate.drop(\"DiscountPerc\" ,axis=1 )\n",
    "df_validate = df_validate.drop(\"Discount\" ,axis=1 )\n",
    "validate_target, validate_loader = create_data_loader(df_validate,target_columns)\n",
    "#df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary algorithm with Deep networks\n",
    "\n",
    "1. Starting with random network with Relu activation functions.\n",
    "2. each generation take the \n",
    "    a. original\n",
    "    b. left original + random   right\n",
    "    c. left random   + original right\n",
    "    d. random (with random size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(model,train_loader,criterion,optimizer):\n",
    "    #print(1)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    running_loss=0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        #print (target)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs = model(data)\n",
    "        \n",
    "        target = target.unsqueeze(1)\n",
    "        target = target.float() \n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs,target)\n",
    "        running_loss += loss.item()\n",
    "        #print(\"Train Loss item: \", loss.item() )\n",
    "        #print (outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    running_loss /= len(train_loader)\n",
    "    print(\"Train Loss: \", running_loss, ' Time: ', end_time-start_time)\n",
    "    return running_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model,test_loader,criterion):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        #print(2)\n",
    "        total_acc =0\n",
    "        index =0\n",
    "        acc =0\n",
    "        acc1 =0\n",
    "        for batch_idx, (data,target) in enumerate(test_loader):\n",
    "            #print(3)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            #print (\"outputs 1= \", outputs[1] ) \n",
    "            #print (\"outputs  \", outputs ) \n",
    "            #print (\"target x = \", target)\n",
    "            # Predicted discount / Discount\n",
    "            acc1 = (outputs.data/target.data)\n",
    "            #print (\"acc is x= \",acc)\n",
    "            loss = criterion(outputs,target).detach()\n",
    "            running_loss += loss.item()\n",
    "            total_acc +=torch.mean(acc1[0]) \n",
    "            index = index + 1\n",
    "        acc = total_acc/index\n",
    "        running_loss /= len(test_loader)\n",
    "        \n",
    "        print (\"mean acc = \",acc )\n",
    "            \n",
    "        return running_loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88948</th>\n",
       "      <td>1022</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6.262231</td>\n",
       "      <td>28</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1606</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5.479452</td>\n",
       "      <td>34</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103134</th>\n",
       "      <td>670</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6.268657</td>\n",
       "      <td>15</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139461</th>\n",
       "      <td>809</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.438813</td>\n",
       "      <td>23</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84564</th>\n",
       "      <td>2479</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5.808794</td>\n",
       "      <td>4</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32048</th>\n",
       "      <td>3045</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.926108</td>\n",
       "      <td>15</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103240</th>\n",
       "      <td>3938</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.713560</td>\n",
       "      <td>18</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47280</th>\n",
       "      <td>3763</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12.968376</td>\n",
       "      <td>21</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16518</th>\n",
       "      <td>3275</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>18.137405</td>\n",
       "      <td>24</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186174</th>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>12.137203</td>\n",
       "      <td>21</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Original Price  Discount Code  Available Rooms  Hotel Stars  weekday  \\\n",
       "88948             1022              2               -1            3        2   \n",
       "244               1606              1                1            4        3   \n",
       "103134             670              2               -1            2        5   \n",
       "139461             809              1               15            4        5   \n",
       "84564             2479              2                2            5        3   \n",
       "...                ...            ...              ...          ...      ...   \n",
       "32048             3045              3                1            4        1   \n",
       "103240            3938              3                8            5        1   \n",
       "47280             3763              2               -1            5        5   \n",
       "16518             3275              3               -1            5        4   \n",
       "186174            1895              2               -1            4        4   \n",
       "\n",
       "        DiscountPerc  delta_days  hotel  \n",
       "88948       6.262231          28    175  \n",
       "244         5.479452          34    299  \n",
       "103134      6.268657          15    351  \n",
       "139461      5.438813          23    106  \n",
       "84564       5.808794           4    387  \n",
       "...              ...         ...    ...  \n",
       "32048       4.926108          15    471  \n",
       "103240      5.713560          18    485  \n",
       "47280      12.968376          21    341  \n",
       "16518      18.137405          24    472  \n",
       "186174     12.137203          21     47  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train_bkup = df_train.copy()\n",
    "\n",
    "#df_train = df_train.drop(\"Available Rooms\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Discount Code\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Hotel Stars\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"weekday\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Discount\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"delta_days\" ,axis=1 )\n",
    "\n",
    "\n",
    "df_train = df_train.drop(\"Discount Price\" ,axis=1 )\n",
    "\n",
    "#df_train = df_train.drop(\"DiscountPerc\" ,axis=1 )\n",
    "\n",
    "\n",
    "\n",
    "df_train = df_train.drop(\"Discount\" ,axis=1 )\n",
    "\n",
    "\n",
    "df_train.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.453892</td>\n",
       "      <td>2.432019</td>\n",
       "      <td>14.533405</td>\n",
       "      <td>3.906879</td>\n",
       "      <td>2.918138</td>\n",
       "      <td>8.875838</td>\n",
       "      <td>17.471819</td>\n",
       "      <td>288.911364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.298465</td>\n",
       "      <td>1.005051</td>\n",
       "      <td>41.415479</td>\n",
       "      <td>0.847168</td>\n",
       "      <td>1.842456</td>\n",
       "      <td>6.023344</td>\n",
       "      <td>10.038521</td>\n",
       "      <td>146.130970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.840183</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1598.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.053789</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>285.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2158.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.942249</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>56.410256</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>553.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Code  Available Rooms    Hotel Stars  \\\n",
       "count   150278.000000  150278.000000    150278.000000  150278.000000   \n",
       "mean      1825.453892       2.432019        14.533405       3.906879   \n",
       "std       1042.298465       1.005051        41.415479       0.847168   \n",
       "min        289.000000       1.000000        -1.000000       0.000000   \n",
       "25%       1160.000000       2.000000        -1.000000       3.000000   \n",
       "50%       1598.000000       2.000000        -1.000000       4.000000   \n",
       "75%       2158.000000       3.000000         8.000000       4.000000   \n",
       "max      29975.000000       4.000000       431.000000       5.000000   \n",
       "\n",
       "             weekday   DiscountPerc     delta_days          hotel  \n",
       "count  150278.000000  150278.000000  150278.000000  150278.000000  \n",
       "mean        2.918138       8.875838      17.471819     288.911364  \n",
       "std         1.842456       6.023344      10.038521     146.130970  \n",
       "min         0.000000       0.581395       1.000000       0.000000  \n",
       "25%         1.000000       4.840183       9.000000     176.000000  \n",
       "50%         3.000000       7.053789      18.000000     285.000000  \n",
       "75%         4.000000      10.942249      26.000000     397.000000  \n",
       "max         6.000000      56.410256      34.000000     553.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  119.61901664733887  Time:  0.04080486297607422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([194])) that is different to the input size (torch.Size([194, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0556)\n",
      "train loss 119.61901664733887\n",
      "test loss 109.71541190633968\n",
      "================================================== gen= 0 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.24741744995117  Time:  0.028885841369628906\n",
      "mean acc =  tensor(0.3611)\n",
      "train loss 116.24741744995117\n",
      "test loss 82.48407864732808\n",
      "================================================== gen= 0 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.24567222595215  Time:  0.02941417694091797\n",
      "mean acc =  tensor(1.4543)\n",
      "train loss 63.24567222595215\n",
      "test loss 62.12513200928565\n",
      "================================================== gen= 0 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  70.6006498336792  Time:  0.0302889347076416\n",
      "mean acc =  tensor(0.8681)\n",
      "train loss 70.6006498336792\n",
      "test loss 57.82615350217235\n",
      "================================================== gen= 0 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.80742073059082  Time:  0.03362703323364258\n",
      "mean acc =  tensor(0.7885)\n",
      "train loss 57.80742073059082\n",
      "test loss 56.46740335996459\n",
      "================================================== gen= 0 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.23537731170654  Time:  0.02991485595703125\n",
      "mean acc =  tensor(1.2964)\n",
      "train loss 60.23537731170654\n",
      "test loss 54.033818251421664\n",
      "================================================== gen= 0 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.89059925079346  Time:  0.029676198959350586\n",
      "mean acc =  tensor(1.0664)\n",
      "train loss 59.89059925079346\n",
      "test loss 51.877600222217794\n",
      "================================================== gen= 0 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  55.496764183044434  Time:  0.027671098709106445\n",
      "mean acc =  tensor(0.8419)\n",
      "train loss 55.496764183044434\n",
      "test loss 55.268007810423974\n",
      "================================================== gen= 0 index 7 vector= 0\n",
      "updating model =======  55.268007810423974\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  106.1935977935791  Time:  0.024643898010253906\n",
      "mean acc =  tensor(0.0128)\n",
      "train loss 106.1935977935791\n",
      "test loss 114.27600891736088\n",
      "================================================== gen= 0 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.34958076477051  Time:  0.05637192726135254\n",
      "mean acc =  tensor(0.0707)\n",
      "train loss 114.34958076477051\n",
      "test loss 107.9555365114796\n",
      "================================================== gen= 0 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  108.07912254333496  Time:  0.02272510528564453\n",
      "mean acc =  tensor(0.2660)\n",
      "train loss 108.07912254333496\n",
      "test loss 87.87966202716439\n",
      "================================================== gen= 0 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  80.37597274780273  Time:  0.025268077850341797\n",
      "mean acc =  tensor(1.1010)\n",
      "train loss 80.37597274780273\n",
      "test loss 56.86277786566286\n",
      "================================================== gen= 0 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.57899284362793  Time:  0.031108856201171875\n",
      "mean acc =  tensor(1.2261)\n",
      "train loss 75.57899284362793\n",
      "test loss 57.00117640592614\n",
      "================================================== gen= 0 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.218671798706055  Time:  0.03309178352355957\n",
      "mean acc =  tensor(0.7260)\n",
      "train loss 63.218671798706055\n",
      "test loss 61.87437906070631\n",
      "================================================== gen= 0 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.963500022888184  Time:  0.024924278259277344\n",
      "mean acc =  tensor(0.6962)\n",
      "train loss 63.963500022888184\n",
      "test loss 62.21724890365081\n",
      "================================================== gen= 0 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.71550369262695  Time:  0.024275779724121094\n",
      "mean acc =  tensor(1.0292)\n",
      "train loss 61.71550369262695\n",
      "test loss 54.002568952080345\n",
      "================================================== gen= 0 index 7 vector= 1\n",
      "updating model =======  54.002568952080345\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  120.3558578491211  Time:  0.029586076736450195\n",
      "mean acc =  tensor(0.0186)\n",
      "train loss 120.3558578491211\n",
      "test loss 113.67233655241881\n",
      "================================================== gen= 0 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.3371753692627  Time:  0.03046703338623047\n",
      "mean acc =  tensor(0.2748)\n",
      "train loss 111.3371753692627\n",
      "test loss 87.37227666945685\n",
      "================================================== gen= 0 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.8234453201294  Time:  0.03607821464538574\n",
      "mean acc =  tensor(1.4697)\n",
      "train loss 69.8234453201294\n",
      "test loss 64.18781106650424\n",
      "================================================== gen= 0 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.565053939819336  Time:  0.029748916625976562\n",
      "mean acc =  tensor(0.7949)\n",
      "train loss 60.565053939819336\n",
      "test loss 59.14702907224902\n",
      "================================================== gen= 0 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.2006425857544  Time:  0.030261993408203125\n",
      "mean acc =  tensor(0.8787)\n",
      "train loss 69.2006425857544\n",
      "test loss 56.979847006246345\n",
      "================================================== gen= 0 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.89690399169922  Time:  0.031313180923461914\n",
      "mean acc =  tensor(1.2750)\n",
      "train loss 59.89690399169922\n",
      "test loss 55.74323791711509\n",
      "================================================== gen= 0 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.04104995727539  Time:  0.03359508514404297\n",
      "mean acc =  tensor(0.9754)\n",
      "train loss 60.04104995727539\n",
      "test loss 54.7751417873668\n",
      "================================================== gen= 0 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.90546894073486  Time:  0.03235507011413574\n",
      "mean acc =  tensor(0.9903)\n",
      "train loss 53.90546894073486\n",
      "test loss 53.684801140610055\n",
      "================================================== gen= 0 index 7 vector= 2\n",
      "updating model =======  53.684801140610055\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.32609558105469  Time:  0.03188681602478027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0334)\n",
      "train loss 114.32609558105469\n",
      "test loss 111.90443757764336\n",
      "================================================== gen= 0 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.04493141174316  Time:  0.03032207489013672\n",
      "mean acc =  tensor(0.3081)\n",
      "train loss 101.04493141174316\n",
      "test loss 86.29345043986832\n",
      "================================================== gen= 0 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.30081367492676  Time:  0.02993631362915039\n",
      "mean acc =  tensor(1.6107)\n",
      "train loss 65.30081367492676\n",
      "test loss 68.65325032448283\n",
      "================================================== gen= 0 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.4921932220459  Time:  0.031123876571655273\n",
      "mean acc =  tensor(0.7574)\n",
      "train loss 63.4921932220459\n",
      "test loss 60.516020067695045\n",
      "================================================== gen= 0 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.57570743560791  Time:  0.0350031852722168\n",
      "mean acc =  tensor(0.7558)\n",
      "train loss 68.57570743560791\n",
      "test loss 58.56712655307484\n",
      "================================================== gen= 0 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.76227569580078  Time:  0.029778003692626953\n",
      "mean acc =  tensor(1.3585)\n",
      "train loss 62.76227569580078\n",
      "test loss 54.8499222061261\n",
      "================================================== gen= 0 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.79673385620117  Time:  0.03350377082824707\n",
      "mean acc =  tensor(1.1824)\n",
      "train loss 62.79673385620117\n",
      "test loss 51.090202227741685\n",
      "================================================== gen= 0 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  54.41318416595459  Time:  0.030107975006103516\n",
      "mean acc =  tensor(1.0088)\n",
      "train loss 54.41318416595459\n",
      "test loss 51.02088468901965\n",
      "================================================== gen= 0 index 7 vector= 3\n",
      "updating model =======  51.02088468901965\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.04496765136719  Time:  0.03191804885864258\n",
      "mean acc =  tensor(0.0312)\n",
      "train loss 111.04496765136719\n",
      "test loss 112.30409022739956\n",
      "================================================== gen= 1 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  108.88454818725586  Time:  0.02948594093322754\n",
      "mean acc =  tensor(0.1642)\n",
      "train loss 108.88454818725586\n",
      "test loss 98.10408383324032\n",
      "================================================== gen= 1 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  84.05757141113281  Time:  0.030460119247436523\n",
      "mean acc =  tensor(1.1579)\n",
      "train loss 84.05757141113281\n",
      "test loss 53.94967186856432\n",
      "================================================== gen= 1 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.52015113830566  Time:  0.030867815017700195\n",
      "mean acc =  tensor(1.0081)\n",
      "train loss 67.52015113830566\n",
      "test loss 54.073680825784905\n",
      "================================================== gen= 1 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.87172222137451  Time:  0.03337502479553223\n",
      "mean acc =  tensor(0.7430)\n",
      "train loss 68.87172222137451\n",
      "test loss 62.15241205124628\n",
      "================================================== gen= 1 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  54.74256896972656  Time:  0.03164219856262207\n",
      "mean acc =  tensor(0.8964)\n",
      "train loss 54.74256896972656\n",
      "test loss 54.690794081914994\n",
      "================================================== gen= 1 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.43062210083008  Time:  0.02867722511291504\n",
      "mean acc =  tensor(1.2968)\n",
      "train loss 62.43062210083008\n",
      "test loss 54.10289733263911\n",
      "================================================== gen= 1 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.19696617126465  Time:  0.029195070266723633\n",
      "mean acc =  tensor(0.9089)\n",
      "train loss 60.19696617126465\n",
      "test loss 53.876011699235356\n",
      "================================================== gen= 1 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.57710838317871  Time:  0.024609088897705078\n",
      "mean acc =  tensor(0.0081)\n",
      "train loss 116.57710838317871\n",
      "test loss 114.75543197320432\n",
      "================================================== gen= 1 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  113.52492713928223  Time:  0.02701401710510254\n",
      "mean acc =  tensor(0.0785)\n",
      "train loss 113.52492713928223\n",
      "test loss 107.40647415887742\n",
      "================================================== gen= 1 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  92.48099136352539  Time:  0.02554607391357422\n",
      "mean acc =  tensor(0.3759)\n",
      "train loss 92.48099136352539\n",
      "test loss 82.50116877653161\n",
      "================================================== gen= 1 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.30146408081055  Time:  0.028896093368530273\n",
      "mean acc =  tensor(1.2913)\n",
      "train loss 72.30146408081055\n",
      "test loss 54.51943165266595\n",
      "================================================== gen= 1 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.73959541320801  Time:  0.02449631690979004\n",
      "mean acc =  tensor(1.1825)\n",
      "train loss 72.73959541320801\n",
      "test loss 52.7401829155124\n",
      "================================================== gen= 1 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.954729080200195  Time:  0.02425694465637207\n",
      "mean acc =  tensor(0.7224)\n",
      "train loss 57.954729080200195\n",
      "test loss 58.8745912564855\n",
      "================================================== gen= 1 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.97789001464844  Time:  0.02425980567932129\n",
      "mean acc =  tensor(0.8568)\n",
      "train loss 64.97789001464844\n",
      "test loss 55.652397752619116\n",
      "================================================== gen= 1 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.20130729675293  Time:  0.02669382095336914\n",
      "mean acc =  tensor(1.1526)\n",
      "train loss 60.20130729675293\n",
      "test loss 51.7782101793354\n",
      "================================================== gen= 1 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.5106315612793  Time:  0.03186798095703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0222)\n",
      "train loss 116.5106315612793\n",
      "test loss 113.19001458615674\n",
      "================================================== gen= 1 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.4984016418457  Time:  0.030858993530273438\n",
      "mean acc =  tensor(0.2180)\n",
      "train loss 114.4984016418457\n",
      "test loss 94.45153585420985\n",
      "================================================== gen= 1 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  83.57366561889648  Time:  0.029792070388793945\n",
      "mean acc =  tensor(1.3600)\n",
      "train loss 83.57366561889648\n",
      "test loss 58.50640799074757\n",
      "================================================== gen= 1 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.79944038391113  Time:  0.032134056091308594\n",
      "mean acc =  tensor(0.9061)\n",
      "train loss 67.79944038391113\n",
      "test loss 54.91193854403333\n",
      "================================================== gen= 1 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.095624923706055  Time:  0.03179478645324707\n",
      "mean acc =  tensor(0.6902)\n",
      "train loss 57.095624923706055\n",
      "test loss 64.1948464062749\n",
      "================================================== gen= 1 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.7395076751709  Time:  0.033582210540771484\n",
      "mean acc =  tensor(0.9259)\n",
      "train loss 72.7395076751709\n",
      "test loss 53.99179173164627\n",
      "================================================== gen= 1 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.55203056335449  Time:  0.034732818603515625\n",
      "mean acc =  tensor(1.2462)\n",
      "train loss 62.55203056335449\n",
      "test loss 53.51764515468052\n",
      "================================================== gen= 1 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.42129611968994  Time:  0.032546043395996094\n",
      "mean acc =  tensor(1.0856)\n",
      "train loss 61.42129611968994\n",
      "test loss 51.736930769317006\n",
      "================================================== gen= 1 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  109.71940612792969  Time:  0.03098297119140625\n",
      "mean acc =  tensor(0.1818)\n",
      "train loss 109.71940612792969\n",
      "test loss 97.7234373546782\n",
      "================================================== gen= 1 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  76.92036056518555  Time:  0.030685901641845703\n",
      "mean acc =  tensor(1.5068)\n",
      "train loss 76.92036056518555\n",
      "test loss 62.87804454362311\n",
      "================================================== gen= 1 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.20804214477539  Time:  0.030217409133911133\n",
      "mean acc =  tensor(0.7864)\n",
      "train loss 69.20804214477539\n",
      "test loss 59.002712068103605\n",
      "================================================== gen= 1 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.996365547180176  Time:  0.03398609161376953\n",
      "mean acc =  tensor(0.7981)\n",
      "train loss 62.996365547180176\n",
      "test loss 56.83325091511214\n",
      "================================================== gen= 1 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.74615955352783  Time:  0.03008294105529785\n",
      "mean acc =  tensor(1.2131)\n",
      "train loss 60.74615955352783\n",
      "test loss 53.81094547193877\n",
      "================================================== gen= 1 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.97072792053223  Time:  0.03087592124938965\n",
      "mean acc =  tensor(0.9902)\n",
      "train loss 67.97072792053223\n",
      "test loss 52.811943287752115\n",
      "================================================== gen= 1 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.62319278717041  Time:  0.03098893165588379\n",
      "mean acc =  tensor(0.8668)\n",
      "train loss 59.62319278717041\n",
      "test loss 55.49943589191047\n",
      "================================================== gen= 1 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  52.01729488372803  Time:  0.03470897674560547\n",
      "mean acc =  tensor(1.3212)\n",
      "train loss 52.01729488372803\n",
      "test loss 52.6729006086077\n",
      "================================================== gen= 1 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  113.16495513916016  Time:  0.03029489517211914\n",
      "mean acc =  tensor(0.0160)\n",
      "train loss 113.16495513916016\n",
      "test loss 113.85841032274726\n",
      "================================================== gen= 2 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.1301383972168  Time:  0.03017282485961914\n",
      "mean acc =  tensor(0.1047)\n",
      "train loss 102.1301383972168\n",
      "test loss 103.69826305155851\n",
      "================================================== gen= 2 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  97.15033340454102  Time:  0.030064821243286133\n",
      "mean acc =  tensor(0.7462)\n",
      "train loss 97.15033340454102\n",
      "test loss 60.50569020485391\n",
      "================================================== gen= 2 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.09190654754639  Time:  0.029884815216064453\n",
      "mean acc =  tensor(1.2170)\n",
      "train loss 64.09190654754639\n",
      "test loss 54.95378351535927\n",
      "================================================== gen= 2 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.3411808013916  Time:  0.03019118309020996\n",
      "mean acc =  tensor(0.6867)\n",
      "train loss 67.3411808013916\n",
      "test loss 62.21771349225725\n",
      "================================================== gen= 2 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.405192375183105  Time:  0.030400753021240234\n",
      "mean acc =  tensor(0.9238)\n",
      "train loss 63.405192375183105\n",
      "test loss 54.642229274827606\n",
      "================================================== gen= 2 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.351694107055664  Time:  0.0301668643951416\n",
      "mean acc =  tensor(1.2507)\n",
      "train loss 59.351694107055664\n",
      "test loss 53.880667524272894\n",
      "================================================== gen= 2 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.43318843841553  Time:  0.03366708755493164\n",
      "mean acc =  tensor(0.9883)\n",
      "train loss 57.43318843841553\n",
      "test loss 52.71099684838535\n",
      "================================================== gen= 2 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  117.19084548950195  Time:  0.025365114212036133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0030)\n",
      "train loss 117.19084548950195\n",
      "test loss 115.34058535828882\n",
      "================================================== gen= 2 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.21441459655762  Time:  0.025146961212158203\n",
      "mean acc =  tensor(0.0414)\n",
      "train loss 112.21441459655762\n",
      "test loss 111.11644750063111\n",
      "================================================== gen= 2 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  100.58449363708496  Time:  0.0247042179107666\n",
      "mean acc =  tensor(0.2100)\n",
      "train loss 100.58449363708496\n",
      "test loss 95.46708819330955\n",
      "================================================== gen= 2 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  81.19926261901855  Time:  0.02815413475036621\n",
      "mean acc =  tensor(0.8678)\n",
      "train loss 81.19926261901855\n",
      "test loss 56.14089052368995\n",
      "================================================== gen= 2 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.92040729522705  Time:  0.026182889938354492\n",
      "mean acc =  tensor(1.6891)\n",
      "train loss 67.92040729522705\n",
      "test loss 61.97417852181156\n",
      "================================================== gen= 2 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.6696424484253  Time:  0.0267031192779541\n",
      "mean acc =  tensor(0.8415)\n",
      "train loss 69.6696424484253\n",
      "test loss 54.833551964792264\n",
      "================================================== gen= 2 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  54.87125110626221  Time:  0.028400182723999023\n",
      "mean acc =  tensor(0.7414)\n",
      "train loss 54.87125110626221\n",
      "test loss 59.82831700642904\n",
      "================================================== gen= 2 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.642635345458984  Time:  0.028677940368652344\n",
      "mean acc =  tensor(0.9402)\n",
      "train loss 63.642635345458984\n",
      "test loss 53.30335720866716\n",
      "================================================== gen= 2 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  115.22531700134277  Time:  0.0358738899230957\n",
      "mean acc =  tensor(0.0175)\n",
      "train loss 115.22531700134277\n",
      "test loss 113.61751431834941\n",
      "================================================== gen= 2 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.48220252990723  Time:  0.0341191291809082\n",
      "mean acc =  tensor(0.2595)\n",
      "train loss 112.48220252990723\n",
      "test loss 90.26800773257301\n",
      "================================================== gen= 2 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.85708141326904  Time:  0.03374195098876953\n",
      "mean acc =  tensor(1.4600)\n",
      "train loss 69.85708141326904\n",
      "test loss 60.10734864319263\n",
      "================================================== gen= 2 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.979613304138184  Time:  0.030364036560058594\n",
      "mean acc =  tensor(0.9689)\n",
      "train loss 53.979613304138184\n",
      "test loss 55.86560520509473\n",
      "================================================== gen= 2 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.544705390930176  Time:  0.03675127029418945\n",
      "mean acc =  tensor(0.8846)\n",
      "train loss 60.544705390930176\n",
      "test loss 56.593435741606214\n",
      "================================================== gen= 2 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.04306507110596  Time:  0.03793597221374512\n",
      "mean acc =  tensor(1.3852)\n",
      "train loss 63.04306507110596\n",
      "test loss 55.561198591375025\n",
      "================================================== gen= 2 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.673707008361816  Time:  0.0384829044342041\n",
      "mean acc =  tensor(0.8685)\n",
      "train loss 63.673707008361816\n",
      "test loss 55.30798825114763\n",
      "================================================== gen= 2 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.67044258117676  Time:  0.029217958450317383\n",
      "mean acc =  tensor(1.0077)\n",
      "train loss 59.67044258117676\n",
      "test loss 51.399335861206055\n",
      "================================================== gen= 2 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  123.43005752563477  Time:  0.03273582458496094\n",
      "mean acc =  tensor(0.1173)\n",
      "train loss 123.43005752563477\n",
      "test loss 103.34618081851882\n",
      "================================================== gen= 2 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.96808815002441  Time:  0.03200411796569824\n",
      "mean acc =  tensor(1.2120)\n",
      "train loss 78.96808815002441\n",
      "test loss 54.84302941147162\n",
      "================================================== gen= 2 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.18357849121094  Time:  0.03177285194396973\n",
      "mean acc =  tensor(0.9118)\n",
      "train loss 67.18357849121094\n",
      "test loss 56.05366944293586\n",
      "================================================== gen= 2 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  70.13193893432617  Time:  0.03596782684326172\n",
      "mean acc =  tensor(0.6870)\n",
      "train loss 70.13193893432617\n",
      "test loss 61.85432332875777\n",
      "================================================== gen= 2 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.32658100128174  Time:  0.03037118911743164\n",
      "mean acc =  tensor(1.2291)\n",
      "train loss 62.32658100128174\n",
      "test loss 54.39115456821156\n",
      "================================================== gen= 2 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.374534606933594  Time:  0.031080961227416992\n",
      "mean acc =  tensor(1.0883)\n",
      "train loss 61.374534606933594\n",
      "test loss 52.830904461088636\n",
      "================================================== gen= 2 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  56.681403160095215  Time:  0.032363176345825195\n",
      "mean acc =  tensor(0.7420)\n",
      "train loss 56.681403160095215\n",
      "test loss 59.46702152693353\n",
      "================================================== gen= 2 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.88994216918945  Time:  0.034471988677978516\n",
      "mean acc =  tensor(1.0973)\n",
      "train loss 65.88994216918945\n",
      "test loss 52.12668704337814\n",
      "================================================== gen= 2 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.52476692199707  Time:  0.030461788177490234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0110)\n",
      "train loss 114.52476692199707\n",
      "test loss 114.46136121685002\n",
      "================================================== gen= 3 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  106.86164093017578  Time:  0.02962207794189453\n",
      "mean acc =  tensor(0.1650)\n",
      "train loss 106.86164093017578\n",
      "test loss 98.69039538117494\n",
      "================================================== gen= 3 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.54873657226562  Time:  0.0290071964263916\n",
      "mean acc =  tensor(1.2329)\n",
      "train loss 78.54873657226562\n",
      "test loss 57.48389725458054\n",
      "================================================== gen= 3 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.17510604858398  Time:  0.031516075134277344\n",
      "mean acc =  tensor(0.9338)\n",
      "train loss 67.17510604858398\n",
      "test loss 56.438334173085735\n",
      "================================================== gen= 3 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.02861881256104  Time:  0.028837919235229492\n",
      "mean acc =  tensor(0.7454)\n",
      "train loss 67.02861881256104\n",
      "test loss 62.073392543662976\n",
      "================================================== gen= 3 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.213385581970215  Time:  0.029224157333374023\n",
      "mean acc =  tensor(1.3637)\n",
      "train loss 61.213385581970215\n",
      "test loss 54.291174492868436\n",
      "================================================== gen= 3 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.63022327423096  Time:  0.029576778411865234\n",
      "mean acc =  tensor(0.9532)\n",
      "train loss 62.63022327423096\n",
      "test loss 53.26755832490467\n",
      "================================================== gen= 3 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.59929084777832  Time:  0.02936720848083496\n",
      "mean acc =  tensor(0.7772)\n",
      "train loss 64.59929084777832\n",
      "test loss 56.77935196753262\n",
      "================================================== gen= 3 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  117.47984886169434  Time:  0.02538132667541504\n",
      "mean acc =  tensor(0.0384)\n",
      "train loss 117.47984886169434\n",
      "test loss 111.60608231940238\n",
      "================================================== gen= 3 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  96.71345520019531  Time:  0.024779081344604492\n",
      "mean acc =  tensor(0.2046)\n",
      "train loss 96.71345520019531\n",
      "test loss 94.1563799073096\n",
      "================================================== gen= 3 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.55223274230957  Time:  0.02496504783630371\n",
      "mean acc =  tensor(0.8600)\n",
      "train loss 72.55223274230957\n",
      "test loss 57.588990062272465\n",
      "================================================== gen= 3 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.82542705535889  Time:  0.024909019470214844\n",
      "mean acc =  tensor(1.2255)\n",
      "train loss 63.82542705535889\n",
      "test loss 56.62748627435593\n",
      "================================================== gen= 3 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.57548427581787  Time:  0.02558302879333496\n",
      "mean acc =  tensor(0.9680)\n",
      "train loss 72.57548427581787\n",
      "test loss 54.271650171604286\n",
      "================================================== gen= 3 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  51.918020248413086  Time:  0.02491593360900879\n",
      "mean acc =  tensor(0.9326)\n",
      "train loss 51.918020248413086\n",
      "test loss 54.21749390063643\n",
      "================================================== gen= 3 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.04340171813965  Time:  0.0245819091796875\n",
      "mean acc =  tensor(1.1120)\n",
      "train loss 63.04340171813965\n",
      "test loss 52.72110460242447\n",
      "================================================== gen= 3 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.97434139251709  Time:  0.026332855224609375\n",
      "mean acc =  tensor(1.0572)\n",
      "train loss 58.97434139251709\n",
      "test loss 52.47360815969454\n",
      "================================================== gen= 3 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.17500877380371  Time:  0.03212785720825195\n",
      "mean acc =  tensor(0.0582)\n",
      "train loss 102.17500877380371\n",
      "test loss 109.35306139057185\n",
      "================================================== gen= 3 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  94.39640045166016  Time:  0.030966997146606445\n",
      "mean acc =  tensor(0.4905)\n",
      "train loss 94.39640045166016\n",
      "test loss 71.69636006257971\n",
      "================================================== gen= 3 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.78737831115723  Time:  0.031239032745361328\n",
      "mean acc =  tensor(1.4386)\n",
      "train loss 66.78737831115723\n",
      "test loss 60.37876928420294\n",
      "================================================== gen= 3 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.61338901519775  Time:  0.029970884323120117\n",
      "mean acc =  tensor(0.6393)\n",
      "train loss 67.61338901519775\n",
      "test loss 64.11514837563443\n",
      "================================================== gen= 3 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.2406702041626  Time:  0.03199601173400879\n",
      "mean acc =  tensor(0.8641)\n",
      "train loss 58.2406702041626\n",
      "test loss 55.58940962707104\n",
      "================================================== gen= 3 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.40273475646973  Time:  0.03113389015197754\n",
      "mean acc =  tensor(1.3575)\n",
      "train loss 60.40273475646973\n",
      "test loss 53.64691130968989\n",
      "================================================== gen= 3 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.623162269592285  Time:  0.03285098075866699\n",
      "mean acc =  tensor(0.9426)\n",
      "train loss 53.623162269592285\n",
      "test loss 54.06206590302136\n",
      "================================================== gen= 3 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  52.39134693145752  Time:  0.032550811767578125\n",
      "mean acc =  tensor(1.0769)\n",
      "train loss 52.39134693145752\n",
      "test loss 50.1955025342046\n",
      "================================================== gen= 3 index 7 vector= 2\n",
      "updating model =======  50.1955025342046\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  113.23997497558594  Time:  0.030210018157958984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0961)\n",
      "train loss 113.23997497558594\n",
      "test loss 104.64832773013991\n",
      "================================================== gen= 3 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  80.92068862915039  Time:  0.03139615058898926\n",
      "mean acc =  tensor(1.0178)\n",
      "train loss 80.92068862915039\n",
      "test loss 55.40372651288298\n",
      "================================================== gen= 3 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.431809425354  Time:  0.028050899505615234\n",
      "mean acc =  tensor(0.8438)\n",
      "train loss 75.431809425354\n",
      "test loss 56.61990195553319\n",
      "================================================== gen= 3 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.15364933013916  Time:  0.0298001766204834\n",
      "mean acc =  tensor(0.6922)\n",
      "train loss 61.15364933013916\n",
      "test loss 62.42221673978429\n",
      "================================================== gen= 3 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.61150646209717  Time:  0.07188725471496582\n",
      "mean acc =  tensor(1.0612)\n",
      "train loss 58.61150646209717\n",
      "test loss 53.85842065097523\n",
      "================================================== gen= 3 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.293121337890625  Time:  0.030406951904296875\n",
      "mean acc =  tensor(1.2110)\n",
      "train loss 61.293121337890625\n",
      "test loss 53.82970599738919\n",
      "================================================== gen= 3 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.96421241760254  Time:  0.03110814094543457\n",
      "mean acc =  tensor(0.8961)\n",
      "train loss 63.96421241760254\n",
      "test loss 55.238527207147506\n",
      "================================================== gen= 3 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.43356513977051  Time:  0.028872013092041016\n",
      "mean acc =  tensor(0.9902)\n",
      "train loss 65.43356513977051\n",
      "test loss 52.51279275595736\n",
      "================================================== gen= 3 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.56763648986816  Time:  0.029706954956054688\n",
      "mean acc =  tensor(0.0458)\n",
      "train loss 112.56763648986816\n",
      "test loss 110.83116720809417\n",
      "================================================== gen= 4 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  96.31593132019043  Time:  0.030035972595214844\n",
      "mean acc =  tensor(0.4219)\n",
      "train loss 96.31593132019043\n",
      "test loss 75.94265326675104\n",
      "================================================== gen= 4 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  79.20868110656738  Time:  0.028666019439697266\n",
      "mean acc =  tensor(1.5471)\n",
      "train loss 79.20868110656738\n",
      "test loss 68.0682417681428\n",
      "================================================== gen= 4 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  70.31377220153809  Time:  0.029516935348510742\n",
      "mean acc =  tensor(0.7137)\n",
      "train loss 70.31377220153809\n",
      "test loss 61.92113380691632\n",
      "================================================== gen= 4 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  74.54942989349365  Time:  0.0293118953704834\n",
      "mean acc =  tensor(0.6604)\n",
      "train loss 74.54942989349365\n",
      "test loss 64.00392387026832\n",
      "================================================== gen= 4 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.65905284881592  Time:  0.02981710433959961\n",
      "mean acc =  tensor(1.1928)\n",
      "train loss 58.65905284881592\n",
      "test loss 52.994725752849966\n",
      "================================================== gen= 4 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.789536476135254  Time:  0.030117034912109375\n",
      "mean acc =  tensor(1.1307)\n",
      "train loss 62.789536476135254\n",
      "test loss 52.55101441363899\n",
      "================================================== gen= 4 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.740410804748535  Time:  0.0292971134185791\n",
      "mean acc =  tensor(0.9470)\n",
      "train loss 57.740410804748535\n",
      "test loss 54.123382231005195\n",
      "================================================== gen= 4 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  108.84891319274902  Time:  0.025712251663208008\n",
      "mean acc =  tensor(0.0298)\n",
      "train loss 108.84891319274902\n",
      "test loss 112.34789089280731\n",
      "================================================== gen= 4 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  115.0526351928711  Time:  0.02307581901550293\n",
      "mean acc =  tensor(0.0767)\n",
      "train loss 115.0526351928711\n",
      "test loss 107.15887461552003\n",
      "================================================== gen= 4 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.38116073608398  Time:  0.022439956665039062\n",
      "mean acc =  tensor(0.2485)\n",
      "train loss 101.38116073608398\n",
      "test loss 91.4559516647235\n",
      "================================================== gen= 4 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  77.39214706420898  Time:  0.024837732315063477\n",
      "mean acc =  tensor(0.7869)\n",
      "train loss 77.39214706420898\n",
      "test loss 58.197763663570896\n",
      "================================================== gen= 4 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.78365993499756  Time:  0.02358102798461914\n",
      "mean acc =  tensor(1.3921)\n",
      "train loss 69.78365993499756\n",
      "test loss 57.82827930061185\n",
      "================================================== gen= 4 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.43005561828613  Time:  0.024905920028686523\n",
      "mean acc =  tensor(0.9079)\n",
      "train loss 65.43005561828613\n",
      "test loss 55.477279247880794\n",
      "================================================== gen= 4 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  56.663315773010254  Time:  0.026154041290283203\n",
      "mean acc =  tensor(0.7444)\n",
      "train loss 56.663315773010254\n",
      "test loss 60.27441824050177\n",
      "================================================== gen= 4 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.50578212738037  Time:  0.02495574951171875\n",
      "mean acc =  tensor(0.8955)\n",
      "train loss 57.50578212738037\n",
      "test loss 54.04509548265107\n",
      "================================================== gen= 4 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  115.03446769714355  Time:  0.03238487243652344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0771)\n",
      "train loss 115.03446769714355\n",
      "test loss 107.33083820991776\n",
      "================================================== gen= 4 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  97.14189720153809  Time:  0.03010082244873047\n",
      "mean acc =  tensor(0.5864)\n",
      "train loss 97.14189720153809\n",
      "test loss 66.28462294494214\n",
      "================================================== gen= 4 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.35263442993164  Time:  0.028804779052734375\n",
      "mean acc =  tensor(1.2249)\n",
      "train loss 62.35263442993164\n",
      "test loss 54.173150588055044\n",
      "================================================== gen= 4 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.94679260253906  Time:  0.02863597869873047\n",
      "mean acc =  tensor(0.7989)\n",
      "train loss 63.94679260253906\n",
      "test loss 57.283793884069745\n",
      "================================================== gen= 4 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.44932842254639  Time:  0.03197503089904785\n",
      "mean acc =  tensor(1.2671)\n",
      "train loss 58.44932842254639\n",
      "test loss 52.73220770699637\n",
      "================================================== gen= 4 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  51.13968086242676  Time:  0.029208898544311523\n",
      "mean acc =  tensor(1.0662)\n",
      "train loss 51.13968086242676\n",
      "test loss 52.13052490130574\n",
      "================================================== gen= 4 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.94661998748779  Time:  0.0318300724029541\n",
      "mean acc =  tensor(1.0385)\n",
      "train loss 53.94661998748779\n",
      "test loss 52.62311673326557\n",
      "================================================== gen= 4 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.15498733520508  Time:  0.029397010803222656\n",
      "mean acc =  tensor(1.3696)\n",
      "train loss 57.15498733520508\n",
      "test loss 52.19687387894611\n",
      "================================================== gen= 4 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.73906326293945  Time:  0.03075098991394043\n",
      "mean acc =  tensor(0.0415)\n",
      "train loss 112.73906326293945\n",
      "test loss 111.04413609926392\n",
      "================================================== gen= 4 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.51936340332031  Time:  0.030618906021118164\n",
      "mean acc =  tensor(0.5410)\n",
      "train loss 101.51936340332031\n",
      "test loss 73.11876185410688\n",
      "================================================== gen= 4 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.29302597045898  Time:  0.03217363357543945\n",
      "mean acc =  tensor(1.2241)\n",
      "train loss 64.29302597045898\n",
      "test loss 54.281168957145844\n",
      "================================================== gen= 4 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.4776496887207  Time:  0.03204798698425293\n",
      "mean acc =  tensor(0.7179)\n",
      "train loss 62.4776496887207\n",
      "test loss 60.484064011346724\n",
      "================================================== gen= 4 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.057772636413574  Time:  0.030643939971923828\n",
      "mean acc =  tensor(1.1012)\n",
      "train loss 58.057772636413574\n",
      "test loss 53.190077489736126\n",
      "================================================== gen= 4 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  51.71310043334961  Time:  0.030215024948120117\n",
      "mean acc =  tensor(1.0783)\n",
      "train loss 51.71310043334961\n",
      "test loss 52.5122100414873\n",
      "================================================== gen= 4 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.80313682556152  Time:  0.029558897018432617\n",
      "mean acc =  tensor(0.9982)\n",
      "train loss 61.80313682556152\n",
      "test loss 53.1976318359375\n",
      "================================================== gen= 4 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.25362968444824  Time:  0.02860116958618164\n",
      "mean acc =  tensor(1.2271)\n",
      "train loss 63.25362968444824\n",
      "test loss 51.215830355274434\n",
      "================================================== gen= 4 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  117.93680763244629  Time:  0.030796051025390625\n",
      "mean acc =  tensor(0.0166)\n",
      "train loss 117.93680763244629\n",
      "test loss 113.89614136364995\n",
      "================================================== gen= 5 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.92885398864746  Time:  0.029675006866455078\n",
      "mean acc =  tensor(0.3237)\n",
      "train loss 98.92885398864746\n",
      "test loss 87.46650521933627\n",
      "================================================== gen= 5 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.29722023010254  Time:  0.02968001365661621\n",
      "mean acc =  tensor(1.6222)\n",
      "train loss 75.29722023010254\n",
      "test loss 69.06400392493423\n",
      "================================================== gen= 5 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.61129379272461  Time:  0.030144929885864258\n",
      "mean acc =  tensor(0.8094)\n",
      "train loss 59.61129379272461\n",
      "test loss 57.723854350394944\n",
      "================================================== gen= 5 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.810065269470215  Time:  0.03318500518798828\n",
      "mean acc =  tensor(0.6459)\n",
      "train loss 61.810065269470215\n",
      "test loss 62.556054874342315\n",
      "================================================== gen= 5 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  74.01272583007812  Time:  0.028867006301879883\n",
      "mean acc =  tensor(1.2149)\n",
      "train loss 74.01272583007812\n",
      "test loss 52.74906664478536\n",
      "================================================== gen= 5 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.9017915725708  Time:  0.029701948165893555\n",
      "mean acc =  tensor(1.0983)\n",
      "train loss 63.9017915725708\n",
      "test loss 52.05387893988161\n",
      "================================================== gen= 5 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.58865737915039  Time:  0.033147335052490234\n",
      "mean acc =  tensor(0.9341)\n",
      "train loss 61.58865737915039\n",
      "test loss 54.37981264283057\n",
      "================================================== gen= 5 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  115.09497451782227  Time:  0.02560114860534668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0026)\n",
      "train loss 115.09497451782227\n",
      "test loss 115.37638756369248\n",
      "================================================== gen= 5 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  109.48715782165527  Time:  0.025509119033813477\n",
      "mean acc =  tensor(0.0463)\n",
      "train loss 109.48715782165527\n",
      "test loss 110.65832815364915\n",
      "================================================== gen= 5 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  103.09577941894531  Time:  0.02520895004272461\n",
      "mean acc =  tensor(0.2069)\n",
      "train loss 103.09577941894531\n",
      "test loss 94.68260099449937\n",
      "================================================== gen= 5 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  87.19661617279053  Time:  0.024407148361206055\n",
      "mean acc =  tensor(0.8277)\n",
      "train loss 87.19661617279053\n",
      "test loss 57.641662727407855\n",
      "================================================== gen= 5 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.65922927856445  Time:  0.025360822677612305\n",
      "mean acc =  tensor(1.4922)\n",
      "train loss 68.65922927856445\n",
      "test loss 56.8356280424157\n",
      "================================================== gen= 5 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.07191181182861  Time:  0.024953126907348633\n",
      "mean acc =  tensor(0.9627)\n",
      "train loss 67.07191181182861\n",
      "test loss 53.501607933822946\n",
      "================================================== gen= 5 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.21426963806152  Time:  0.025092124938964844\n",
      "mean acc =  tensor(0.8801)\n",
      "train loss 53.21426963806152\n",
      "test loss 54.15469098253315\n",
      "================================================== gen= 5 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.94843864440918  Time:  0.024571895599365234\n",
      "mean acc =  tensor(1.1919)\n",
      "train loss 57.94843864440918\n",
      "test loss 49.78979725740394\n",
      "================================================== gen= 5 index 7 vector= 1\n",
      "updating model =======  49.78979725740394\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.41726112365723  Time:  0.032814979553222656\n",
      "mean acc =  tensor(0.0376)\n",
      "train loss 111.41726112365723\n",
      "test loss 111.79792962106718\n",
      "================================================== gen= 5 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.21015930175781  Time:  0.030143022537231445\n",
      "mean acc =  tensor(0.3868)\n",
      "train loss 114.21015930175781\n",
      "test loss 81.10143529152384\n",
      "================================================== gen= 5 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.13752746582031  Time:  0.029758930206298828\n",
      "mean acc =  tensor(1.6511)\n",
      "train loss 78.13752746582031\n",
      "test loss 61.630992526099796\n",
      "================================================== gen= 5 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.07951068878174  Time:  0.02966594696044922\n",
      "mean acc =  tensor(0.7105)\n",
      "train loss 65.07951068878174\n",
      "test loss 61.58514108463209\n",
      "================================================== gen= 5 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.46403789520264  Time:  0.02966928482055664\n",
      "mean acc =  tensor(0.8953)\n",
      "train loss 71.46403789520264\n",
      "test loss 57.8453200463535\n",
      "================================================== gen= 5 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.198296546936035  Time:  0.029577016830444336\n",
      "mean acc =  tensor(1.2852)\n",
      "train loss 53.198296546936035\n",
      "test loss 53.854571361931\n",
      "================================================== gen= 5 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.47873306274414  Time:  0.029883861541748047\n",
      "mean acc =  tensor(0.9463)\n",
      "train loss 58.47873306274414\n",
      "test loss 53.70119652780546\n",
      "================================================== gen= 5 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.96896171569824  Time:  0.02848672866821289\n",
      "mean acc =  tensor(0.9960)\n",
      "train loss 53.96896171569824\n",
      "test loss 52.35072695154722\n",
      "================================================== gen= 5 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  104.53058242797852  Time:  0.030656099319458008\n",
      "mean acc =  tensor(0.0591)\n",
      "train loss 104.53058242797852\n",
      "test loss 109.10945684731412\n",
      "================================================== gen= 5 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  87.45089340209961  Time:  0.03131818771362305\n",
      "mean acc =  tensor(0.4540)\n",
      "train loss 87.45089340209961\n",
      "test loss 75.368678008618\n",
      "================================================== gen= 5 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  70.73092079162598  Time:  0.03283262252807617\n",
      "mean acc =  tensor(1.5787)\n",
      "train loss 70.73092079162598\n",
      "test loss 62.08633674569681\n",
      "================================================== gen= 5 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.39209747314453  Time:  0.02991008758544922\n",
      "mean acc =  tensor(0.7385)\n",
      "train loss 60.39209747314453\n",
      "test loss 61.17946751912435\n",
      "================================================== gen= 5 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.12914371490479  Time:  0.03054523468017578\n",
      "mean acc =  tensor(0.8131)\n",
      "train loss 69.12914371490479\n",
      "test loss 57.743502402792174\n",
      "================================================== gen= 5 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.2301549911499  Time:  0.030415773391723633\n",
      "mean acc =  tensor(1.3184)\n",
      "train loss 64.2301549911499\n",
      "test loss 52.20315702269677\n",
      "================================================== gen= 5 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.547142028808594  Time:  0.029719829559326172\n",
      "mean acc =  tensor(1.0508)\n",
      "train loss 58.547142028808594\n",
      "test loss 50.91005455068991\n",
      "================================================== gen= 5 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.72147464752197  Time:  0.02797222137451172\n",
      "mean acc =  tensor(0.9257)\n",
      "train loss 59.72147464752197\n",
      "test loss 51.482024497726336\n",
      "================================================== gen= 5 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.89293098449707  Time:  0.029841899871826172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0781)\n",
      "train loss 110.89293098449707\n",
      "test loss 107.8888190133231\n",
      "================================================== gen= 6 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  89.71265125274658  Time:  0.030195951461791992\n",
      "mean acc =  tensor(0.8741)\n",
      "train loss 89.71265125274658\n",
      "test loss 57.74139611899447\n",
      "================================================== gen= 6 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.8306884765625  Time:  0.029882192611694336\n",
      "mean acc =  tensor(1.1467)\n",
      "train loss 72.8306884765625\n",
      "test loss 54.44859090792079\n",
      "================================================== gen= 6 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  56.155213356018066  Time:  0.02988576889038086\n",
      "mean acc =  tensor(0.5549)\n",
      "train loss 56.155213356018066\n",
      "test loss 69.78766201304741\n",
      "================================================== gen= 6 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  76.93941020965576  Time:  0.028855085372924805\n",
      "mean acc =  tensor(0.7823)\n",
      "train loss 76.93941020965576\n",
      "test loss 58.72537003082483\n",
      "================================================== gen= 6 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.119290351867676  Time:  0.029410123825073242\n",
      "mean acc =  tensor(1.4378)\n",
      "train loss 59.119290351867676\n",
      "test loss 56.42779387908728\n",
      "================================================== gen= 6 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.68891716003418  Time:  0.029165029525756836\n",
      "mean acc =  tensor(0.9575)\n",
      "train loss 61.68891716003418\n",
      "test loss 53.737450995412814\n",
      "================================================== gen= 6 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.7755708694458  Time:  0.029602766036987305\n",
      "mean acc =  tensor(0.9188)\n",
      "train loss 60.7755708694458\n",
      "test loss 54.433698174093855\n",
      "================================================== gen= 6 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.1428050994873  Time:  0.02470707893371582\n",
      "mean acc =  tensor(0.0456)\n",
      "train loss 116.1428050994873\n",
      "test loss 110.73230239971966\n",
      "================================================== gen= 6 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.4254035949707  Time:  0.0245819091796875\n",
      "mean acc =  tensor(0.1287)\n",
      "train loss 101.4254035949707\n",
      "test loss 101.34272413188909\n",
      "================================================== gen= 6 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.20596313476562  Time:  0.024980783462524414\n",
      "mean acc =  tensor(0.4527)\n",
      "train loss 98.20596313476562\n",
      "test loss 75.7141669915647\n",
      "================================================== gen= 6 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.64829635620117  Time:  0.02362990379333496\n",
      "mean acc =  tensor(1.3770)\n",
      "train loss 71.64829635620117\n",
      "test loss 59.6917958421772\n",
      "================================================== gen= 6 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.19724655151367  Time:  0.024451017379760742\n",
      "mean acc =  tensor(1.1475)\n",
      "train loss 59.19724655151367\n",
      "test loss 54.42749970624236\n",
      "================================================== gen= 6 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.68427753448486  Time:  0.025075912475585938\n",
      "mean acc =  tensor(0.8482)\n",
      "train loss 60.68427753448486\n",
      "test loss 57.80954983769631\n",
      "================================================== gen= 6 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.93707466125488  Time:  0.024788856506347656\n",
      "mean acc =  tensor(0.8457)\n",
      "train loss 63.93707466125488\n",
      "test loss 56.24847440654729\n",
      "================================================== gen= 6 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.862741470336914  Time:  0.026026010513305664\n",
      "mean acc =  tensor(1.1360)\n",
      "train loss 60.862741470336914\n",
      "test loss 53.369335434063764\n",
      "================================================== gen= 6 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  108.7796401977539  Time:  0.030447959899902344\n",
      "mean acc =  tensor(0.0442)\n",
      "train loss 108.7796401977539\n",
      "test loss 110.57299939629172\n",
      "================================================== gen= 6 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.5359001159668  Time:  0.03269696235656738\n",
      "mean acc =  tensor(0.6379)\n",
      "train loss 102.5359001159668\n",
      "test loss 66.81744545657618\n",
      "================================================== gen= 6 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  76.34375953674316  Time:  0.029568910598754883\n",
      "mean acc =  tensor(1.2835)\n",
      "train loss 76.34375953674316\n",
      "test loss 55.923784450608856\n",
      "================================================== gen= 6 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.41995811462402  Time:  0.03113412857055664\n",
      "mean acc =  tensor(0.6748)\n",
      "train loss 62.41995811462402\n",
      "test loss 62.65392254161186\n",
      "================================================== gen= 6 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.550095558166504  Time:  0.03005504608154297\n",
      "mean acc =  tensor(0.9149)\n",
      "train loss 58.550095558166504\n",
      "test loss 54.92002614339193\n",
      "================================================== gen= 6 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.390602111816406  Time:  0.030342817306518555\n",
      "mean acc =  tensor(1.3732)\n",
      "train loss 63.390602111816406\n",
      "test loss 54.201409917299436\n",
      "================================================== gen= 6 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.82163619995117  Time:  0.029623985290527344\n",
      "mean acc =  tensor(0.9326)\n",
      "train loss 58.82163619995117\n",
      "test loss 53.157138279506135\n",
      "================================================== gen= 6 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.16500949859619  Time:  0.029462099075317383\n",
      "mean acc =  tensor(1.0632)\n",
      "train loss 59.16500949859619\n",
      "test loss 50.95130951550542\n",
      "================================================== gen= 6 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.74275970458984  Time:  0.03320908546447754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0325)\n",
      "train loss 110.74275970458984\n",
      "test loss 112.25284664647108\n",
      "================================================== gen= 6 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.41510772705078  Time:  0.02965712547302246\n",
      "mean acc =  tensor(0.4906)\n",
      "train loss 101.41510772705078\n",
      "test loss 75.79846702627584\n",
      "================================================== gen= 6 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  73.21072578430176  Time:  0.029342174530029297\n",
      "mean acc =  tensor(1.3969)\n",
      "train loss 73.21072578430176\n",
      "test loss 59.59940226548383\n",
      "================================================== gen= 6 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.19400119781494  Time:  0.029862165451049805\n",
      "mean acc =  tensor(0.7669)\n",
      "train loss 65.19400119781494\n",
      "test loss 60.13458259738221\n",
      "================================================== gen= 6 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.28753852844238  Time:  0.0317530632019043\n",
      "mean acc =  tensor(1.0679)\n",
      "train loss 57.28753852844238\n",
      "test loss 54.148966237801275\n",
      "================================================== gen= 6 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.89436054229736  Time:  0.029986143112182617\n",
      "mean acc =  tensor(1.1249)\n",
      "train loss 62.89436054229736\n",
      "test loss 53.216180814366766\n",
      "================================================== gen= 6 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.146060943603516  Time:  0.03020477294921875\n",
      "mean acc =  tensor(0.9010)\n",
      "train loss 60.146060943603516\n",
      "test loss 55.113407135009766\n",
      "================================================== gen= 6 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.36676502227783  Time:  0.03012824058532715\n",
      "mean acc =  tensor(1.3332)\n",
      "train loss 62.36676502227783\n",
      "test loss 52.73641679724869\n",
      "================================================== gen= 6 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.64815330505371  Time:  0.02950906753540039\n",
      "mean acc =  tensor(-0.0028)\n",
      "train loss 114.64815330505371\n",
      "test loss 115.95005097681162\n",
      "================================================== gen= 7 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.64887809753418  Time:  0.029178142547607422\n",
      "mean acc =  tensor(0.0110)\n",
      "train loss 110.64887809753418\n",
      "test loss 114.45473355663066\n",
      "================================================== gen= 7 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.58757209777832  Time:  0.0287017822265625\n",
      "mean acc =  tensor(0.1126)\n",
      "train loss 112.58757209777832\n",
      "test loss 104.27369119034333\n",
      "================================================== gen= 7 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  88.51708030700684  Time:  0.029169082641601562\n",
      "mean acc =  tensor(0.7646)\n",
      "train loss 88.51708030700684\n",
      "test loss 60.26161489681322\n",
      "================================================== gen= 7 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.25319004058838  Time:  0.029253005981445312\n",
      "mean acc =  tensor(1.2768)\n",
      "train loss 67.25319004058838\n",
      "test loss 54.95016863556946\n",
      "================================================== gen= 7 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.11703777313232  Time:  0.029799938201904297\n",
      "mean acc =  tensor(0.6909)\n",
      "train loss 64.11703777313232\n",
      "test loss 61.25400953227971\n",
      "================================================== gen= 7 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.13824939727783  Time:  0.031517982482910156\n",
      "mean acc =  tensor(1.1674)\n",
      "train loss 65.13824939727783\n",
      "test loss 52.98202436797473\n",
      "================================================== gen= 7 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.63111209869385  Time:  0.029980182647705078\n",
      "mean acc =  tensor(1.2928)\n",
      "train loss 61.63111209869385\n",
      "test loss 53.090113477642035\n",
      "================================================== gen= 7 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  115.3358039855957  Time:  0.02452993392944336\n",
      "mean acc =  tensor(0.0268)\n",
      "train loss 115.3358039855957\n",
      "test loss 112.86129906063988\n",
      "================================================== gen= 7 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  95.10360527038574  Time:  0.02483987808227539\n",
      "mean acc =  tensor(0.1431)\n",
      "train loss 95.10360527038574\n",
      "test loss 100.22271328880673\n",
      "================================================== gen= 7 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  85.39537811279297  Time:  0.024990081787109375\n",
      "mean acc =  tensor(0.7039)\n",
      "train loss 85.39537811279297\n",
      "test loss 65.51306666160116\n",
      "================================================== gen= 7 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  77.90630149841309  Time:  0.026752948760986328\n",
      "mean acc =  tensor(1.3651)\n",
      "train loss 77.90630149841309\n",
      "test loss 59.35518796752099\n",
      "================================================== gen= 7 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.8765172958374  Time:  0.025664091110229492\n",
      "mean acc =  tensor(0.8411)\n",
      "train loss 68.8765172958374\n",
      "test loss 57.7184352485501\n",
      "================================================== gen= 7 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  54.75073051452637  Time:  0.024560928344726562\n",
      "mean acc =  tensor(0.8686)\n",
      "train loss 54.75073051452637\n",
      "test loss 57.953822881997034\n",
      "================================================== gen= 7 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.1859712600708  Time:  0.023833036422729492\n",
      "mean acc =  tensor(1.1040)\n",
      "train loss 63.1859712600708\n",
      "test loss 53.79560273358611\n",
      "================================================== gen= 7 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.9999885559082  Time:  0.02452397346496582\n",
      "mean acc =  tensor(1.2359)\n",
      "train loss 57.9999885559082\n",
      "test loss 53.404993537331926\n",
      "================================================== gen= 7 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.01114654541016  Time:  0.03488516807556152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0894)\n",
      "train loss 102.01114654541016\n",
      "test loss 106.9865255031456\n",
      "================================================== gen= 7 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  100.87567329406738  Time:  0.03150510787963867\n",
      "mean acc =  tensor(0.8102)\n",
      "train loss 100.87567329406738\n",
      "test loss 58.35680750113766\n",
      "================================================== gen= 7 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.54125499725342  Time:  0.029703855514526367\n",
      "mean acc =  tensor(0.9632)\n",
      "train loss 71.54125499725342\n",
      "test loss 55.46180748452946\n",
      "================================================== gen= 7 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.372703552246094  Time:  0.029778003692626953\n",
      "mean acc =  tensor(0.6409)\n",
      "train loss 60.372703552246094\n",
      "test loss 64.1294728493204\n",
      "================================================== gen= 7 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.3173360824585  Time:  0.029642105102539062\n",
      "mean acc =  tensor(1.0158)\n",
      "train loss 66.3173360824585\n",
      "test loss 54.92041809704839\n",
      "================================================== gen= 7 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.70401477813721  Time:  0.03244900703430176\n",
      "mean acc =  tensor(1.3138)\n",
      "train loss 57.70401477813721\n",
      "test loss 55.617394869019385\n",
      "================================================== gen= 7 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.43659019470215  Time:  0.028573989868164062\n",
      "mean acc =  tensor(0.8771)\n",
      "train loss 59.43659019470215\n",
      "test loss 55.10106407217428\n",
      "================================================== gen= 7 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  51.502553939819336  Time:  0.03147482872009277\n",
      "mean acc =  tensor(1.0548)\n",
      "train loss 51.502553939819336\n",
      "test loss 52.88614236740839\n",
      "================================================== gen= 7 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.64749526977539  Time:  0.031100034713745117\n",
      "mean acc =  tensor(0.0651)\n",
      "train loss 111.64749526977539\n",
      "test loss 108.64178487557132\n",
      "================================================== gen= 7 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  104.74129104614258  Time:  0.034605979919433594\n",
      "mean acc =  tensor(0.4240)\n",
      "train loss 104.74129104614258\n",
      "test loss 76.7916259765625\n",
      "================================================== gen= 7 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  86.74910545349121  Time:  0.0315096378326416\n",
      "mean acc =  tensor(1.4750)\n",
      "train loss 86.74910545349121\n",
      "test loss 59.979389683729934\n",
      "================================================== gen= 7 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.21836566925049  Time:  0.03281593322753906\n",
      "mean acc =  tensor(0.7132)\n",
      "train loss 60.21836566925049\n",
      "test loss 60.62089325781582\n",
      "================================================== gen= 7 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.39226913452148  Time:  0.030174970626831055\n",
      "mean acc =  tensor(0.7813)\n",
      "train loss 64.39226913452148\n",
      "test loss 57.10251168491078\n",
      "================================================== gen= 7 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.53974914550781  Time:  0.030399084091186523\n",
      "mean acc =  tensor(1.2476)\n",
      "train loss 58.53974914550781\n",
      "test loss 54.08276144014735\n",
      "================================================== gen= 7 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  55.48587703704834  Time:  0.030202150344848633\n",
      "mean acc =  tensor(1.1307)\n",
      "train loss 55.48587703704834\n",
      "test loss 52.15679991326365\n",
      "================================================== gen= 7 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.56353950500488  Time:  0.029925823211669922\n",
      "mean acc =  tensor(0.9474)\n",
      "train loss 58.56353950500488\n",
      "test loss 52.762770049426024\n",
      "================================================== gen= 7 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  107.49796295166016  Time:  0.02919483184814453\n",
      "mean acc =  tensor(0.0213)\n",
      "train loss 107.49796295166016\n",
      "test loss 113.3156805752086\n",
      "================================================== gen= 8 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.98432350158691  Time:  0.0332489013671875\n",
      "mean acc =  tensor(0.2583)\n",
      "train loss 112.98432350158691\n",
      "test loss 90.86749771014362\n",
      "================================================== gen= 8 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  79.60073471069336  Time:  0.029739856719970703\n",
      "mean acc =  tensor(1.9418)\n",
      "train loss 79.60073471069336\n",
      "test loss 82.11188084089837\n",
      "================================================== gen= 8 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.66162014007568  Time:  0.03495502471923828\n",
      "mean acc =  tensor(0.8177)\n",
      "train loss 69.66162014007568\n",
      "test loss 59.888874572961505\n",
      "================================================== gen= 8 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.62074661254883  Time:  0.02822089195251465\n",
      "mean acc =  tensor(0.5902)\n",
      "train loss 75.62074661254883\n",
      "test loss 67.30374319374967\n",
      "================================================== gen= 8 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.04075241088867  Time:  0.029294252395629883\n",
      "mean acc =  tensor(0.9569)\n",
      "train loss 63.04075241088867\n",
      "test loss 55.60613543971055\n",
      "================================================== gen= 8 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.28761577606201  Time:  0.029160022735595703\n",
      "mean acc =  tensor(1.2268)\n",
      "train loss 63.28761577606201\n",
      "test loss 54.32649329568253\n",
      "================================================== gen= 8 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.11039447784424  Time:  0.029192209243774414\n",
      "mean acc =  tensor(0.9545)\n",
      "train loss 65.11039447784424\n",
      "test loss 54.41791731646272\n",
      "================================================== gen= 8 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  123.9906997680664  Time:  0.024773836135864258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0233)\n",
      "train loss 123.9906997680664\n",
      "test loss 113.06615095073674\n",
      "================================================== gen= 8 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  117.21597480773926  Time:  0.02462291717529297\n",
      "mean acc =  tensor(0.1114)\n",
      "train loss 117.21597480773926\n",
      "test loss 103.33878689720517\n",
      "================================================== gen= 8 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.18808555603027  Time:  0.02437615394592285\n",
      "mean acc =  tensor(0.4953)\n",
      "train loss 98.18808555603027\n",
      "test loss 74.81267026005959\n",
      "================================================== gen= 8 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  74.33551406860352  Time:  0.026648998260498047\n",
      "mean acc =  tensor(1.4287)\n",
      "train loss 74.33551406860352\n",
      "test loss 59.233276860243606\n",
      "================================================== gen= 8 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.8034610748291  Time:  0.02486705780029297\n",
      "mean acc =  tensor(1.1257)\n",
      "train loss 59.8034610748291\n",
      "test loss 53.79216234375831\n",
      "================================================== gen= 8 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.05915641784668  Time:  0.02752685546875\n",
      "mean acc =  tensor(0.7886)\n",
      "train loss 61.05915641784668\n",
      "test loss 59.40487751344434\n",
      "================================================== gen= 8 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.76596355438232  Time:  0.024424314498901367\n",
      "mean acc =  tensor(0.9185)\n",
      "train loss 66.76596355438232\n",
      "test loss 54.20666908731266\n",
      "================================================== gen= 8 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.09889602661133  Time:  0.025548934936523438\n",
      "mean acc =  tensor(1.2206)\n",
      "train loss 62.09889602661133\n",
      "test loss 53.16991341519518\n",
      "================================================== gen= 8 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  118.98449325561523  Time:  0.029793977737426758\n",
      "mean acc =  tensor(0.0784)\n",
      "train loss 118.98449325561523\n",
      "test loss 107.21279720384247\n",
      "================================================== gen= 8 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.99496841430664  Time:  0.03304600715637207\n",
      "mean acc =  tensor(0.8344)\n",
      "train loss 102.99496841430664\n",
      "test loss 60.7625172154433\n",
      "================================================== gen= 8 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.85956287384033  Time:  0.03056192398071289\n",
      "mean acc =  tensor(1.3047)\n",
      "train loss 75.85956287384033\n",
      "test loss 56.344866927789184\n",
      "================================================== gen= 8 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.40717124938965  Time:  0.06907176971435547\n",
      "mean acc =  tensor(0.6369)\n",
      "train loss 67.40717124938965\n",
      "test loss 65.6421644249741\n",
      "================================================== gen= 8 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  70.2143611907959  Time:  0.02915787696838379\n",
      "mean acc =  tensor(0.6701)\n",
      "train loss 70.2143611907959\n",
      "test loss 60.97424908073581\n",
      "================================================== gen= 8 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.33370113372803  Time:  0.028976917266845703\n",
      "mean acc =  tensor(1.2747)\n",
      "train loss 62.33370113372803\n",
      "test loss 53.919786725725444\n",
      "================================================== gen= 8 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.95865345001221  Time:  0.03204989433288574\n",
      "mean acc =  tensor(1.0310)\n",
      "train loss 63.95865345001221\n",
      "test loss 52.422580796845104\n",
      "================================================== gen= 8 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  55.857802391052246  Time:  0.036206960678100586\n",
      "mean acc =  tensor(0.8733)\n",
      "train loss 55.857802391052246\n",
      "test loss 56.20968015502099\n",
      "================================================== gen= 8 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  123.52392768859863  Time:  0.033721208572387695\n",
      "mean acc =  tensor(0.0592)\n",
      "train loss 123.52392768859863\n",
      "test loss 109.57183251413359\n",
      "================================================== gen= 8 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.84180641174316  Time:  0.03046393394470215\n",
      "mean acc =  tensor(0.6724)\n",
      "train loss 102.84180641174316\n",
      "test loss 64.63956435845823\n",
      "================================================== gen= 8 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.96451950073242  Time:  0.03170013427734375\n",
      "mean acc =  tensor(1.1346)\n",
      "train loss 64.96451950073242\n",
      "test loss 55.22623383917776\n",
      "================================================== gen= 8 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.57289028167725  Time:  0.03335309028625488\n",
      "mean acc =  tensor(0.7985)\n",
      "train loss 66.57289028167725\n",
      "test loss 61.01149869127338\n",
      "================================================== gen= 8 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.382883071899414  Time:  0.03368067741394043\n",
      "mean acc =  tensor(1.1778)\n",
      "train loss 59.382883071899414\n",
      "test loss 53.984800766925424\n",
      "================================================== gen= 8 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.84270763397217  Time:  0.031662702560424805\n",
      "mean acc =  tensor(0.9721)\n",
      "train loss 60.84270763397217\n",
      "test loss 53.60249442794696\n",
      "================================================== gen= 8 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.01198959350586  Time:  0.029678821563720703\n",
      "mean acc =  tensor(0.9059)\n",
      "train loss 57.01198959350586\n",
      "test loss 55.99116499245572\n",
      "================================================== gen= 8 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.030747413635254  Time:  0.03135824203491211\n",
      "mean acc =  tensor(1.0223)\n",
      "train loss 58.030747413635254\n",
      "test loss 52.63767577190789\n",
      "================================================== gen= 8 index 7 vector= 3\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 124, 54, 103, 18, 19, 66, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=54, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=54, out_features=103, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=103, out_features=18, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=18, out_features=19, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=19, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.58150482177734  Time:  0.033483028411865234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(-0.0041)\n",
      "train loss 110.58150482177734\n",
      "test loss 116.08222265957164\n",
      "================================================== gen= 9 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  109.21502113342285  Time:  0.033551692962646484\n",
      "mean acc =  tensor(0.0245)\n",
      "train loss 109.21502113342285\n",
      "test loss 112.90518136575919\n",
      "================================================== gen= 9 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.49917793273926  Time:  0.03237318992614746\n",
      "mean acc =  tensor(0.1974)\n",
      "train loss 110.49917793273926\n",
      "test loss 96.04531642368862\n",
      "================================================== gen= 9 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  77.2363510131836  Time:  0.028367042541503906\n",
      "mean acc =  tensor(1.2988)\n",
      "train loss 77.2363510131836\n",
      "test loss 55.69128952545373\n",
      "================================================== gen= 9 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.60593891143799  Time:  0.02893996238708496\n",
      "mean acc =  tensor(1.0582)\n",
      "train loss 67.60593891143799\n",
      "test loss 54.64227484359222\n",
      "================================================== gen= 9 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.5545883178711  Time:  0.03099989891052246\n",
      "mean acc =  tensor(0.7010)\n",
      "train loss 64.5545883178711\n",
      "test loss 62.228189221855736\n",
      "================================================== gen= 9 index 5 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.0933780670166  Time:  0.029352903366088867\n",
      "mean acc =  tensor(1.2954)\n",
      "train loss 63.0933780670166\n",
      "test loss 55.509158102022546\n",
      "================================================== gen= 9 index 6 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.22553539276123  Time:  0.02883315086364746\n",
      "mean acc =  tensor(1.0398)\n",
      "train loss 60.22553539276123\n",
      "test loss 53.17691512334915\n",
      "================================================== gen= 9 index 7 vector= 0\n",
      "create_model =  [7, 10, 44, 8, 9, 31, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=9, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  113.47445869445801  Time:  0.025333881378173828\n",
      "mean acc =  tensor(0.0217)\n",
      "train loss 113.47445869445801\n",
      "test loss 113.26975753680378\n",
      "================================================== gen= 9 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  105.42180824279785  Time:  0.025604963302612305\n",
      "mean acc =  tensor(0.0659)\n",
      "train loss 105.42180824279785\n",
      "test loss 108.83107274892379\n",
      "================================================== gen= 9 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.23876953125  Time:  0.024391889572143555\n",
      "mean acc =  tensor(0.2495)\n",
      "train loss 101.23876953125\n",
      "test loss 92.05721028645833\n",
      "================================================== gen= 9 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.09069347381592  Time:  0.02240300178527832\n",
      "mean acc =  tensor(0.9791)\n",
      "train loss 78.09069347381592\n",
      "test loss 55.93620269152583\n",
      "================================================== gen= 9 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.07908630371094  Time:  0.027925968170166016\n",
      "mean acc =  tensor(1.2470)\n",
      "train loss 62.07908630371094\n",
      "test loss 56.14116219760609\n",
      "================================================== gen= 9 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.59824466705322  Time:  0.024435043334960938\n",
      "mean acc =  tensor(0.8396)\n",
      "train loss 65.59824466705322\n",
      "test loss 56.77391467451238\n",
      "================================================== gen= 9 index 5 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.56086730957031  Time:  0.04066586494445801\n",
      "mean acc =  tensor(0.9389)\n",
      "train loss 66.56086730957031\n",
      "test loss 54.52433880656755\n",
      "================================================== gen= 9 index 6 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.83808326721191  Time:  0.02387261390686035\n",
      "mean acc =  tensor(0.9805)\n",
      "train loss 66.83808326721191\n",
      "test loss 53.38657695744313\n",
      "================================================== gen= 9 index 7 vector= 1\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  129.19128608703613  Time:  0.03182101249694824\n",
      "mean acc =  tensor(0.0328)\n",
      "train loss 129.19128608703613\n",
      "test loss 111.89209534521817\n",
      "================================================== gen= 9 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.16604995727539  Time:  0.030061006546020508\n",
      "mean acc =  tensor(0.3596)\n",
      "train loss 98.16604995727539\n",
      "test loss 81.76013059032206\n",
      "================================================== gen= 9 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.26090145111084  Time:  0.031913042068481445\n",
      "mean acc =  tensor(1.8263)\n",
      "train loss 71.26090145111084\n",
      "test loss 89.17484947775498\n",
      "================================================== gen= 9 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  74.50691890716553  Time:  0.029464006423950195\n",
      "mean acc =  tensor(0.7254)\n",
      "train loss 74.50691890716553\n",
      "test loss 61.61033596635676\n",
      "================================================== gen= 9 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  79.41253566741943  Time:  0.030570030212402344\n",
      "mean acc =  tensor(0.5480)\n",
      "train loss 79.41253566741943\n",
      "test loss 70.88281854642491\n",
      "================================================== gen= 9 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.34446907043457  Time:  0.028734922409057617\n",
      "mean acc =  tensor(0.9048)\n",
      "train loss 75.34446907043457\n",
      "test loss 56.91612080165318\n",
      "================================================== gen= 9 index 5 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.01496696472168  Time:  0.032347917556762695\n",
      "mean acc =  tensor(1.4134)\n",
      "train loss 60.01496696472168\n",
      "test loss 58.05307523247336\n",
      "================================================== gen= 9 index 6 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.71146774291992  Time:  0.06340789794921875\n",
      "mean acc =  tensor(0.9640)\n",
      "train loss 61.71146774291992\n",
      "test loss 54.18351431606578\n",
      "================================================== gen= 9 index 7 vector= 2\n",
      "create_model =  [7, 134, 115, 140, 50, 114, 131, 42, 51, 28, 125, 94, 86, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=134, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=134, out_features=115, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=115, out_features=140, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=140, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=114, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=114, out_features=131, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=131, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=51, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=51, out_features=28, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=28, out_features=125, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=125, out_features=94, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=94, out_features=86, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=86, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  113.79437255859375  Time:  0.03013777732849121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0305)\n",
      "train loss 113.79437255859375\n",
      "test loss 112.48582925601882\n",
      "================================================== gen= 9 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  100.85870361328125  Time:  0.03299379348754883\n",
      "mean acc =  tensor(0.3081)\n",
      "train loss 100.85870361328125\n",
      "test loss 86.03997156571369\n",
      "================================================== gen= 9 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.91121196746826  Time:  0.029481172561645508\n",
      "mean acc =  tensor(1.3891)\n",
      "train loss 67.91121196746826\n",
      "test loss 56.34410736187785\n",
      "================================================== gen= 9 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  55.94496440887451  Time:  0.029805660247802734\n",
      "mean acc =  tensor(0.6946)\n",
      "train loss 55.94496440887451\n",
      "test loss 62.08015262837313\n",
      "================================================== gen= 9 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.14062213897705  Time:  0.03175497055053711\n",
      "mean acc =  tensor(0.9090)\n",
      "train loss 66.14062213897705\n",
      "test loss 56.380975321036615\n",
      "================================================== gen= 9 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  56.84836387634277  Time:  0.030560016632080078\n",
      "mean acc =  tensor(1.3986)\n",
      "train loss 56.84836387634277\n",
      "test loss 54.44854853104572\n",
      "================================================== gen= 9 index 5 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.81655693054199  Time:  0.03105616569519043\n",
      "mean acc =  tensor(0.9831)\n",
      "train loss 60.81655693054199\n",
      "test loss 53.892036619640535\n",
      "================================================== gen= 9 index 6 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.11108016967773  Time:  0.0313410758972168\n",
      "mean acc =  tensor(0.9900)\n",
      "train loss 66.11108016967773\n",
      "test loss 52.61513503716917\n",
      "================================================== gen= 9 index 7 vector= 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epoch =8\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "#model2,optimizer2 = create_model()\n",
    "acc1 =0\n",
    "acc2 =0\n",
    "\n",
    "arx = create_ar(8)\n",
    "best , ar1,ar2 = next_gen(arx)\n",
    "#This is the 4 models that we are working on \n",
    "loops = [best,ar1,ar2,arx]\n",
    "\n",
    "target_columns='DiscountPerc'\n",
    "#target_columns = 'perc'\n",
    "results =[]\n",
    "best_score =100000000\n",
    "best_index =-1\n",
    "a,b,best =[],[],[]\n",
    "for generations in range (10):\n",
    "    #plt.plot(Test_acc)\n",
    "    #Loop over the models and choose the best one.\n",
    "    for index in range(4):\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        model ,optimizer ,ar = create_model(loops[index])\n",
    "        temp_model = model\n",
    "        temp_optimizer = optimizer\n",
    "       \n",
    "        for i in range(n_epoch):\n",
    "            #print (df_train.head())\n",
    "            train_loader = create_set(1000,df_train,target_columns)\n",
    "            #print(df_train.head())\n",
    "            train_loss = train_epoch(temp_model,train_loader,criterion,temp_optimizer)\n",
    "            test_loss ,acc= test_epoch(temp_model,validate_loader,criterion)\n",
    "            \n",
    "            #Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "            print(\"train loss\",train_loss)\n",
    "            print(\"test loss\" , test_loss)\n",
    "            #Test_acc.append(test_acc)\n",
    "            print('='*50,'gen=',generations,'index',i,'vector=',index)\n",
    "        \n",
    "        if (test_loss < best_score):\n",
    "            print (\"updating model ======= \", test_loss)\n",
    "            best_model = temp_model\n",
    "            best_optimizer = temp_optimizer\n",
    "            best_score = test_loss\n",
    "            best_index = index\n",
    "        \n",
    "        results.append(test_loss)\n",
    "    \n",
    "    \n",
    "    a,b,best = next_gen(loops[index])\n",
    "    c=create_ar(8)\n",
    "    results =[]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue to train with the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target column is  DiscountPerc\n",
      "Train Loss:  43.509727867282166  Time:  1.2682158946990967\n",
      "mean acc =  tensor(1.4997)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  34.828976709015514  Time:  1.2442409992218018\n",
      "mean acc =  tensor(1.4480)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  34.36869135681464  Time:  1.3505358695983887\n",
      "mean acc =  tensor(1.4616)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  33.553925446101594  Time:  1.26761794090271\n",
      "mean acc =  tensor(1.4707)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  33.14901985440935  Time:  1.2816102504730225\n",
      "mean acc =  tensor(1.3923)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.54704367384619  Time:  1.3060078620910645\n",
      "mean acc =  tensor(1.4893)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.45576979189503  Time:  1.291234016418457\n",
      "mean acc =  tensor(1.4279)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.58317809202233  Time:  1.3442602157592773\n",
      "mean acc =  tensor(1.5148)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.427440156742016  Time:  1.3035707473754883\n",
      "mean acc =  tensor(1.5381)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.44544487583394  Time:  1.2459230422973633\n",
      "mean acc =  tensor(1.3813)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.248814962348156  Time:  1.2844281196594238\n",
      "mean acc =  tensor(1.4291)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.586848015687902  Time:  1.2719359397888184\n",
      "mean acc =  tensor(1.3563)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.29862119713608  Time:  1.234144926071167\n",
      "mean acc =  tensor(1.4853)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.27003003626454  Time:  1.271874189376831\n",
      "mean acc =  tensor(1.4182)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.372363917681636  Time:  1.3546319007873535\n",
      "mean acc =  tensor(1.4179)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.4078175680978  Time:  1.2962470054626465\n",
      "mean acc =  tensor(1.3669)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.560655175423136  Time:  1.30849289894104\n",
      "mean acc =  tensor(1.3751)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.513909320442043  Time:  1.2568058967590332\n",
      "mean acc =  tensor(1.5274)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.564454010554723  Time:  1.2917418479919434\n",
      "mean acc =  tensor(1.3534)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.152951318390514  Time:  1.2383019924163818\n",
      "mean acc =  tensor(1.4216)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.45317689739928  Time:  1.253819227218628\n",
      "mean acc =  tensor(1.3461)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.9869141675988  Time:  1.3328158855438232\n",
      "mean acc =  tensor(1.5915)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.851048124079803  Time:  1.3047699928283691\n",
      "mean acc =  tensor(1.3905)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.99665795540323  Time:  1.3156888484954834\n",
      "mean acc =  tensor(1.4284)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.558927905802825  Time:  1.2802860736846924\n",
      "mean acc =  tensor(1.4418)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.576600726769897  Time:  1.2869172096252441\n",
      "mean acc =  tensor(1.5136)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.287634752234634  Time:  1.3061230182647705\n",
      "mean acc =  tensor(1.5041)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.411279765927063  Time:  1.2737421989440918\n",
      "mean acc =  tensor(1.4901)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.59448816338364  Time:  1.2928640842437744\n",
      "mean acc =  tensor(1.3904)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.999285688205642  Time:  1.322033166885376\n",
      "mean acc =  tensor(1.5193)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.669630488570856  Time:  1.2329277992248535\n",
      "mean acc =  tensor(1.3758)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.966564859662736  Time:  1.2621681690216064\n",
      "mean acc =  tensor(1.3866)\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.182269008792176  Time:  1.3462369441986084\n",
      "mean acc =  tensor(1.4750)\n",
      "[tensor(1.4997), tensor(1.4480), tensor(1.4616), tensor(1.4707), tensor(1.3923), tensor(1.4893), tensor(1.4279), tensor(1.5148), tensor(1.5381), tensor(1.3813), tensor(1.4291), tensor(1.3563), tensor(1.4853), tensor(1.4182), tensor(1.4179), tensor(1.3669), tensor(1.3751), tensor(1.5274), tensor(1.3534), tensor(1.4216), tensor(1.3461), tensor(1.5915), tensor(1.3905), tensor(1.4284), tensor(1.4418), tensor(1.5136), tensor(1.5041), tensor(1.4901), tensor(1.3904), tensor(1.5193), tensor(1.3758), tensor(1.3866), tensor(1.4750)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(33):\n",
    "            train_loader = create_set(50000,df_train,target_columns)\n",
    "            train_loss = train_epoch(best_model,train_loader,criterion,best_optimizer)\n",
    "            test_loss ,acc= test_epoch(best_model,validate_loader,criterion)\n",
    "\n",
    "            Test_acc.append(acc)\n",
    "            Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "      \n",
    "            #print('='*50,i)\n",
    "\n",
    "print (Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.4997), tensor(1.4480), tensor(1.4616), tensor(1.4707), tensor(1.3923), tensor(1.4893), tensor(1.4279), tensor(1.5148), tensor(1.5381), tensor(1.3813), tensor(1.4291), tensor(1.3563), tensor(1.4853), tensor(1.4182), tensor(1.4179), tensor(1.3669), tensor(1.3751), tensor(1.5274), tensor(1.3534), tensor(1.4216), tensor(1.3461), tensor(1.5915), tensor(1.3905), tensor(1.4284), tensor(1.4418), tensor(1.5136), tensor(1.5041), tensor(1.4901), tensor(1.3904), tensor(1.5193), tensor(1.3758), tensor(1.3866), tensor(1.4750)]\n"
     ]
    }
   ],
   "source": [
    "print (Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9543a650>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss ')\n",
    "plt.plot(Test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba7733fb10>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss ')\n",
    "plt.plot(Train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba7739e610>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_epoch(model,data):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        results = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        for batch_idx, data in enumerate(data):\n",
    "          \n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            print (outputs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.6421])\n",
      "tensor([8.1925])\n",
      "tensor([8.3114])\n",
      "tensor([9.7588])\n",
      "tensor([8.1506])\n",
      "tensor([8.1409])\n",
      "tensor([8.4337])\n",
      "tensor([8.2220])\n",
      "tensor([7.9009])\n",
      "tensor([10.1017])\n",
      "tensor([8.3710])\n",
      "tensor([8.7853])\n",
      "tensor([8.4291])\n",
      "tensor([9.0499])\n",
      "tensor([8.7486])\n",
      "tensor([8.8020])\n",
      "tensor([8.1786])\n",
      "tensor([14.3878])\n",
      "tensor([8.1435])\n",
      "tensor([9.1003])\n",
      "tensor([8.4362])\n",
      "tensor([8.4697])\n",
      "tensor([9.6469])\n",
      "tensor([8.7623])\n",
      "tensor([8.8943])\n",
      "tensor([9.4030])\n",
      "tensor([19.2607])\n",
      "tensor([8.1668])\n",
      "tensor([8.4914])\n",
      "tensor([8.1341])\n",
      "tensor([8.4183])\n",
      "tensor([17.3899])\n",
      "tensor([8.0846])\n",
      "tensor([14.6154])\n",
      "tensor([12.7358])\n",
      "tensor([9.7158])\n",
      "tensor([8.1588])\n",
      "tensor([9.1655])\n",
      "tensor([8.4164])\n",
      "tensor([11.8003])\n",
      "tensor([16.1549])\n",
      "tensor([7.6640])\n",
      "tensor([12.8964])\n",
      "tensor([9.7227])\n",
      "tensor([6.5389])\n",
      "tensor([16.4680])\n",
      "tensor([8.0675])\n",
      "tensor([6.5562])\n",
      "tensor([17.5450])\n",
      "tensor([8.5339])\n",
      "tensor([8.2651])\n",
      "tensor([8.4301])\n",
      "tensor([18.6262])\n",
      "tensor([9.4792])\n",
      "tensor([8.2849])\n",
      "tensor([8.9451])\n",
      "tensor([6.3285])\n",
      "tensor([9.8664])\n",
      "tensor([17.9167])\n",
      "tensor([8.6161])\n",
      "tensor([8.2659])\n",
      "tensor([6.5284])\n",
      "tensor([8.4346])\n",
      "tensor([11.1844])\n",
      "tensor([8.2220])\n",
      "tensor([8.5263])\n",
      "tensor([8.7029])\n",
      "tensor([8.4940])\n",
      "tensor([9.1622])\n",
      "tensor([8.2205])\n",
      "tensor([9.0055])\n",
      "tensor([8.2146])\n",
      "tensor([9.3267])\n",
      "tensor([16.3996])\n",
      "tensor([7.8043])\n",
      "tensor([8.9639])\n",
      "tensor([11.4906])\n",
      "tensor([8.1622])\n",
      "tensor([9.1732])\n",
      "tensor([8.6265])\n",
      "tensor([7.9464])\n",
      "tensor([8.9204])\n",
      "tensor([10.8706])\n",
      "tensor([12.5743])\n",
      "tensor([8.7102])\n",
      "tensor([11.6891])\n",
      "tensor([9.8506])\n",
      "tensor([8.1387])\n",
      "tensor([8.1524])\n",
      "tensor([8.7618])\n",
      "tensor([8.3319])\n",
      "tensor([17.9854])\n",
      "tensor([8.7703])\n",
      "tensor([8.4745])\n",
      "tensor([9.2807])\n",
      "tensor([9.1486])\n",
      "tensor([8.1368])\n",
      "tensor([18.1238])\n",
      "tensor([8.5042])\n",
      "tensor([8.2522])\n"
     ]
    }
   ],
   "source": [
    "X = df_train.sample(n=10000)\n",
    "\n",
    "X.describe()\n",
    "\n",
    "newset = create_set2(100,X,target_columns)\n",
    "submit_epoch(best_model,newset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the learning algorithm in this notebook assume we can learn some data relevant for all hotels and \n",
    "## create neural network that predict by the delta time from the checkin date and hotel number and discount code and to\n",
    "## get the discount percentage "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader.data as web\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Zeev\n",
    "## 033363870\n",
    "## Assaf\n",
    "## 204249197\n",
    "### setting and Downloading the tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = pd.read_csv('/Users/bebik/Documents/hotels_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/17/2015 0:00\n",
      "<class 'datetime.timedelta'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "date_time_str = '2018/8/04'\n",
    "print(hotels['Snapshot Date'][0])\n",
    "#for index, row in hotels.iterrows():\n",
    "tabletime = datetime.datetime.strptime( hotels['Snapshot Date'][0] , '%m/%d/%Y %H:%M')\n",
    "now = datetime.datetime.strptime( date_time_str , '%Y/%m/%d')\n",
    "newdt = tabletime - now\n",
    "print (type(newdt))\n",
    "print ( np.timedelta64(1, 'D').astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hotels['Days'] #always 5\n",
    "del hotels['Snapshot ID']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars  \n",
       "count  187848.000000  \n",
       "mean        3.905823  \n",
       "std         0.846496  \n",
       "min         0.000000  \n",
       "25%         3.000000  \n",
       "50%         4.000000  \n",
       "75%         4.000000  \n",
       "max         5.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars  \n",
       "count  187848.000000  \n",
       "mean        3.905823  \n",
       "std         0.846496  \n",
       "min         0.000000  \n",
       "25%         3.000000  \n",
       "50%         4.000000  \n",
       "75%         4.000000  \n",
       "max         5.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels['SnapDate']= pd.to_datetime(hotels['Snapshot Date']) \n",
    "hotels['CheckingDate']= pd.to_datetime(hotels['Checkin Date']) \n",
    "hotels['weekday']= hotels['CheckingDate'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels['DayDiff'] = hotels['CheckingDate']-hotels['SnapDate']\n",
    "hotels['Discount'] = hotels['Original Price']-hotels['Discount Price']\n",
    "hotels['DiscountPerc'] = hotels['Discount']/hotels['Original Price']\n",
    "#hotels['delta_days']= (hotels['CheckingDate']- now).astype(int)/8.636207/10**13\n",
    "hotels['delta_days']=(pd.to_datetime(hotels['Checkin Date']) - pd.to_datetime(hotels['Snapshot Date'])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DayDiff</th>\n",
       "      <th>Discount</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "      <td>2.917763</td>\n",
       "      <td>17 days 11:10:35.185894</td>\n",
       "      <td>152.569860</td>\n",
       "      <td>0.088777</td>\n",
       "      <td>17.465685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "      <td>1.840536</td>\n",
       "      <td>10 days 00:57:55.438813</td>\n",
       "      <td>143.316985</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>10.040225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9 days 00:00:00</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.048450</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18 days 00:00:00</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.070562</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>26 days 00:00:00</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0.109339</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>34 days 00:00:00</td>\n",
       "      <td>3760.000000</td>\n",
       "      <td>0.684258</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars        weekday                  DayDiff       Discount  \\\n",
       "count  187848.000000  187848.000000                   187848  187848.000000   \n",
       "mean        3.905823       2.917763  17 days 11:10:35.185894     152.569860   \n",
       "std         0.846496       1.840536  10 days 00:57:55.438813     143.316985   \n",
       "min         0.000000       0.000000          1 days 00:00:00      15.000000   \n",
       "25%         3.000000       2.000000          9 days 00:00:00      70.000000   \n",
       "50%         4.000000       3.000000         18 days 00:00:00     103.000000   \n",
       "75%         4.000000       4.000000         26 days 00:00:00     180.000000   \n",
       "max         5.000000       6.000000         34 days 00:00:00    3760.000000   \n",
       "\n",
       "        DiscountPerc     delta_days  \n",
       "count  187848.000000  187848.000000  \n",
       "mean        0.088777      17.465685  \n",
       "std         0.060302      10.040225  \n",
       "min         0.005814       1.000000  \n",
       "25%         0.048450       9.000000  \n",
       "50%         0.070562      18.000000  \n",
       "75%         0.109339      26.000000  \n",
       "max         0.684258      34.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Createing Dendogram\n",
    "1. calculating the 40 hotels with most checking information\n",
    "2. add the dates of the 40 hotels to the selected dates\n",
    "3. create df_df with the 40 X 40 (dates*codes) * hotel ids\n",
    "4. hotel ids and names are in the hotels DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "554 554\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "vals = numpy.unique(hotels['Hotel Name'].values)\n",
    "checkins = numpy.unique(hotels['Checkin Date'].values)\n",
    "hotels_num = numpy.unique(hotels['Hotel Name'].values)\n",
    "print (type(hotels_num[0]))\n",
    "#print(checkins,len(checkins))             \n",
    "#print(hotels_num,len(hotels_num))             \n",
    "numbers = []\n",
    "for i in range(len(vals)) :\n",
    "    numbers.append(i)\n",
    "print (len(vals),len(numbers) )\n",
    "hotels['hotel']=hotels['Hotel Name']\n",
    "hotels['hotel'].replace(to_replace=vals, value=numbers,inplace = True)\n",
    "hotels.head()\n",
    "\n",
    "lens = []\n",
    "for date in checkins:\n",
    "    xdf = hotels['Checkin Date']==date\n",
    "    lens.append (len(hotels[xdf]))\n",
    "lens.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1307\n",
      "65626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DayDiff</th>\n",
       "      <th>Discount</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1860.317816</td>\n",
       "      <td>1702.353077</td>\n",
       "      <td>2.438881</td>\n",
       "      <td>15.247783</td>\n",
       "      <td>3.904550</td>\n",
       "      <td>2.861229</td>\n",
       "      <td>17 days 13:30:13.659220</td>\n",
       "      <td>157.964740</td>\n",
       "      <td>0.088780</td>\n",
       "      <td>17.562658</td>\n",
       "      <td>288.245192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>991.594738</td>\n",
       "      <td>931.516379</td>\n",
       "      <td>0.996660</td>\n",
       "      <td>41.035860</td>\n",
       "      <td>0.826034</td>\n",
       "      <td>1.099896</td>\n",
       "      <td>10 days 00:09:07.970866</td>\n",
       "      <td>145.354637</td>\n",
       "      <td>0.057369</td>\n",
       "      <td>10.006342</td>\n",
       "      <td>147.396464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1242.000000</td>\n",
       "      <td>1117.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9 days 00:00:00</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.049460</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1670.000000</td>\n",
       "      <td>1527.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18 days 00:00:00</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.072452</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>286.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2156.000000</td>\n",
       "      <td>1966.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>26 days 00:00:00</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10625.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>34 days 00:00:00</td>\n",
       "      <td>2900.000000</td>\n",
       "      <td>0.683057</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>553.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count    65626.000000    65626.000000   65626.000000     65626.000000   \n",
       "mean      1860.317816     1702.353077       2.438881        15.247783   \n",
       "std        991.594738      931.516379       0.996660        41.035860   \n",
       "min        300.000000      281.000000       1.000000        -1.000000   \n",
       "25%       1242.000000     1117.000000       2.000000        -1.000000   \n",
       "50%       1670.000000     1527.000000       2.000000        -1.000000   \n",
       "75%       2156.000000     1966.000000       3.000000         8.000000   \n",
       "max      10625.000000    10500.000000       4.000000       383.000000   \n",
       "\n",
       "        Hotel Stars       weekday                  DayDiff      Discount  \\\n",
       "count  65626.000000  65626.000000                    65626  65626.000000   \n",
       "mean       3.904550      2.861229  17 days 13:30:13.659220    157.964740   \n",
       "std        0.826034      1.099896  10 days 00:09:07.970866    145.354637   \n",
       "min        0.000000      1.000000          1 days 00:00:00     16.000000   \n",
       "25%        3.000000      2.000000          9 days 00:00:00     74.000000   \n",
       "50%        4.000000      3.000000         18 days 00:00:00    112.000000   \n",
       "75%        4.000000      4.000000         26 days 00:00:00    190.000000   \n",
       "max        5.000000      5.000000         34 days 00:00:00   2900.000000   \n",
       "\n",
       "       DiscountPerc    delta_days         hotel  \n",
       "count  65626.000000  65626.000000  65626.000000  \n",
       "mean       0.088780     17.562658    288.245192  \n",
       "std        0.057369     10.006342    147.396464  \n",
       "min        0.010265      1.000000      0.000000  \n",
       "25%        0.049460      9.000000    175.000000  \n",
       "50%        0.072452     18.000000    286.000000  \n",
       "75%        0.111111     26.000000    396.000000  \n",
       "max        0.683057     34.000000    553.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (lens[len(lens) - 41])\n",
    "bar = lens[len(lens) - 41]\n",
    "cnt =0\n",
    "lista =[]\n",
    "selected_dates = [] \n",
    "df_top40 = pd.DataFrame()\n",
    "for date in checkins:\n",
    "    xdf = hotels['Checkin Date']==date\n",
    "    if (len(hotels[xdf]) > bar):\n",
    "        selected_dates.append(date)\n",
    "        cnt+=len(hotels[xdf])\n",
    "        df_top40 = df_top40.append(hotels[xdf])\n",
    "\n",
    "print(cnt)\n",
    "df_top40.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10/1/2015 0:00', '10/14/2015 0:00', '10/15/2015 0:00', '10/16/2015 0:00', '10/2/2015 0:00', '10/21/2015 0:00', '10/22/2015 0:00', '10/27/2015 0:00', '10/28/2015 0:00', '10/29/2015 0:00', '10/30/2015 0:00', '10/31/2015 0:00', '10/7/2015 0:00', '11/10/2015 0:00', '11/11/2015 0:00', '11/12/2015 0:00', '11/13/2015 0:00', '11/18/2015 0:00', '11/21/2015 0:00', '11/26/2015 0:00', '11/27/2015 0:00', '11/28/2015 0:00', '11/3/2015 0:00', '11/4/2015 0:00', '11/5/2015 0:00', '11/6/2015 0:00', '11/7/2015 0:00', '12/30/2015 0:00', '8/12/2015 0:00', '8/19/2015 0:00', '8/26/2015 0:00', '8/27/2015 0:00', '8/28/2015 0:00', '9/10/2015 0:00', '9/11/2015 0:00', '9/16/2015 0:00', '9/17/2015 0:00', '9/18/2015 0:00', '9/30/2015 0:00', '9/9/2015 0:00']\n"
     ]
    }
   ],
   "source": [
    "print(selected_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "our_columns =[]\n",
    "for date in selected_dates:\n",
    "    our_columns.append(date+'_1')\n",
    "    our_columns.append(date+'_2')\n",
    "    our_columns.append(date+'_3')\n",
    "    our_columns.append(date+'_4')\n",
    "dandogram_df = pd.DataFrame(columns=our_columns)\n",
    "print (len(our_columns))\n",
    "\n",
    "#print ((our_columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Original Price  Available Rooms        weekday   DiscountPerc  \\\n",
      "count   187848.000000    187848.000000  187848.000000  187848.000000   \n",
      "mean      1825.742143        14.588561       2.917763       0.088777   \n",
      "std       1042.355712        41.575482       1.840536       0.060302   \n",
      "min        289.000000        -1.000000       0.000000       0.005814   \n",
      "25%       1160.000000        -1.000000       2.000000       0.048450   \n",
      "50%       1599.000000        -1.000000       3.000000       0.070562   \n",
      "75%       2160.000000         8.000000       4.000000       0.109339   \n",
      "max      29975.000000       431.000000       6.000000       0.684258   \n",
      "\n",
      "          delta_days          hotel     days_delta  \n",
      "count  187848.000000  187848.000000  187848.000000  \n",
      "mean       17.465685     289.105655      17.465685  \n",
      "std        10.040225     146.185570      10.040225  \n",
      "min         1.000000       0.000000       1.000000  \n",
      "25%         9.000000     176.000000       9.000000  \n",
      "50%        18.000000     285.000000      18.000000  \n",
      "75%        26.000000     397.000000      26.000000  \n",
      "max        34.000000     553.000000      34.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X, y = make_classification(random_state=0)\n",
    "X = hotels.copy()\n",
    "y = X['Discount Code']\n",
    "del X['Hotel Stars']\n",
    "del X['Discount Code']\n",
    "\n",
    "#del X['DiscountPerc']\n",
    "\n",
    "\n",
    "\n",
    "X['days_delta'] = X['DayDiff'].astype('timedelta64[D]')\n",
    "del X['DayDiff']\n",
    "del X['Snapshot Date']\n",
    "del X['Checkin Date']\n",
    "del X['Hotel Name']\n",
    "del X['SnapDate']\n",
    "del X['CheckingDate']\n",
    "del X['Discount Price']\n",
    "del X['Discount']\n",
    "\n",
    "print(X.describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "#X, y = load_iris(return_X_y=True)\n",
    "#clf2 = tree.DecisionTreeClassifier()\n",
    "#clf2 = clf2.fit(X_train, y_train)\n",
    "#clf2 = clf2.predict(X_test)\n",
    "#tree.plot_tree(clf2) \n",
    "#print(clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.plot_tree(clf2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test=X_test.append([2000,2,3,4,5])\n",
    "#print (len(X_test))\n",
    "#print (clf.predict(X_test[:1]))\n",
    "#print (X_test[:1])\n",
    "\n",
    "#array([1, 0])\n",
    "#clf.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "pic_size = 256\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self,size_list):\n",
    "        super(Simple_MLP,self).__init__()\n",
    "        layers=[]\n",
    "        self.size_list = size_list\n",
    "        for i in range(len(size_list) -2):\n",
    "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "            op = random.randint(0,2) \n",
    "            \n",
    "            layers.append(torch.nn.PReLU())\n",
    "        #layers.append(nn.Linear(size_list[-3],size_list[-2]))\n",
    "        #layers.append(nn.Softmax(dim=1))\n",
    "        layers.append(nn.Linear(size_list[-2],size_list[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelsDataset(data.Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index].float()\n",
    "        Y = self.Y[index].long()\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    #device = get_device()\n",
    "    return torch.from_numpy(df.values)\n",
    "    \n",
    "def create_set(num,df,target_col):\n",
    "    print(' target column is ',target_col)\n",
    "    df_train_sample = df.copy()\n",
    "    if (num != 0):\n",
    "        df_train_sample = df_train_sample.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[target_col] )\n",
    "    ten_train_target = torch.from_numpy(target_sample[target_col].values)\n",
    "    #print(\"train_target \",ten_train_target)\n",
    "    del df_train_sample[target_col]\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "\n",
    "    #print(ten_train_target)\n",
    "    train_dataset = HotelsDataset(ten_train_data,ten_train_target)\n",
    "\n",
    "\n",
    "    train_loader_args = dict(shuffle=True,batch_size=pic_size)\n",
    "    train_loader = data.DataLoader(train_dataset,**train_loader_args)\n",
    "    return train_loader\n",
    "\n",
    "def create_set2(num,df,target_col):\n",
    "    df_train_sample = df.copy()\n",
    "    if (num != 0):\n",
    "        df_train_sample = df_train_sample.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[target_col] )\n",
    "    ten_train_target = torch.from_numpy(target_sample[target_col].values)\n",
    "    \n",
    "    del df_train_sample[target_col]\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "    return ten_train_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model =  [7, 83, 121, 103, 73, 57, 81, 86, 57, 106, 120, 108, 76, 77, 95, 130, 29, 99, 43, 107, 101, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=83, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=83, out_features=121, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=121, out_features=103, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=103, out_features=73, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=73, out_features=57, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=57, out_features=81, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=81, out_features=86, bias=True)\n",
      "    (13): PReLU(num_parameters=1)\n",
      "    (14): Linear(in_features=86, out_features=57, bias=True)\n",
      "    (15): PReLU(num_parameters=1)\n",
      "    (16): Linear(in_features=57, out_features=106, bias=True)\n",
      "    (17): PReLU(num_parameters=1)\n",
      "    (18): Linear(in_features=106, out_features=120, bias=True)\n",
      "    (19): PReLU(num_parameters=1)\n",
      "    (20): Linear(in_features=120, out_features=108, bias=True)\n",
      "    (21): PReLU(num_parameters=1)\n",
      "    (22): Linear(in_features=108, out_features=76, bias=True)\n",
      "    (23): PReLU(num_parameters=1)\n",
      "    (24): Linear(in_features=76, out_features=77, bias=True)\n",
      "    (25): PReLU(num_parameters=1)\n",
      "    (26): Linear(in_features=77, out_features=95, bias=True)\n",
      "    (27): PReLU(num_parameters=1)\n",
      "    (28): Linear(in_features=95, out_features=130, bias=True)\n",
      "    (29): PReLU(num_parameters=1)\n",
      "    (30): Linear(in_features=130, out_features=29, bias=True)\n",
      "    (31): PReLU(num_parameters=1)\n",
      "    (32): Linear(in_features=29, out_features=99, bias=True)\n",
      "    (33): PReLU(num_parameters=1)\n",
      "    (34): Linear(in_features=99, out_features=43, bias=True)\n",
      "    (35): PReLU(num_parameters=1)\n",
      "    (36): Linear(in_features=43, out_features=107, bias=True)\n",
      "    (37): PReLU(num_parameters=1)\n",
      "    (38): Linear(in_features=107, out_features=101, bias=True)\n",
      "    (39): PReLU(num_parameters=1)\n",
      "    (40): Linear(in_features=101, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([7,\n",
       "  83,\n",
       "  121,\n",
       "  103,\n",
       "  73,\n",
       "  57,\n",
       "  81,\n",
       "  86,\n",
       "  57,\n",
       "  106,\n",
       "  120,\n",
       "  29,\n",
       "  54,\n",
       "  113,\n",
       "  77,\n",
       "  25,\n",
       "  43,\n",
       "  95,\n",
       "  5,\n",
       "  5,\n",
       "  38,\n",
       "  1],\n",
       " [7,\n",
       "  125,\n",
       "  57,\n",
       "  59,\n",
       "  41,\n",
       "  55,\n",
       "  29,\n",
       "  110,\n",
       "  52,\n",
       "  136,\n",
       "  14,\n",
       "  108,\n",
       "  76,\n",
       "  77,\n",
       "  95,\n",
       "  130,\n",
       "  29,\n",
       "  99,\n",
       "  43,\n",
       "  107,\n",
       "  101,\n",
       "  1],\n",
       " [7,\n",
       "  83,\n",
       "  121,\n",
       "  103,\n",
       "  73,\n",
       "  57,\n",
       "  81,\n",
       "  86,\n",
       "  57,\n",
       "  106,\n",
       "  120,\n",
       "  108,\n",
       "  76,\n",
       "  77,\n",
       "  95,\n",
       "  130,\n",
       "  29,\n",
       "  99,\n",
       "  43,\n",
       "  107,\n",
       "  101,\n",
       "  1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "#criterion = nn.L1Loss()\n",
    "first_round = 7\n",
    "#first_round = 5\n",
    "\n",
    "\n",
    "end_round  = 1\n",
    "device = torch.device( \"cpu\")\n",
    "def create_ar(lens):\n",
    "    \n",
    "    r1 = 160\n",
    "    r2 = 160\n",
    "    ar = [first_round]\n",
    "    for i in range(random.randint(1,lens)):\n",
    "        r1 = random.randint(4,140)\n",
    "        ar.append( r1  )\n",
    "        r2 = random.randint(4,140)\n",
    "        ar.append(  r2 )\n",
    "    ar.append(end_round)\n",
    "    return ar\n",
    "\n",
    "\n",
    "#create next generations\n",
    "def next_gen(best_ar):\n",
    "    \n",
    "    ar_left = []\n",
    "    ar_right = []\n",
    "    for index,item in enumerate(best_ar):\n",
    "        r1 = random.randint(4,140)\n",
    "        if (index is 0 or index is len(best_ar)-1):\n",
    "            r1=item\n",
    "        if (index < len(best_ar)/2 ):\n",
    "            ar_left.append(item)\n",
    "            ar_right.append(r1)\n",
    "        else:\n",
    "            ar_left.append(r1)\n",
    "            ar_right.append(item)\n",
    "    \n",
    "    return ar_left,ar_right,best_ar\n",
    "\n",
    "\n",
    "\n",
    "def create_model(ar): \n",
    "    print(\"create_model = \",ar)\n",
    "    model = Simple_MLP(ar)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    print(model)\n",
    "    return model,optimizer ,ar\n",
    "ar2=create_ar(10)\n",
    "model,optimizer ,ar2= create_model(ar2)\n",
    "next_gen(ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "def create_data_loader(df,target_col):\n",
    "    target = pd.DataFrame(df[ target_col] )\n",
    "    #print(target)\n",
    "    ten_target = torch.from_numpy(target[target_col].values)\n",
    "    df = df.drop(target_col ,axis=1 )\n",
    "    ten_data = df_to_tensor(df)\n",
    "    #print (ten_data)\n",
    "\n",
    "    _dataset = HotelsDataset(ten_data,ten_target)\n",
    "\n",
    "\n",
    "\n",
    "#    test_loader_args = dict(shuffle=True,batch_size=pic_size,num_workers=0,pin_memory=True) if cuda\\\n",
    "#    else \n",
    "    test_loader_args = dict(shuffle=True,batch_size=pic_size)\n",
    "    loader = data.DataLoader(_dataset,**test_loader_args)\n",
    "    return ten_target,loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = hotels.copy()\n",
    "del df_train['DayDiff']\n",
    "del df_train['Snapshot Date']\n",
    "#hotels_df['SnapshotUnixDate']  = pd.to_datetime(hotels_df['Snapshot Date'])\n",
    "\n",
    "del df_train['Checkin Date']\n",
    "del df_train['Hotel Name']\n",
    "del df_train['SnapDate']\n",
    "del df_train['CheckingDate']\n",
    "\n",
    "\n",
    "target_columns = 'DiscountPerc'\n",
    "#target_columns = 'Discount Price'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_validate = train_test_split(df_train, test_size=0.2)\n",
    "df_train2 = df_train.copy()\n",
    "#print(df_validate.describe())\n",
    "ten_target,loader = create_data_loader(df_train2,target_columns)\n",
    "#df_validate =  df_train.sample(n = 10000)\n",
    "\n",
    "\n",
    "df_validate = df_validate.drop(\"Discount Price\" ,axis=1 )\n",
    "df_validate = df_validate.drop(\"Discount\" ,axis=1 )\n",
    "validate_target, validate_loader = create_data_loader(df_validate,target_columns)\n",
    "#df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary algorithm with Deep networks\n",
    "\n",
    "1. Starting with random network with Relu activation functions.\n",
    "2. each generation take the \n",
    "    a. original\n",
    "    b. left original + random   right\n",
    "    c. left random   + original right\n",
    "    d. random (with random size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(model,train_loader,criterion,optimizer):\n",
    "    #print(1)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    running_loss=0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs = model(data)\n",
    "        \n",
    "        target = target.unsqueeze(1)\n",
    "        target = target.float() \n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs,target)\n",
    "        running_loss += loss.item()\n",
    "        #print(\"Train Loss item: \", loss.item() )\n",
    "        #print (outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    running_loss /= len(train_loader)\n",
    "    print(\"Train Loss: \", running_loss, ' Time: ', end_time-start_time)\n",
    "    return running_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model,test_loader,criterion):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        #print(2)\n",
    "        \n",
    "        for batch_idx, (data,target) in enumerate(test_loader):\n",
    "            #print(3)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            #print(outputs)\n",
    "            #_, predicted = torch.max(outputs.data,1)\n",
    "            #print(predicted)\n",
    "            #total_predictions += target.size(0)\n",
    "            #correct_predictions += (predicted==target).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs,target).detach()\n",
    "            running_loss += loss.item()\n",
    "        running_loss /= len(test_loader)\n",
    "        return running_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185380</th>\n",
       "      <td>3740</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.046524</td>\n",
       "      <td>22</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56623</th>\n",
       "      <td>5075</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029557</td>\n",
       "      <td>25</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119630</th>\n",
       "      <td>907</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.070562</td>\n",
       "      <td>6</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187119</th>\n",
       "      <td>1909</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.233106</td>\n",
       "      <td>26</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28033</th>\n",
       "      <td>1995</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>19</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64114</th>\n",
       "      <td>2156</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.083952</td>\n",
       "      <td>21</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93740</th>\n",
       "      <td>1567</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.098277</td>\n",
       "      <td>30</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148746</th>\n",
       "      <td>716</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068436</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87228</th>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.105051</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85122</th>\n",
       "      <td>850</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>30</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Original Price  Discount Code  Available Rooms  Hotel Stars  weekday  \\\n",
       "185380            3740              3               -1            5        2   \n",
       "56623             5075              2                3            5        2   \n",
       "119630             907              3               -1            3        1   \n",
       "187119            1909              2               -1            4        3   \n",
       "28033             1995              3               -1            5        2   \n",
       "...                ...            ...              ...          ...      ...   \n",
       "64114             2156              2               -1            4        5   \n",
       "93740             1567              4               -1            4        4   \n",
       "148746             716              2               -1            4        0   \n",
       "87228              990              1                1            3        5   \n",
       "85122              850              3               -1            4        1   \n",
       "\n",
       "        DiscountPerc  delta_days  hotel  \n",
       "185380      0.046524          22    285  \n",
       "56623       0.029557          25    482  \n",
       "119630      0.070562           6    374  \n",
       "187119      0.233106          26    192  \n",
       "28033       0.140351          19     79  \n",
       "...              ...         ...    ...  \n",
       "64114       0.083952          21    273  \n",
       "93740       0.098277          30    194  \n",
       "148746      0.068436          23    112  \n",
       "87228       0.105051          18     31  \n",
       "85122       0.117647          30    199  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train_bkup = df_train.copy()\n",
    "\n",
    "#df_train = df_train.drop(\"Available Rooms\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Discount Code\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Hotel Stars\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"weekday\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Discount\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"delta_days\" ,axis=1 )\n",
    "df_train = df_train.drop(\"Discount Price\" ,axis=1 )\n",
    "\n",
    "\n",
    "df_train = df_train.drop(\"Discount\" ,axis=1 )\n",
    "\n",
    "\n",
    "df_train.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1826.826914</td>\n",
       "      <td>2.434468</td>\n",
       "      <td>14.625720</td>\n",
       "      <td>3.908569</td>\n",
       "      <td>2.917886</td>\n",
       "      <td>0.088681</td>\n",
       "      <td>17.456341</td>\n",
       "      <td>289.064520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.301997</td>\n",
       "      <td>1.004995</td>\n",
       "      <td>41.753881</td>\n",
       "      <td>0.844449</td>\n",
       "      <td>1.842423</td>\n",
       "      <td>0.060248</td>\n",
       "      <td>10.033175</td>\n",
       "      <td>146.245146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>291.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.048402</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1601.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.070499</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>285.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.109167</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.683057</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>553.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Code  Available Rooms    Hotel Stars  \\\n",
       "count   150278.000000  150278.000000    150278.000000  150278.000000   \n",
       "mean      1826.826914       2.434468        14.625720       3.908569   \n",
       "std       1042.301997       1.004995        41.753881       0.844449   \n",
       "min        291.000000       1.000000        -1.000000       0.000000   \n",
       "25%       1160.000000       2.000000        -1.000000       3.000000   \n",
       "50%       1601.000000       2.000000        -1.000000       4.000000   \n",
       "75%       2160.000000       3.000000         8.000000       4.000000   \n",
       "max      29975.000000       4.000000       431.000000       5.000000   \n",
       "\n",
       "             weekday   DiscountPerc     delta_days          hotel  \n",
       "count  150278.000000  150278.000000  150278.000000  150278.000000  \n",
       "mean        2.917886       0.088681      17.456341     289.064520  \n",
       "std         1.842423       0.060248      10.033175     146.245146  \n",
       "min         0.000000       0.005814       1.000000       0.000000  \n",
       "25%         2.000000       0.048402       9.000000     176.000000  \n",
       "50%         3.000000       0.070499      18.000000     285.000000  \n",
       "75%         4.000000       0.109167      26.000000     397.000000  \n",
       "max         6.000000       0.683057      34.000000     553.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model =  [7, 105, 44, 57, 111, 122, 43, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=105, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=105, out_features=44, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=44, out_features=57, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=57, out_features=111, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=111, out_features=122, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=122, out_features=43, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=43, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  4.935834741219878  Time:  0.04764270782470703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([194])) that is different to the input size (torch.Size([194, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.935834741219878\n",
      "test loss 1.3055074466329042\n",
      "================================================== gen= 0 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.5783748286776245  Time:  0.03112030029296875\n",
      "train loss 0.5783748286776245\n",
      "test loss 0.4909727380794733\n",
      "================================================== gen= 0 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.2209504572674632  Time:  0.03312087059020996\n",
      "train loss 0.2209504572674632\n",
      "test loss 0.20574440700667246\n",
      "================================================== gen= 0 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.11707863118499517  Time:  0.03109598159790039\n",
      "train loss 0.11707863118499517\n",
      "test loss 0.02520696481778508\n",
      "================================================== gen= 0 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.06697386922314763  Time:  0.03246784210205078\n",
      "train loss 0.06697386922314763\n",
      "test loss 0.018417473008133928\n",
      "================================================== gen= 0 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.02568027493543923  Time:  0.036234140396118164\n",
      "train loss 0.02568027493543923\n",
      "test loss 0.044673304396624466\n",
      "================================================== gen= 0 index 5 vector= 0\n",
      "updating model =======  0.044673304396624466\n",
      "create_model =  [7, 86, 121, 80, 15, 12, 127, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=86, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=86, out_features=121, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=121, out_features=80, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=80, out_features=15, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=15, out_features=12, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=12, out_features=127, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=127, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  1.9884947557002306  Time:  0.03154110908508301\n",
      "train loss 1.9884947557002306\n",
      "test loss 0.10569452220688061\n",
      "================================================== gen= 0 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.24732979573309422  Time:  0.027098894119262695\n",
      "train loss 0.24732979573309422\n",
      "test loss 0.2991620077162373\n",
      "================================================== gen= 0 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.18680138513445854  Time:  0.029162883758544922\n",
      "train loss 0.18680138513445854\n",
      "test loss 0.012360010201073423\n",
      "================================================== gen= 0 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.03842015622649342  Time:  0.029349088668823242\n",
      "train loss 0.03842015622649342\n",
      "test loss 0.13161380675171508\n",
      "================================================== gen= 0 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.08873905334621668  Time:  0.034548044204711914\n",
      "train loss 0.08873905334621668\n",
      "test loss 0.008275630057086142\n",
      "================================================== gen= 0 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.023295247927308083  Time:  0.034337759017944336\n",
      "train loss 0.023295247927308083\n",
      "test loss 0.033503202810173945\n",
      "================================================== gen= 0 index 5 vector= 1\n",
      "updating model =======  0.033503202810173945\n",
      "create_model =  [7, 105, 44, 57, 15, 12, 127, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=105, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=105, out_features=44, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=44, out_features=57, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=57, out_features=15, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=15, out_features=12, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=12, out_features=127, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=127, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  1.3916856739670038  Time:  0.030924081802368164\n",
      "train loss 1.3916856739670038\n",
      "test loss 0.34264506571957853\n",
      "================================================== gen= 0 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.17291597998701036  Time:  0.03359270095825195\n",
      "train loss 0.17291597998701036\n",
      "test loss 0.05618586231555257\n",
      "================================================== gen= 0 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.09030753746628761  Time:  0.03131294250488281\n",
      "train loss 0.09030753746628761\n",
      "test loss 0.011017397857036721\n",
      "================================================== gen= 0 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.034218972781673074  Time:  0.030509233474731445\n",
      "train loss 0.034218972781673074\n",
      "test loss 0.03560253649595238\n",
      "================================================== gen= 0 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.015294843149604276  Time:  0.03460383415222168\n",
      "train loss 0.015294843149604276\n",
      "test loss 0.02240882912764744\n",
      "================================================== gen= 0 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.009947700702468865  Time:  0.034173011779785156\n",
      "train loss 0.009947700702468865\n",
      "test loss 0.010482887059551518\n",
      "================================================== gen= 0 index 5 vector= 2\n",
      "updating model =======  0.010482887059551518\n",
      "create_model =  [7, 105, 44, 57, 15, 12, 127, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=105, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=105, out_features=44, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=44, out_features=57, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=57, out_features=15, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=15, out_features=12, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=12, out_features=127, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=127, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.47777550108730793  Time:  0.02779221534729004\n",
      "train loss 0.47777550108730793\n",
      "test loss 0.07116280531599409\n",
      "================================================== gen= 0 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.030899159843102098  Time:  0.02797079086303711\n",
      "train loss 0.030899159843102098\n",
      "test loss 0.05183121088106616\n",
      "================================================== gen= 0 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.028209754265844822  Time:  0.027994871139526367\n",
      "train loss 0.028209754265844822\n",
      "test loss 0.009350316554662726\n",
      "================================================== gen= 0 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.008950045506935567  Time:  0.0327301025390625\n",
      "train loss 0.008950045506935567\n",
      "test loss 0.018690584240113797\n",
      "================================================== gen= 0 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.009511011303402483  Time:  0.03178906440734863\n",
      "train loss 0.009511011303402483\n",
      "test loss 0.004762792332592059\n",
      "================================================== gen= 0 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.00794717634562403  Time:  0.03352999687194824\n",
      "train loss 0.00794717634562403\n",
      "test loss 0.00841749368273482\n",
      "================================================== gen= 0 index 5 vector= 3\n",
      "updating model =======  0.00841749368273482\n",
      "create_model =  [7, 105, 44, 57, 111, 122, 43, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=105, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=105, out_features=44, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=44, out_features=57, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=57, out_features=111, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=111, out_features=122, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=122, out_features=43, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=43, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  3.1886599101126194  Time:  0.03141975402832031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 3.1886599101126194\n",
      "test loss 0.7582221704275429\n",
      "================================================== gen= 1 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.8230658024549484  Time:  0.030139923095703125\n",
      "train loss 0.8230658024549484\n",
      "test loss 0.0065551768437179985\n",
      "================================================== gen= 1 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.3656117881182581  Time:  0.03043508529663086\n",
      "train loss 0.3656117881182581\n",
      "test loss 0.0029592104493735396\n",
      "================================================== gen= 1 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.2228002821211703  Time:  0.03694272041320801\n",
      "train loss 0.2228002821211703\n",
      "test loss 0.024478214748558543\n",
      "================================================== gen= 1 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.0952675393782556  Time:  0.03567934036254883\n",
      "train loss 0.0952675393782556\n",
      "test loss 0.06363513716021363\n",
      "================================================== gen= 1 index 4 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.04714956588577479  Time:  0.030102014541625977\n",
      "train loss 0.04714956588577479\n",
      "test loss 0.07113029810340227\n",
      "================================================== gen= 1 index 5 vector= 0\n",
      "create_model =  [7, 86, 121, 80, 15, 12, 127, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=86, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=86, out_features=121, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=121, out_features=80, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=80, out_features=15, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=15, out_features=12, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=12, out_features=127, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=127, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  27.320084426552057  Time:  0.0380399227142334\n",
      "train loss 27.320084426552057\n",
      "test loss 0.36332080579128395\n",
      "================================================== gen= 1 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.4643651880323887  Time:  0.0354311466217041\n",
      "train loss 0.4643651880323887\n",
      "test loss 0.03491206056609446\n",
      "================================================== gen= 1 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.3024953808635473  Time:  0.032794952392578125\n",
      "train loss 0.3024953808635473\n",
      "test loss 0.11876890499170134\n",
      "================================================== gen= 1 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.2593274926766753  Time:  0.03145909309387207\n",
      "train loss 0.2593274926766753\n",
      "test loss 0.09593245812824794\n",
      "================================================== gen= 1 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.175612042658031  Time:  0.029868125915527344\n",
      "train loss 0.175612042658031\n",
      "test loss 0.05372738909153711\n",
      "================================================== gen= 1 index 4 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.10615695361047983  Time:  0.028172016143798828\n",
      "train loss 0.10615695361047983\n",
      "test loss 0.019229328608279733\n",
      "================================================== gen= 1 index 5 vector= 1\n",
      "create_model =  [7, 105, 44, 57, 15, 12, 127, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=105, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=105, out_features=44, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=44, out_features=57, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=57, out_features=15, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=15, out_features=12, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=12, out_features=127, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=127, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  1.3678785637021065  Time:  0.030942201614379883\n",
      "train loss 1.3678785637021065\n",
      "test loss 1.0761152469381994\n",
      "================================================== gen= 1 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.5863223413471133  Time:  0.027843236923217773\n",
      "train loss 0.5863223413471133\n",
      "test loss 0.42564057918632925\n",
      "================================================== gen= 1 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.1468300875276327  Time:  0.03139376640319824\n",
      "train loss 0.1468300875276327\n",
      "test loss 0.13073166976777875\n",
      "================================================== gen= 1 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.08198307082056999  Time:  0.027159929275512695\n",
      "train loss 0.08198307082056999\n",
      "test loss 0.0023415338186243053\n",
      "================================================== gen= 1 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.014696847356390208  Time:  0.029272079467773438\n",
      "train loss 0.014696847356390208\n",
      "test loss 0.04471004589581165\n",
      "================================================== gen= 1 index 4 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.04008424188941717  Time:  0.026859045028686523\n",
      "train loss 0.04008424188941717\n",
      "test loss 0.014654270151541346\n",
      "================================================== gen= 1 index 5 vector= 2\n",
      "create_model =  [7, 105, 44, 57, 15, 12, 127, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=105, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=105, out_features=44, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=44, out_features=57, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Linear(in_features=57, out_features=15, bias=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=15, out_features=12, bias=True)\n",
      "    (9): PReLU(num_parameters=1)\n",
      "    (10): Linear(in_features=12, out_features=127, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Linear(in_features=127, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  6.615128688514233  Time:  0.03177618980407715\n",
      "train loss 6.615128688514233\n",
      "test loss 2.205950687531711\n",
      "================================================== gen= 1 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.7420510568190366  Time:  0.03043508529663086\n",
      "train loss 0.7420510568190366\n",
      "test loss 0.9264237961801541\n",
      "================================================== gen= 1 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.690927267074585  Time:  0.030536890029907227\n",
      "train loss 0.690927267074585\n",
      "test loss 0.001090061066428288\n",
      "================================================== gen= 1 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.153933510533534  Time:  0.029101848602294922\n",
      "train loss 0.153933510533534\n",
      "test loss 0.18475102841043148\n",
      "================================================== gen= 1 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.06671946542337537  Time:  0.038387298583984375\n",
      "train loss 0.06671946542337537\n",
      "test loss 0.02636701988727868\n",
      "================================================== gen= 1 index 4 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.02908455370925367  Time:  0.030443668365478516\n",
      "train loss 0.02908455370925367\n",
      "test loss 0.004434250825762647\n",
      "================================================== gen= 1 index 5 vector= 3\n",
      "updating model =======  0.004434250825762647\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epoch = 6\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "#model2,optimizer2 = create_model()\n",
    "acc1 =0\n",
    "acc2 =0\n",
    "\n",
    "arx = create_ar(8)\n",
    "best , ar1,ar2 = next_gen(arx)\n",
    "#This is the 4 models that we are working on \n",
    "loops = [best,ar1,ar2,arx]\n",
    "\n",
    "target_columns='DiscountPerc'\n",
    "#target_columns = 'perc'\n",
    "results =[]\n",
    "best_score =100\n",
    "best_index =-1\n",
    "a,b,best =[],[],[]\n",
    "for generations in range (2):\n",
    "    #plt.plot(Test_acc)\n",
    "    #Loop over the models and choose the best one.\n",
    "    for index in range(4):\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        model ,optimizer ,ar = create_model(loops[index])\n",
    "        temp_model = model\n",
    "        temp_optimizer = optimizer\n",
    "       \n",
    "        for i in range(n_epoch):\n",
    "            #print (df_train.head())\n",
    "            train_loader = create_set(1000,df_train,target_columns)\n",
    "            #print(df_train.head())\n",
    "            train_loss = train_epoch(temp_model,train_loader,criterion,temp_optimizer)\n",
    "            test_loss = test_epoch(temp_model,validate_loader,criterion)\n",
    "            \n",
    "            #Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "            print(\"train loss\",train_loss)\n",
    "            print(\"test loss\" , test_loss)\n",
    "            #Test_acc.append(test_acc)\n",
    "            print('='*50,'gen=',generations,'index',i,'vector=',index)\n",
    "        \n",
    "        if (test_loss < best_score):\n",
    "            print (\"updating model ======= \", test_loss)\n",
    "            best_model = temp_model\n",
    "            best_optimizer = temp_optimizer\n",
    "            best_score = test_loss\n",
    "            best_index = index\n",
    "        \n",
    "        results.append(test_loss)\n",
    "    \n",
    "    \n",
    "    a,b,best = next_gen(loops[index])\n",
    "    c=create_ar(8)\n",
    "    results =[]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target column is  DiscountPerc\n",
      "Train Loss:  0.014304536604322493  Time:  0.14209675788879395\n",
      "================================================== 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.005626590951578692  Time:  0.16116809844970703\n",
      "================================================== 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.0015049316658405586  Time:  0.14701294898986816\n",
      "================================================== 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.0007897923816926778  Time:  0.14505481719970703\n",
      "================================================== 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.000676194069092162  Time:  0.1470787525177002\n",
      "================================================== 4\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.0005807084147818386  Time:  0.14788079261779785\n",
      "================================================== 5\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.0004973920338670723  Time:  0.16919398307800293\n",
      "================================================== 6\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.00045067063183523715  Time:  0.14780902862548828\n",
      "================================================== 7\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.00037152807344682516  Time:  0.13616204261779785\n",
      "================================================== 8\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  0.00029123034692020157  Time:  0.14519095420837402\n",
      "================================================== 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "            train_loader = create_set(5000,df_train,target_columns)\n",
    "            train_loss = train_epoch(best_model,train_loader,criterion,best_optimizer)\n",
    "            test_loss = test_epoch(best_model,validate_loader,criterion)\n",
    "\n",
    "            \n",
    "            Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "      \n",
    "            print('='*50,i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbf08c6f650>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss ')\n",
    "plt.plot(Test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbee5a40290>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAESCAYAAADjS5I+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de1zUZd4//tccmAFmBhQHz2KKTKJFHKzVlCkPtJl2b5EO3NRkd9u2ave6Eu3aadG1fmpZWLveZu2StmwKtMu2Vm7+FnWZ1PJAKmGQRopnQSFlRuY83z/AwTEYDBk+zMzr+XjsY2Y+1+fw9nq4vfycrkvkcrlcICIi6oBY6AKIiKh3Y1AQEZFXDAoiIvKKQUFERF4xKIiIyCsGBREReSUVugAiIb388svYu3cvAKCmpgZDhgxBaGgoAKCoqMj9vTNbt27F559/jhdffPG6j/3ss88iLi4OP//5z3984UQ9SMT3KIhaTJkyBW+++SZuvfXWHjkeg4L8Bc8oiLy45ZZbMHXqVFRXV+O1117DN998g6KiIthsNly8eBG/+MUvkJWVhZKSEmzZsgVvv/029Ho9EhMT8eWXX+LMmTOYMGECXnrpJYjF13+lt7S0FKtXr4bT6YRCocBzzz2HhIQE1NTU4IUXXoDVaoXL5cKsWbPw8MMPd7icqDvwHgWRFzabDZMnT8aWLVswcuRIfPDBB3jnnXfw4YcfYtWqVVi5cmW72x0/fhwFBQXYtGkTDAYD9uzZc93HrKmpweLFi/HHP/4RmzZtwoIFCzB//nwYjUbk5+djypQpKCkpwTvvvIN9+/bB6XR2uJyoO/CMgqgT48aNAwAoFAqsXbsWZWVlOHbsGKqrq3H58uV2t5k8eTLEYjGUSiWGDx+OixcvXvfxvvjiC4wfPx7Dhg0DAEyYMAFRUVGorKxEWloaFi1ahIqKCkyYMAEvvvgixGJxh8uJugP/JhF1Ijw8HABw9uxZPPDAAzh16hRSUlKwcOHCDre5+ia4SCTCj7kV6HQ6IRKJPJa5XC7Y7Xb32c306dNRVVWF+++/H2fPnu1wOVF3YFAQXafKykpERUVh/vz5mDRpErZv3w4AcDgc3XqcCRMmYMeOHThx4gQA4PPPP8eZM2dw2223IScnB5s3b8aMGTOwePFiKJVKHD9+vMPlRN2Bl56IrtPEiRPxt7/9Dffeey9EIhHuuOMOREVFoba2tsv7XLVqFVavXu3+PXnyZOTl5WHx4sX43//9XzgcDoSGhmLt2rVQqVSYP38+XnjhBRQVFUEikWDatGm4/fbb0a9fv3aXE3UHPh5LRERe8dITERF5xaAgIiKvGBREROQVg4KIiLwKuKeeDhw4ALlc3qVtLRZLl7cNROwPT+yPNuwLT4HQHxaLBYmJie22BVxQyOVyxMfHd2nbqqqqLm8biNgfntgfbdgXngKhP6qqqjps46UnIiLyikFBREReMSiIiMgrBgUREXnFoCAiIq8YFERE5BWDgoiIvGJQtDJZ7Nj2XROcTg6mS0R0NQZFq0OnL2HlZ/XYVXNB6FKIiHoVBkWrW4ZEQCoGDEfqhS6FiKhX8UlQOJ1O5ObmIiMjA3q9/gczgBUXFyM9PR06nc49neQV69evx2uvvfaDff7ud79rd3l3CZdJMbZ/KAyHGRRERFfzSVCUlpbCarWiqKgIOTk5WLFihbutvr4eBQUFKCwsRH5+PvLy8mC1WmE2m/HMM89gw4YNP9hfYWEhDh8+7ItSPSQPDkf12SbUXTL7/FhERP7CJ0FRXl6O1NRUAEBiYiIqKyvdbRUVFUhKSoJMJoNKpUJMTAyqq6thsVjwwAMPYO7cuR772r9/Pw4ePIiMjAxflOohZUgYAMBw5LzPj0VE5C98Mnqs0WiEUql0/5ZIJLDb7ZBKpTAajVCpVO42hUIBo9GIyMhITJo0CSUlJe62uro6rF69GqtXr8a//vWv6zq2xWLxOgqiN4PCnOgbKsHH+77F2PCmLu0jkJjN5i73ZSBif7RhX3gK9P7wSVAolUqYTCb3b6fTCalU2m6byWTyCI6rffrpp2hsbMSTTz6J+vp6mM1mjBw5Eunp6R0e+0aHGZ8cPxD/OVyPm28eDbFY1KX9BIpAGDq5O7E/2rAvPAVCf/T4MOPJyckwGAwAWiYS0mg07raEhASUl5fDYrGgqakJNTU1Hu1Xe/TRR1FSUoKCggI8+eSTmDlzpteQ6A6pGjUaTFYcOn3Jp8chIvIXPjmjSEtLw86dO5GZmQmXy4Vly5Zh3bp1iImJwdSpU6HX65GVlQWXy4Xs7OxeNTNUalw0gJbHZG8dGilwNUREwvNJUIjFYixdutRjWWxsrPu7TqeDTqdrd9uOzhh8fSZxhVopx9jBESg7XI+nJo/qkWMSEfVmfOGuHalx0fiythFGi13oUoiIBMegaIdWo4bd6cLnHM6DiIhB0Z5xw6MQLpPwLW0iIjAo2iWTijFhZD+O+0REBAZFh1Lj1Ki9cBm1F0ydr0xEFMAYFB3Qaq48JsvhPIgouDEoOjBCrcDQvmG8T0FEQY9B0QGRSAStJhqf11yAzeEUuhwiIsEwKLzQxqlhtNix//j3QpdCRCQYBoUXd45SQyIW8fITEQU1BoUXEaEhSBrWh4/JElFQY1B0IjUuGl+duogGk1XoUoiIBMGg6IRWo4bLBez4lo/JElFwYlB0ImFoH/QJD+F9CiIKWgyKTkjEIkwcpcZnR+rhcrmELoeIqMcxKK6DNk6Nc5csOHzOKHQpREQ9jkFxHdzDefDyExEFIQbFdRgUGYa4/ko+JktEQYlBcZ1S46Kx+2gDzDaH0KUQEfUoBsV10mrUsNqd2H20QehSiIh6lE+Cwul0Ijc3FxkZGdDr9aitrfVoLy4uRnp6OnQ6HbZv3+7Rtn79erz22mvu3x9//DFmz56NzMxM5ObmwukUZoC+n4zoB5lUzPsURBR0fBIUpaWlsFqtKCoqQk5ODlasWOFuq6+vR0FBAQoLC5Gfn4+8vDxYrVaYzWY888wz2LBhg3tds9mMN954A3/5y19QWFgIo9H4g2DpKWEyCX4yIopBQURBxydBUV5ejtTUVABAYmIiKisr3W0VFRVISkqCTCaDSqVCTEwMqqurYbFY8MADD2Du3LnudWUyGQoLCxEWFgYAsNvtkMvlvij5uqTGqXGkzogzF5sFq4GIqKdJfbFTo9EIpVLp/i2RSGC32yGVSmE0GqFSqdxtCoUCRqMRkZGRmDRpEkpKStxtYrEYarUaAFBQUIDLly9j4sSJXo9tsVhQVVXVpbrNZrPXbYdKW8Z7KiqrwE/jIrp0DH/SWX8EG/ZHG/aFp0DvD58EhVKphMnUNte00+mEVCptt81kMnkEx7WcTidWrlyJo0eP4o9//CNEIpHXY8vlcsTHx3ep7qqqKq/bjna50H97PY4YQ7Cwi8fwJ531R7Bhf7RhX3gKhP7wFnQ+ufSUnJwMg8EAADhw4AA0Go27LSEhAeXl5bBYLGhqakJNTY1H+7Vyc3NhsViwZs0a9yUooYhEIqTGRWPHkfNwODmcBxEFB5+cUaSlpWHnzp3IzMyEy+XCsmXLsG7dOsTExGDq1KnQ6/XIysqCy+VCdnZ2h/cdDh06hL/97W8YN24c5syZAwB49NFHkZaW5ouyr4tWo8bfvzyJr05dROKwPoLVQUTUU3wSFGKxGEuXLvVYFhsb6/6u0+mg0+na3TY9Pd39fezYsaiurvZFiV2WGhcNkahlOA8GBREFA75w9yNFKWS4dUgkH5MloqDBoOiC1Dg19p/4HpfMNqFLISLyOQZFF2jjouFwurDr2wtCl0JE5HMMii5IHt4XCpmEo8kSUVBgUHRBiESMCbFqGA5z1jsiCnwMii66S6PGycZmHLtwWehSiIh8ikHRRZz1joiCBYOii4b3U2B4v3AGBREFPAbFDUiNU+Pz7y7Aahdmjgwiop7AoLgB2rhoXLY6UF7bKHQpREQ+w6C4ARNi+0EqFvExWSIKaAyKG6AKDUHy8L68T0FEAY1BcYO0cWocOn0J540WoUshIvIJBsUNuvKY7I4j5wWuhIjINxgUN+iWwZHoGx7Cy09EFLAYFDdILBZhUlw0DEfOw8lZ74goADEouoE2To3zRguqzzYJXQoRUbdjUHQD93AefEyWiAIQg6IbDIgIxeiBKt6nIKKAxKDoJqlxauw71ojLVrvQpRARdSufBIXT6URubi4yMjKg1+tRW1vr0V5cXIz09HTodDps377do239+vV47bXX3L+3bduGhx56CBkZGSguLvZFud1Cq4mG1eHE7u8ahC6FiKhb+SQoSktLYbVaUVRUhJycHKxYscLdVl9fj4KCAhQWFiI/Px95eXmwWq0wm8145plnsGHDBve6NpsNy5cvx7vvvouCggIUFRWhvr53Xt65/aYoyKVilPHyExEFGJ8ERXl5OVJTUwEAiYmJqKysdLdVVFQgKSkJMpkMKpUKMTExqK6uhsViwQMPPIC5c+e6162pqUFMTAwiIyMhk8mQkpKCffv2+aLkGxYaIsFPRvbjDW0iCjhSX+zUaDRCqVS6f0skEtjtdkilUhiNRqhUKnebQqGA0WhEZGQkJk2ahJKSEo/9tLeuNxaLBVVVVV2q22w2d3lbABgd4YDhsAn/2VuBAcqQLu+nt7jR/gg07I827AtPgd4fPgkKpVIJk8nk/u10OiGVStttM5lMHmHgbT/e1r1CLpcjPj6+S3VXVVV1eVsAkEY14Z19Bpxx9cHd8TFd3k9vcaP9EWjYH23YF54CoT+8BZ1PLj0lJyfDYDAAAA4cOACNRuNuS0hIQHl5OSwWC5qamlBTU+PRfrXY2FjU1tbi+++/h9Vqxb59+5CUlOSLkrvFqP5KDIoM5WOyRBRQfHJGkZaWhp07dyIzMxMulwvLli3DunXrEBMTg6lTp0Kv1yMrKwsulwvZ2dmQy+Xt7ickJATPPvssfv7zn8PlcuGhhx7CgAEDfFFytxCJREiNU+NflWdhdzghlfDpYyLyfz4JCrFYjKVLl3osi42NdX/X6XTQ6XTtbpuenu7xe8qUKZgyZUr3F+kjWk00ivedxMGTF5EyvK/Q5RAR3TD+k7ebTRqlhkgEXn4iooDBoOhmfcJlSBjah4/JElHAYFD4wF1xahw88T0uXrYJXQoR0Q1jUPiAVhMNpwvYWcNZ74jI/zEofCBxWB+o5FLepyCigMCg8AGpRIw7R/WD4XA9XC7OekdE/o1B4SNaTTROXzSjpt7U+cpERL0Yg8JHtHGts97x8hMR+TkGhY8MiwrHCLWCj8kSkd9jUPiQNk6NL767AIvdIXQpRERdxqDwIa0mGmabE/uONQpdChFRlzEofGj8yH4IkYh4n4KI/BqDwocUcilShvfl9KhE5NcYFD6m1USj+mwT6i6ZhS6FiKhLGBQ+duUx2c+OcDgPIvJPDAofGzMoAmqljI/JEpHfYlD4mFgswqRRanx25DycTg7nQUT+h0HRA7SaaDSYrPj6zCWhSyEi+tEYFD1gUpwaAPj0ExH5JQZFD+ivCkX8oAi+T0FEfolB0UO0GjW+PN4Io8UudClERD+KT4LC6XQiNzcXGRkZ0Ov1qK2t9WgvLi5Geno6dDodtm/fDgBoaGjA448/jqysLCxcuBDNzc0AgPz8fKSnp+Ohhx7Cv//9b1+U2yPuiouGzeHCFzUXhC6FiOhH8UlQlJaWwmq1oqioCDk5OVixYoW7rb6+HgUFBSgsLER+fj7y8vJgtVqxZs0azJw5Exs2bMCYMWNQVFSES5cuudd99913sWzZMl+U2yNSbuqLsBAJH5MlIr8j9cVOy8vLkZqaCgBITExEZWWlu62iogJJSUmQyWSQyWSIiYlBdXU1ysvL8ctf/hIAoNVqkZeXh4cffhiDBw9Gc3MzmpubIRKJOj22xWJBVVVVl+o2m81d3vZ63NJfhtLKU/hvjcRnx+hOvu4Pf8P+aMO+8BTo/eGToDAajVAqle7fEokEdrsdUqkURqMRKpXK3aZQKGA0Gj2WKxQKNDU1AQAGDRqEGTNmwOFwuIPEG7lcjvj4+C7VXVVV1eVtr8d9DaH4/UdfQ9F/OGL6hfvsON3F1/3hb9gfbdgXngKhP7wFnU8uPSmVSphMbVOAOp1OSKXSdttMJhNUKpXHcpPJhIiICBgMBtTV1WHr1q34z3/+g9LSUlRUVPii5B6h1bTOesfLT0TkRzoNir1798JgMKCsrAzTpk3DRx991OlOk5OTYTAYAAAHDhyARqNxtyUkJKC8vBwWiwVNTU2oqamBRqNBcnIyysrKAAAGgwEpKSmIjIxEaGgoZDIZ5HI5VCoVLl3y35fWRqoVGNInjI/JEpFf6TQoVq5ciZtuugl/+ctfsHHjRhQWFna607S0NMhkMmRmZmL58uV47rnnsG7dOmzduhXR0dHQ6/XIysrCnDlzkJ2dDblcjnnz5uGTTz5BZmYm9u/fj0ceeQTjxo3DrbfeCp1Oh4yMDNx0002YOHFit/zBhSASiaDVqLGr5gJsDqfQ5RARXZdO71HI5XL069cPUqkU0dHRsFqtne5ULBZj6dKlHstiY2Pd33U6HXQ6nUe7Wq1Gfn7+D/a1YMECLFiwoNNj+gttXDQ27jmBAye+x+03RQldDhFRpzo9o1Aqlfif//kfTJ8+He+//z4GDRrUE3UFrDtHqSEWgZefiMhvdHpG8eabb+L48eMYNWoUjhw5gtmzZ/dEXQErMiwEicP6wHC4Hjn33Cx0OUREner0jKK2thZNTU04ePAgXn75ZZSXl/dEXQFNq4lGxamLaDR1fhmPiEhonQbF4sWLIZPJ8NZbbyE7OxurV6/uiboCmlYTDZcL2PEtZ70jot6v06CQSqWIi4uDzWZDYmIiHA5HT9QV0BKGRCIiVMr7FETkFzoNCpFIhJycHGi1WmzevBlhYWE9UVdAk0rEmBSnhuFIPVwuznpHRL1bp0GxatUqzJo1C3PmzEG/fv2watWqnqgr4GnjonHukgVH6oxCl0JE5FWnQSGTyfDFF1/gySefxNatW3uipqCQemU4D15+IqJertOgeP755zF48GBkZ2djyJAhePbZZ3uiroA3pE8YYqMVnB6ViHq9Tt+jaGxshF6vBwDEx8djy5YtPi8qWGg10diw+zjMNgdCQ/xj6HEiCj6dnlFYLBbU17f8q/f8+fNwOjlGUXfRaqJhsTux52iD0KUQEXWo0zOKX//618jMzIRKpYLRaMRLL73UE3UFhZ+MiIJMIobhcL17CHIiot6m06CYOHEitm7dioaGBkRFRf1g/mvqunCZFLeP6Mv5KYioV7vuiYuiolpGOs3JyfFZMcFIGxeNw+eMOHvRLHQpRETt+tEz3PEFse7FWe+IqLf70UEhEol8UUfQGj1QhWiVnO9TEFGv1eE9iqeffvoHoeByuXDixAmfFxVMRCIRUuPU2FZdB4fTBYmYQUxEvUuHQZGZmfmjllPX3aWJRsmXp1B56iJuG9ZH6HKIiDx0GBR33HFHT9YR1CaNUgNoGc6DQUFEvc2PvkdB3a+fUo5bhkTwhjYR9Uo+CQqn04nc3FxkZGRAr9f/4N2L4uJipKenQ6fTYfv27QCAhoYGPP7448jKysLChQvR3NwMACgrK4NOp4NOp8OSJUsC9qkrbVw0vjz+PZrMNqFLISLy4JOgKC0thdVqRVFREXJycrBixQp3W319PQoKClBYWIj8/Hzk5eXBarVizZo1mDlzJjZs2IAxY8agqKgIRqMRK1euxNq1a1FcXIwhQ4agsbHRFyULTquJhsPpwq6aC0KXQkTkodM3s7uivLwcqampAIDExERUVla62yoqKpCUlASZTAaZTIaYmBhUV1ejvLwcv/zlLwEAWq0WeXl5iI2NhUajwSuvvIITJ05g9uzZ7hf/OmKxWFBVVdWlus1mc5e3vVHhDhfCpCL8c/dhxIh7RxgK2R+9EfujDfvCU6D3h0+Cwmg0QqlUun9LJBLY7XZIpVIYjUaoVCp3m0KhgNFo9FiuUCjQ1NSExsZG7N69Gx9++CHCw8Px8MMPIzExESNGjOjw2HK5HPHx8V2qu6qqqsvbdoeJcSZ8da4Jo0eP7hXvqwjdH70N+6MN+8JTIPSHt6DzyaUnpVIJk8nk/u10OiGVStttM5lMUKlUHstNJhMiIiLQp08f3HrrrYiOjoZCocC4ceMCOrW1mmicaGhG7YXLQpdCROTmk6BITk6GwWAAABw4cAAajcbdlpCQgPLyclgsFjQ1NaGmpgYajQbJyckoKysDABgMBqSkpOCWW27B4cOH0dDQALvdjoMHD2LUqFG+KLlX0MZxOA8i6n18cukpLS0NO3fuRGZmJlwuF5YtW4Z169YhJiYGU6dOhV6vR1ZWFlwuF7KzsyGXyzFv3jwsWrQIxcXF6Nu3L15//XWEh4cjJycHTzzxBADg3nvv9QidQDO8XziGRYXBcLgej064SehyiIgA+CgoxGIxli5d6rEsNjbW/f3K465XU6vVyM/P/8G+ZsyYgRkzZviizF5HJBLhLk00/l5+CheMFvRTyoUuiYiIL9z1No/deRNsDide//dhoUshIgLAoOh1RvVX4dEJN2HjnuM4dPqi0OUQETEoeqNfT4tDVLgMv9/0dcC+iU5E/oNB0QtFhoXgmZ/ejD3HGvBxxRmhyyGiIMeg6KV044Zh7OAILN9chWarQ+hyiCiIMSh6KYlYhMX3j8Xpi2a8VVYjdDlEFMQYFL3YHSOicP9tg/F2WQ1ONvJtbSISBoOil3tu+miIRMCyzYE7dAkR9W4Mil5ucJ8wzL97FDZ/dRa7as4LXQ4RBSEGhR94UjsSQ/uGYelHX8PucApdDhEFGQaFHwgNkeCF++JRfbYJG/eeELocIgoyDAo/ce8tAzFhZD+8/v9/g+8vW4Uuh4iCCIPCT4hEIiz+rzG41GzDKo4DRUQ9iEHhR0YPjMAj44fjr7uP45uzTUKXQ0RBgkHhZ7KnaaCUS/H7jw5xHCgi6hEMCj/TVyFDzj0a7Kq5gC2HzgpdDhEFAQaFH8q6IwajB6rw8idVMNs4DhQR+RaDwg9JJWLk3j8GJxub8SfDd0KXQ0QBjkHhp+6MVWP6LQOx5j81OHOxWehyiCiAMSj82PP3xcPpcmH55mqhSyGiAOaToHA6ncjNzUVGRgb0ej1qa2s92ouLi5Geng6dToft27cDABoaGvD4448jKysLCxcuRHNzs8f+nnjiCWzcuNEX5fqtYVHh+KV2JDYdPI29xxqELoeIApRPgqK0tBRWqxVFRUXIycnBihUr3G319fUoKChAYWEh8vPzkZeXB6vVijVr1mDmzJnYsGEDxowZg6KiIvc2b7zxBi5e5PzR7Zl7dywGRYZiyaZDcDj5uCwRdT+fBEV5eTlSU1MBAImJiaisrHS3VVRUICkpCTKZDCqVCjExMaiurvbYRqvVYteuXQCATz/9FCKRCFqt1hel+r1wmRTP3RePQ6cv4YN9HAeKiLqf1Bc7NRqNUCqV7t8SiQR2ux1SqRRGoxEqlcrdplAoYDQaPZYrFAo0NTXh8OHD+Pjjj/GHP/wB//d//3ddx7ZYLKiq6trcDWazucvbCmlUiAtj+4di+eZDiJVfglIm6Zb9+mt/+Ar7ow37wlOg94dPgkKpVMJkMrl/O51OSKXSdttMJhNUKpV7eWhoKEwmEyIiIvDhhx/i3LlzmDNnDk6dOoWQkBAMGTLE69mFXC5HfHx8l+quqqrq8rZCeyVyCO5fvQOfnhDjdzO758/gz/3hC+yPNuwLT4HQH96CzieXnpKTk2EwGAAABw4cgEajcbclJCSgvLwcFosFTU1NqKmpgUajQXJyMsrKygAABoMBKSkp+O1vf4sPPvgABQUFePDBB/HYY4/xElQHbhkSiczbh+G9XcfwbR3HgSKi7uOToEhLS4NMJkNmZiaWL1+O5557DuvWrcPWrVsRHR0NvV6PrKwszJkzB9nZ2ZDL5Zg3bx4++eQTZGZmYv/+/XjkkUd8UVpAe+aemxEmk2Dpx1UcB4qIuo1PLj2JxWIsXbrUY1lsbKz7u06ng06n82hXq9XIz8/vcJ+/+tWvurfIANRPKcfCaRq89PHX2FpVh2ljBghdEhEFAL5wF2AenTAco/or8dInX8Ni5zhQRHTjGBQBJkQiRu7MMai9cBnv7jgmdDlEFAAYFAFIq4nGtPgBWL3tCOoumYUuh4j8HIMiQP1uZjxsDhdWfMpxoIjoxjAoAtTwfgr8PHUESr48hf3HG4Uuh4j8GIMigD01eRT6q+RYsukQnBwHioi6iEERwJRyKRbdOxoHT15Eyf5TQpdDRH6KQRHgHkwagsRhffDKp9UwWuxCl0NEfohBEeDEYhGW/NdY1DdZ8MdtR4Quh4j8EIMiCCQO64NZKUPx7o6jOHre1PkGRERXYVAEid/eezPkUgle/vhroUshIj/DoAgS/VWh+NWUUdhaXYf/fFMndDlE5EcYFEHkfyaOwAi1Aks//hpWu1PocojITzAogohM2jKp0Xf1Jvzl82NCl0NEfoJBEWSmjB6Au2+OxpulR1DfZBG6HCLyAwyKIPTijDFotjnw2pZvhC6FiPwAgyIIjeqvxGN33oTi8hP46uRFocshol6OQRGkFkyLQz+FDEs+OsRpU4nIKwZFkIoIDcFvfnozymsbsengaaHLIaJejEERxGanDMOtQyKxfHM1Lls5DhQRtc8nQeF0OpGbm4uMjAzo9XrU1tZ6tBcXFyM9PR06nQ7bt28HADQ0NODxxx9HVlYWFi5ciObmZgDA+vXrMXv2bMyePRurV6/2RblBq2UcqDE4e8mMNdtrhC6HiHopnwRFaWkprFYrioqKkJOTgxUrVrjb6uvrUVBQgMLCQuTn5yMvLw9WqxVr1qzBzJkzsWHDBowZMwZFRUU4ceIENm3ahMLCQhQVFWHHjh2oruaMbd0pZXgUHkgcjHc++w7HL1wWuhwi6oV8EhTl5eVITU0FACQmJqKystLdVlFRgaSkJMhkMqhUKsTExKC6utpjG61Wi127dmHgwIH485//DIlEArFYDLvdDrlc7ouSg9qz0+MhFYvw/23mOFBE9ENSXybjHUAAAA+4SURBVOzUaDRCqVS6f0skEtjtdkilUhiNRqhUKnebQqGA0Wj0WK5QKNDU1ISQkBBERUXB5XLh1VdfxZgxYzBixAivx7ZYLKiqqupS3Wazucvb+rvZYyPw3v5z2LDtSyQNCgMQ3P3RHvZHG/aFp0DvD58EhVKphMnUNpy10+mEVCptt81kMkGlUrmXh4aGwmQyISIiAkDLf/iff/55KBQKLF68uNNjy+VyxMfHd6nuqqqqLm/r754b5cC22jKsP9gE3V2JkErEQd0f7WF/tGFfeAqE/vAWdD659JScnAyDwQAAOHDgADQajbstISEB5eXlsFgsaGpqQk1NDTQaDZKTk1FWVgYAMBgMSElJgcvlwvz583HzzTdj6dKlkEgkviiXAISGSPDCfWNw+JwRf/2itvMNiCho+OSMIi0tDTt37kRmZiZcLheWLVuGdevWISYmBlOnToVer0dWVhZcLheys7Mhl8sxb948LFq0CMXFxejbty9ef/11lJaWYs+ePbBarfjss88AAE8//TSSkpJ8UXbQ++nYAZg4qh/y/n0Y/5U4ROhyiKiXELkC7LXcGzkFDITTxxt1+FwTpr/5GTJvH4ZHRkuDvj+uxr8fbdgXngKhP7z9GfjCHXnQDFBBP344Nu45ju8aOLosETEoqB3Z0zSIDAtBbulZLP3oa+z89jwnOiIKYj65R0H+LTI8BG89koLXPjmIv+6uxbs7j0Ill0KricaU0f0xeXR/RClkQpdJRD2EQUHtGj+yH16aNggxI+Ow89vz2FZdh63VdfjkqzMQiYDkmL6YMro/psUPgGaAEiKRSOiSichHGBTklUIuxT1jB+KesQPhdLpQefoitlbVYWv1Oazc8g1WbvkGQ/uGYero/pgSPwDjR0ZBLuVjzESBhEFB100sFiFhaB8kDO2D7DQNzl40Y/s3ddhadQ5F+07gvc9rES6TIDVOjamjB2Dy6P6IVnHIFSJ/x6CgLhsYGYr/viMG/31HDMw2Bz6vuYDSqnPYVl2HLYfOAQBuG9YHU0f3x9T4/hgzKIKXqIj8EIOCukVoiASTW290u1wuVJ1pwtaqc9haXYdVpYeR9+/DGBQZiimtoXFnrBqhIbxEReQPGBTU7UQiEcYMjsCYwRH41dQ41DdZsP2bOmyrqsOH+0/h/d3HERoixqRRakwZPQBT4/tjQESo0GUTUQcYFORz0So5dOOGQTduGCx2B3Z/14Bt1XUorTqH0qo64B/ALUMiMGX0AEyL749bBkdCLOYlKqLegkFBPUoulUCriYZWE43F94/BkTojtlbVYVv1OazedgR/2HoE0So5ptzccolqUpwa4TL+NSUSEv8fSIIRiUTQDFBBM0CFeXfHosFkRdnhOpRW1WHzV2dQtO8EZFIxYqLCoZBJEC6TQiG/5lMmgUIuRbhc2u46Cpm0pV0mgVwq5s10oi5gUFCvEaWQ4cGkoXgwaShsDif2HmvA9uo6nP7eDJPVjssWB85cNOOy1QGTxd7yabXjeoe1lIhFCJe1hEd4a4iEyyRQegmacJkUytbPc+fMcERcRJhMgrCQ1v8xgCgIMCioVwqRiHFnrBp3xqq9rudyudBsc8BkceCy1d72aXXgsqXl02Sxu4PG47O17VyTGZfPe7Y5OwqfT0//YJFIBHdwhLaGx9VBcuUz1L1MjHCZ1ON3y6f0qu3E7vZwmRRyqZj3bUgwDAryayKRCOEyaet9jO55uc/lcsFid7rPWowWOy5b7fjm26OIHjQUl612mG0ONFsdaLY50WxzoNlqb/10trTZWgLr+2Ybzl40t7S1bnPZWxB5ERoidodRaEjLmYw8RIJQqbh1WeuntO27vHU9d7tU4rluiBhyadv+rm4LkXDMUGrBoCC6hkgkcv/HuN9Vy8Mvn0N8/IAb3r/L5YLN0XImZLY5cNl6JXQc7gC6bHPA3Lqs2dbW3mxtWcdsbwkks80Bi82JxstWWGxOmO2t7baWdssNjPorEYuuCiEJ5O6gEcNhNSNi1yWESMSQikUtnxIRpGIxQiSia76LESJu+ZRKRAgRt6571fKQ1vWlEtE13zvaf8u2IRKxO+R4xuU7DAqiHiYSiSCTiiCTihEZFuLTY105O7o2PK6EzLWBcyVo2r5fta3dAUvr90Yz0GS2w+50wu5wweZwwu50eXy3OVra7E4nbA7fz48mk4hbz7JazpKuBJu3zytnUVd/yq/57V7eGpJXzuKufEqD4MyLQUEUwK4+O+pOP3ZGN5fLBYfT5REgttaQufp7W+C0hMu1QXRt+NgcTnfwtfdpcX860WBqO+u69vNG5vmUiEWQiYEw+Ul3gFx7WfDa4Gm75Of5KQ9pf3351Wd2rd8lPXgGxaAgIp8TiVovR0nQ64ZuuXIp0B0cV8Kl9YzK0rr86t9Xf5ptDpypOw9FRGTrcs/A+r7Z5g4s99lb6/eu3Ku6QioWeQRHuEyC3/9sbKcPgHTpWN2+RyIiP3L1pUB0cSSZrsyZ7XK1nGF1dDZ05RJgRwF17afd4UTfcN9MKOaToHA6nViyZAm++eYbyGQyvPzyyxg+fLi7vbi4GIWFhZBKpZg3bx4mT56MhoYGPPPMMzCbzejfvz+WL1+OsLCwdtclIvJ3IlHLjfsQiRgqoYvphE/uwpSWlsJqtaKoqAg5OTlYsWKFu62+vh4FBQUoLCxEfn4+8vLyYLVasWbNGsycORMbNmzAmDFjUFRU1OG6RETUc3wSFOXl5UhNTQUAJCYmorKy0t1WUVGBpKQkyGQyqFQqxMTEoLq62mMbrVaLXbt2dbguERH1HJ9cejIajVAqle7fEokEdrsdUqkURqMRKlXbiZZCoYDRaPRYrlAo0NTU1OG63lgsFlRVVXWpbrPZ3OVtAxH7wxP7ow37wlOg94dPgkKpVMJkMrl/O51OSKXSdttMJhNUKpV7eWhoKEwmEyIiIjpc1xu5XP6jbypd0ZUbUoGM/eGJ/dGGfeEpEPrDW9D55NJTcnIyDAYDAODAgQPQaDTutoSEBJSXl8NisaCpqQk1NTXQaDRITk5GWVkZAMBgMCAlJaXDdYmIqOf45IwiLS0NO3fuRGZmJlwuF5YtW4Z169YhJiYGU6dOhV6vR1ZWFlwuF7KzsyGXyzFv3jwsWrQIxcXF6Nu3L15//XWEh4e3uy4REfUckct1I+8k9j43cgoYCKeP3Yn94Yn90YZ94SkQ+sPbnyHwBykhIqIbEnBnFAcOHODlKSKiH8lisSAxMbHdtoALCiIi6l689ERERF4xKIiIyCsGBRERecWgICIirxgURETkFYOCiIi8YlCgZdDC3NxcZGRkQK/Xo7a2VuiSBGWz2fCb3/wGWVlZmDVrFrZu3Sp0SYK7cOEC7rrrLtTU1AhdiuDefvttZGRkID09HR988IHQ5QjKZrMhJycHmZmZyMrKCti/HwwKeJ9oKRht2rQJffr0wYYNG/CnP/0JL730ktAlCcpmsyE3NxehoV2cJzOA7N69G/v378fGjRtRUFCAs2fPCl2SoMrKymC321FYWIinnnoKb7zxhtAl+QSDAt4nWgpG9957L37961+7f0skEgGrEd4rr7yCzMxM9O/fX+hSBLdjxw5oNBo89dRTmDt3Lu6++26hSxLUiBEj4HA44HQ6YTQa3dMpBJrA/FP9SN4mWgpGCoUCQEu/LFiwAAsXLhS4IuGUlJQgKioKqampeOedd4QuR3CNjY04ffo01q5di5MnT2LevHn49NNPIRKJhC5NEOHh4Th16hSmT5+OxsZGrF27VuiSfIJnFPA+0VKwOnPmDB599FH87Gc/w/333y90OYL5+9//jl27dkGv16OqqgqLFi1CfX290GUJpk+fPpg0aRJkMhlGjhwJuVyOhoYGocsSzPr16zFp0iRs2bIF//znP/Hss8/CYrEIXVa3Y1DA+0RLwej8+fN4/PHH8Zvf/AazZs0SuhxBvf/++/jrX/+KgoICxMfH45VXXkF0dLTQZQkmJSUFn332GVwuF86dO4fm5mb06dNH6LIEExER4Z51MzIyEna7HQ6HQ+Cqul9w/7O5VXsTLQWztWvX4tKlS1izZg3WrFkDAPjTn/7Em7mEyZMnY+/evZg1axZcLhdyc3OD+h7WY489hueffx5ZWVmw2WzIzs5GeHi40GV1O44eS0REXvHSExERecWgICIirxgURETkFYOCiIi8YlAQEZFXDAoKGrt378aECROg1+vd/1uwYEG37PvZZ591v4vTE/R6fcAOQEe9D9+joKAyfvx4rFq1SugyiPwKg4IILf9CHzFiBI4ePQqXy4VVq1YhOjoaK1asQHl5OQBg5syZmDNnDo4dO4YXX3wRNpsNoaGh7uApKirCn//8ZxiNRixZsgQJCQnu/ZeUlKCsrAxmsxnHjx/HL37xC6Snp0Ov12PJkiWIjY3Fxo0bcf78eTz44IPIzs7GoEGDcPLkScyYMQNHjhzB119/jbvvvhtPP/00AOAPf/gDGhsbIZPJ8OqrryIqKgqvv/469u7dC5fLhcceewzTp0+HXq9H3759cenSJeTn5wf1C3LUNQwKCipffPEF9Hq9+/ddd92FJ554AkDLUC5Lly7F+++/j7fffhsTJ07EyZMnUVxcDLvdjqysLIwfPx5vvPEGnnzySWi1WmzevBlff/01AGDs2LGYP38+SkpKUFJS4hEUQMsgi/n5+Th27Bjmzp2L9PT0Dus8ceIE3n33XZjNZkydOhUGgwFhYWGYPHmyOyjuuecezJgxw13vnXfeiZMnT6KwsBAWiwU6nQ4TJ04EANx///1IS0vr1r6k4MGgoKDi7dLT+PHjAbQExrZt2zBw4ECMGzcOIpEIISEhuO2221BTU4OjR48iKSkJAHDfffcBAD7++GOMHTsWAKBWq2E2m3+w/9GjRwMABg0aBKvV+oP2qwdJGDZsGFQqFWQyGdRqtXs8patHaR03bpy73rKyMqjVahw6dMgdhHa7HadPnwbQMhw2UVfxZjZRqyvzkHz55ZcYNWoUYmNj3ZedbDYb9u/fj+HDhyM2NhZfffUVgJZJngoKCgCg06G222uXyWTu0WivnJlcz74AuGvYt28f4uLiMHLkSPzkJz9BQUEB3nvvPUyfPh1Dhw697v0RdYRnFBRUrr30BLQMeAgA//jHP7B+/XqEhYXh1VdfRd++fbFnzx5kZGTAZrPh3nvvxdixY/Hb3/4Wubm5eOuttxAaGoqVK1fi0KFDXarn0UcfxdKlSzFo0KAfPTFSaWkp3nvvPSgUCrzyyiuIiIjAnj17kJWVhcuXL2PatGke86wQdRUHBSQCPG4qE5EnXnoiIiKveEZBRERe8YyCiIi8YlAQEZFXDAoiIvKKQUFERF4xKIiIyKv/Bza8/KEm35hRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss ')\n",
    "plt.plot(Train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_epoch(model,data):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        results = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        for batch_idx, data in enumerate(data):\n",
    "          \n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            print (outputs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f5ae33b65153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnewset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_set2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msubmit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-68de96269d9c>\u001b[0m in \u001b[0;36msubmit_epoch\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2acdf6211f42>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out"
     ]
    }
   ],
   "source": [
    "X = df_train.sample(n=10000)\n",
    "\n",
    "X.describe()\n",
    "\n",
    "newset = create_set2(100,X,target_columns)\n",
    "submit_epoch(best_model,newset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

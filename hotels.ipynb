{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader.data as web\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Zeev\n",
    "## 033363870\n",
    "## Assaf\n",
    "## 204249197\n",
    "### setting and Downloading the tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = pd.read_csv('/Users/bebik/Documents/hotels_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/17/2015 0:00\n",
      "<class 'datetime.timedelta'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "date_time_str = '2018/8/04'\n",
    "print(hotels['Snapshot Date'][0])\n",
    "#for index, row in hotels.iterrows():\n",
    "tabletime = datetime.datetime.strptime( hotels['Snapshot Date'][0] , '%m/%d/%Y %H:%M')\n",
    "now = datetime.datetime.strptime( date_time_str , '%Y/%m/%d')\n",
    "newdt = tabletime - now\n",
    "print (type(newdt))\n",
    "print ( np.timedelta64(1, 'D').astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hotels['Days'] #always 5\n",
    "del hotels['Snapshot ID']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars  \n",
       "count  187848.000000  \n",
       "mean        3.905823  \n",
       "std         0.846496  \n",
       "min         0.000000  \n",
       "25%         3.000000  \n",
       "50%         4.000000  \n",
       "75%         4.000000  \n",
       "max         5.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars  \n",
       "count  187848.000000  \n",
       "mean        3.905823  \n",
       "std         0.846496  \n",
       "min         0.000000  \n",
       "25%         3.000000  \n",
       "50%         4.000000  \n",
       "75%         4.000000  \n",
       "max         5.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels['SnapDate']= pd.to_datetime(hotels['Snapshot Date']) \n",
    "hotels['CheckingDate']= pd.to_datetime(hotels['Checkin Date']) \n",
    "hotels['weekday']= hotels['CheckingDate'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels['DayDiff'] = hotels['CheckingDate']-hotels['SnapDate']\n",
    "hotels['Discount'] = hotels['Original Price']-hotels['Discount Price']\n",
    "hotels['DiscountPerc'] = hotels['Discount']/hotels['Original Price']*100\n",
    "#hotels['delta_days']= (hotels['CheckingDate']- now).astype(int)/8.636207/10**13\n",
    "hotels['delta_days']=(pd.to_datetime(hotels['Checkin Date']) - pd.to_datetime(hotels['Snapshot Date'])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DayDiff</th>\n",
       "      <th>Discount</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "      <td>187848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.742143</td>\n",
       "      <td>1673.172283</td>\n",
       "      <td>2.434186</td>\n",
       "      <td>14.588561</td>\n",
       "      <td>3.905823</td>\n",
       "      <td>2.917763</td>\n",
       "      <td>17 days 11:10:35.185894</td>\n",
       "      <td>152.569860</td>\n",
       "      <td>8.877715</td>\n",
       "      <td>17.465685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.355712</td>\n",
       "      <td>983.618013</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>41.575482</td>\n",
       "      <td>0.846496</td>\n",
       "      <td>1.840536</td>\n",
       "      <td>10 days 00:57:55.438813</td>\n",
       "      <td>143.316985</td>\n",
       "      <td>6.030248</td>\n",
       "      <td>10.040225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1035.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9 days 00:00:00</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>4.844961</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18 days 00:00:00</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>7.056229</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>1963.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>26 days 00:00:00</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>10.933941</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>28675.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>34 days 00:00:00</td>\n",
       "      <td>3760.000000</td>\n",
       "      <td>68.425842</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count   187848.000000   187848.000000  187848.000000    187848.000000   \n",
       "mean      1825.742143     1673.172283       2.434186        14.588561   \n",
       "std       1042.355712      983.618013       1.005191        41.575482   \n",
       "min        289.000000      260.000000       1.000000        -1.000000   \n",
       "25%       1160.000000     1035.000000       2.000000        -1.000000   \n",
       "50%       1599.000000     1475.000000       2.000000        -1.000000   \n",
       "75%       2160.000000     1963.250000       3.000000         8.000000   \n",
       "max      29975.000000    28675.000000       4.000000       431.000000   \n",
       "\n",
       "         Hotel Stars        weekday                  DayDiff       Discount  \\\n",
       "count  187848.000000  187848.000000                   187848  187848.000000   \n",
       "mean        3.905823       2.917763  17 days 11:10:35.185894     152.569860   \n",
       "std         0.846496       1.840536  10 days 00:57:55.438813     143.316985   \n",
       "min         0.000000       0.000000          1 days 00:00:00      15.000000   \n",
       "25%         3.000000       2.000000          9 days 00:00:00      70.000000   \n",
       "50%         4.000000       3.000000         18 days 00:00:00     103.000000   \n",
       "75%         4.000000       4.000000         26 days 00:00:00     180.000000   \n",
       "max         5.000000       6.000000         34 days 00:00:00    3760.000000   \n",
       "\n",
       "        DiscountPerc     delta_days  \n",
       "count  187848.000000  187848.000000  \n",
       "mean        8.877715      17.465685  \n",
       "std         6.030248      10.040225  \n",
       "min         0.581395       1.000000  \n",
       "25%         4.844961       9.000000  \n",
       "50%         7.056229      18.000000  \n",
       "75%        10.933941      26.000000  \n",
       "max        68.425842      34.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Createing Dendogram\n",
    "1. calculating the 40 hotels with most checking information\n",
    "2. add the dates of the 40 hotels to the selected dates\n",
    "3. create df_df with the 40 X 40 (dates*codes) * hotel ids\n",
    "4. hotel ids and names are in the hotels DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "554 554\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "vals = numpy.unique(hotels['Hotel Name'].values)\n",
    "checkins = numpy.unique(hotels['Checkin Date'].values)\n",
    "hotels_num = numpy.unique(hotels['Hotel Name'].values)\n",
    "print (type(hotels_num[0]))\n",
    "#print(checkins,len(checkins))             \n",
    "#print(hotels_num,len(hotels_num))             \n",
    "numbers = []\n",
    "for i in range(len(vals)) :\n",
    "    numbers.append(i)\n",
    "print (len(vals),len(numbers) )\n",
    "hotels['hotel']=hotels['Hotel Name']\n",
    "hotels['hotel'].replace(to_replace=vals, value=numbers,inplace = True)\n",
    "hotels.head()\n",
    "\n",
    "lens = []\n",
    "for date in checkins:\n",
    "    xdf = hotels['Checkin Date']==date\n",
    "    lens.append (len(hotels[xdf]))\n",
    "lens.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1307\n",
      "65626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DayDiff</th>\n",
       "      <th>Discount</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "      <td>65626.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1860.317816</td>\n",
       "      <td>1702.353077</td>\n",
       "      <td>2.438881</td>\n",
       "      <td>15.247783</td>\n",
       "      <td>3.904550</td>\n",
       "      <td>2.861229</td>\n",
       "      <td>17 days 13:30:13.659220</td>\n",
       "      <td>157.964740</td>\n",
       "      <td>8.877975</td>\n",
       "      <td>17.562658</td>\n",
       "      <td>288.245192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>991.594738</td>\n",
       "      <td>931.516379</td>\n",
       "      <td>0.996660</td>\n",
       "      <td>41.035860</td>\n",
       "      <td>0.826034</td>\n",
       "      <td>1.099896</td>\n",
       "      <td>10 days 00:09:07.970866</td>\n",
       "      <td>145.354637</td>\n",
       "      <td>5.736948</td>\n",
       "      <td>10.006342</td>\n",
       "      <td>147.396464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.026518</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1242.000000</td>\n",
       "      <td>1117.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9 days 00:00:00</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>4.946043</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1670.000000</td>\n",
       "      <td>1527.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18 days 00:00:00</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>7.245223</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>286.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2156.000000</td>\n",
       "      <td>1966.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>26 days 00:00:00</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10625.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>34 days 00:00:00</td>\n",
       "      <td>2900.000000</td>\n",
       "      <td>68.305669</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>553.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Price  Discount Code  Available Rooms  \\\n",
       "count    65626.000000    65626.000000   65626.000000     65626.000000   \n",
       "mean      1860.317816     1702.353077       2.438881        15.247783   \n",
       "std        991.594738      931.516379       0.996660        41.035860   \n",
       "min        300.000000      281.000000       1.000000        -1.000000   \n",
       "25%       1242.000000     1117.000000       2.000000        -1.000000   \n",
       "50%       1670.000000     1527.000000       2.000000        -1.000000   \n",
       "75%       2156.000000     1966.000000       3.000000         8.000000   \n",
       "max      10625.000000    10500.000000       4.000000       383.000000   \n",
       "\n",
       "        Hotel Stars       weekday                  DayDiff      Discount  \\\n",
       "count  65626.000000  65626.000000                    65626  65626.000000   \n",
       "mean       3.904550      2.861229  17 days 13:30:13.659220    157.964740   \n",
       "std        0.826034      1.099896  10 days 00:09:07.970866    145.354637   \n",
       "min        0.000000      1.000000          1 days 00:00:00     16.000000   \n",
       "25%        3.000000      2.000000          9 days 00:00:00     74.000000   \n",
       "50%        4.000000      3.000000         18 days 00:00:00    112.000000   \n",
       "75%        4.000000      4.000000         26 days 00:00:00    190.000000   \n",
       "max        5.000000      5.000000         34 days 00:00:00   2900.000000   \n",
       "\n",
       "       DiscountPerc    delta_days         hotel  \n",
       "count  65626.000000  65626.000000  65626.000000  \n",
       "mean       8.877975     17.562658    288.245192  \n",
       "std        5.736948     10.006342    147.396464  \n",
       "min        1.026518      1.000000      0.000000  \n",
       "25%        4.946043      9.000000    175.000000  \n",
       "50%        7.245223     18.000000    286.000000  \n",
       "75%       11.111111     26.000000    396.000000  \n",
       "max       68.305669     34.000000    553.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (lens[len(lens) - 41])\n",
    "bar = lens[len(lens) - 41]\n",
    "cnt =0\n",
    "lista =[]\n",
    "selected_dates = [] \n",
    "df_top40 = pd.DataFrame()\n",
    "for date in checkins:\n",
    "    xdf = hotels['Checkin Date']==date\n",
    "    if (len(hotels[xdf]) > bar):\n",
    "        selected_dates.append(date)\n",
    "        cnt+=len(hotels[xdf])\n",
    "        df_top40 = df_top40.append(hotels[xdf])\n",
    "\n",
    "print(cnt)\n",
    "df_top40.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10/1/2015 0:00', '10/14/2015 0:00', '10/15/2015 0:00', '10/16/2015 0:00', '10/2/2015 0:00', '10/21/2015 0:00', '10/22/2015 0:00', '10/27/2015 0:00', '10/28/2015 0:00', '10/29/2015 0:00', '10/30/2015 0:00', '10/31/2015 0:00', '10/7/2015 0:00', '11/10/2015 0:00', '11/11/2015 0:00', '11/12/2015 0:00', '11/13/2015 0:00', '11/18/2015 0:00', '11/21/2015 0:00', '11/26/2015 0:00', '11/27/2015 0:00', '11/28/2015 0:00', '11/3/2015 0:00', '11/4/2015 0:00', '11/5/2015 0:00', '11/6/2015 0:00', '11/7/2015 0:00', '12/30/2015 0:00', '8/12/2015 0:00', '8/19/2015 0:00', '8/26/2015 0:00', '8/27/2015 0:00', '8/28/2015 0:00', '9/10/2015 0:00', '9/11/2015 0:00', '9/16/2015 0:00', '9/17/2015 0:00', '9/18/2015 0:00', '9/30/2015 0:00', '9/9/2015 0:00']\n"
     ]
    }
   ],
   "source": [
    "print(selected_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "our_columns =[]\n",
    "for date in selected_dates:\n",
    "    our_columns.append(date+'_1')\n",
    "    our_columns.append(date+'_2')\n",
    "    our_columns.append(date+'_3')\n",
    "    our_columns.append(date+'_4')\n",
    "dandogram_df = pd.DataFrame(columns=our_columns)\n",
    "print (len(our_columns))\n",
    "\n",
    "#print ((our_columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Original Price  Discount Price  Available Rooms        weekday  \\\n",
      "count   187848.000000   187848.000000    187848.000000  187848.000000   \n",
      "mean      1825.742143     1673.172283        14.588561       2.917763   \n",
      "std       1042.355712      983.618013        41.575482       1.840536   \n",
      "min        289.000000      260.000000        -1.000000       0.000000   \n",
      "25%       1160.000000     1035.000000        -1.000000       2.000000   \n",
      "50%       1599.000000     1475.000000        -1.000000       3.000000   \n",
      "75%       2160.000000     1963.250000         8.000000       4.000000   \n",
      "max      29975.000000    28675.000000       431.000000       6.000000   \n",
      "\n",
      "        DiscountPerc     delta_days          hotel     days_delta  \n",
      "count  187848.000000  187848.000000  187848.000000  187848.000000  \n",
      "mean        8.877715      17.465685     289.105655      17.465685  \n",
      "std         6.030248      10.040225     146.185570      10.040225  \n",
      "min         0.581395       1.000000       0.000000       1.000000  \n",
      "25%         4.844961       9.000000     176.000000       9.000000  \n",
      "50%         7.056229      18.000000     285.000000      18.000000  \n",
      "75%        10.933941      26.000000     397.000000      26.000000  \n",
      "max        68.425842      34.000000     553.000000      34.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X, y = make_classification(random_state=0)\n",
    "X = hotels.copy()\n",
    "y = X['Discount Code']\n",
    "del X['Hotel Stars']\n",
    "del X['Discount Code']\n",
    "\n",
    "#del X['DiscountPerc']\n",
    "\n",
    "\n",
    "\n",
    "X['days_delta'] = X['DayDiff'].astype('timedelta64[D]')\n",
    "del X['DayDiff']\n",
    "del X['Snapshot Date']\n",
    "del X['Checkin Date']\n",
    "del X['Hotel Name']\n",
    "del X['SnapDate']\n",
    "del X['CheckingDate']\n",
    "#del X['Discount Price']\n",
    "del X['Discount']\n",
    "\n",
    "print(X.describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "#X, y = load_iris(return_X_y=True)\n",
    "#clf2 = tree.DecisionTreeClassifier()\n",
    "#clf2 = clf2.fit(X_train, y_train)\n",
    "#clf2 = clf2.predict(X_test)\n",
    "#tree.plot_tree(clf2) \n",
    "#print(clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.plot_tree(clf2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test=X_test.append([2000,2,3,4,5])\n",
    "#print (len(X_test))\n",
    "#print (clf.predict(X_test[:1]))\n",
    "#print (X_test[:1])\n",
    "\n",
    "#array([1, 0])\n",
    "#clf.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "pic_size = 256\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self,size_list):\n",
    "        super(Simple_MLP,self).__init__()\n",
    "        layers=[]\n",
    "        self.size_list = size_list\n",
    "        for i in range(len(size_list) -2):\n",
    "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "            op = random.randint(0,2) \n",
    "            \n",
    "            layers.append(torch.nn.ReLU())\n",
    "        #layers.append(nn.Linear(size_list[-3],size_list[-2]))\n",
    "        #layers.append(nn.Softmax(dim=1))\n",
    "        layers.append(nn.Linear(size_list[-2],size_list[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelsDataset(data.Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index].float()\n",
    "        Y = self.Y[index].float()\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    #device = get_device()\n",
    "    #return torch.from_numpy(df.values)\n",
    "    return torch.FloatTensor(df.values)\n",
    "def create_set(num,df,target_col):\n",
    "    print(' target column is ',target_col)\n",
    "    df_train_sample = df.copy()\n",
    "    if (num != 0):\n",
    "        df_train_sample = df_train_sample.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[target_col] )\n",
    "    ten_train_target = torch.FloatTensor(target_sample[target_col].values)\n",
    "    #print(\"train_target \",ten_train_target)\n",
    "    del df_train_sample[target_col]\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "    #print (\"target is \" , ten_train_target)\n",
    "    #print(ten_train_target)\n",
    "    train_dataset = HotelsDataset(ten_train_data,ten_train_target)\n",
    "\n",
    "\n",
    "    train_loader_args = dict(shuffle=True,batch_size=pic_size)\n",
    "    train_loader = data.DataLoader(train_dataset,**train_loader_args)\n",
    "    return train_loader\n",
    "\n",
    "def create_set2(num,df,target_col):\n",
    "    df_train_sample = df.copy()\n",
    "    if (num != 0):\n",
    "        df_train_sample = df_train_sample.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[target_col] )\n",
    "    ten_train_target = torch.FloatTensor(target_sample[target_col].values)\n",
    "    \n",
    "    del df_train_sample[target_col]\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "    return ten_train_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model =  [7, 74, 103, 91, 91, 97, 80, 62, 4, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=74, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=74, out_features=103, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=103, out_features=91, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=91, out_features=91, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=91, out_features=97, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=97, out_features=80, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=80, out_features=62, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=62, out_features=4, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([7, 74, 103, 91, 91, 112, 35, 134, 69, 1],\n",
       " [7, 11, 116, 72, 8, 97, 80, 62, 4, 1],\n",
       " [7, 74, 103, 91, 91, 97, 80, 62, 4, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "#criterion = nn.L1Loss()\n",
    "first_round = 7\n",
    "#first_round = 5\n",
    "\n",
    "\n",
    "end_round  = 1\n",
    "device = torch.device( \"cpu\")\n",
    "def create_ar(lens):\n",
    "    \n",
    "    r1 = 160\n",
    "    r2 = 160\n",
    "    ar = [first_round]\n",
    "    for i in range(random.randint(1,lens)):\n",
    "        r1 = random.randint(4,140)\n",
    "        ar.append( r1  )\n",
    "        r2 = random.randint(4,140)\n",
    "        ar.append(  r2 )\n",
    "    ar.append(end_round)\n",
    "    return ar\n",
    "\n",
    "\n",
    "#create next generations\n",
    "def next_gen(best_ar):\n",
    "    \n",
    "    ar_left = []\n",
    "    ar_right = []\n",
    "    for index,item in enumerate(best_ar):\n",
    "        r1 = random.randint(4,140)\n",
    "        if (index is 0 or index is len(best_ar)-1):\n",
    "            r1=item\n",
    "        if (index < len(best_ar)/2 ):\n",
    "            ar_left.append(item)\n",
    "            ar_right.append(r1)\n",
    "        else:\n",
    "            ar_left.append(r1)\n",
    "            ar_right.append(item)\n",
    "    \n",
    "    return ar_left,ar_right,best_ar\n",
    "\n",
    "\n",
    "\n",
    "def create_model(ar): \n",
    "    print(\"create_model = \",ar)\n",
    "    model = Simple_MLP(ar)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    print(model)\n",
    "    return model,optimizer ,ar\n",
    "ar2=create_ar(10)\n",
    "model,optimizer ,ar2= create_model(ar2)\n",
    "next_gen(ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "def create_data_loader(df,target_col):\n",
    "    target = pd.DataFrame(df[ target_col] )\n",
    "    #print(target)\n",
    "    ten_target = torch.FloatTensor(target[target_col].values)\n",
    "    df = df.drop(target_col ,axis=1 )\n",
    "    ten_data = df_to_tensor(df)\n",
    "    #print (ten_data)\n",
    "\n",
    "    _dataset = HotelsDataset(ten_data,ten_target)\n",
    "\n",
    "\n",
    "\n",
    "#    test_loader_args = dict(shuffle=True,batch_size=pic_size,num_workers=0,pin_memory=True) if cuda\\\n",
    "#    else \n",
    "    test_loader_args = dict(shuffle=True,batch_size=pic_size)\n",
    "    loader = data.DataLoader(_dataset,**test_loader_args)\n",
    "    return ten_target,loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = hotels.copy()\n",
    "del df_train['DayDiff']\n",
    "del df_train['Snapshot Date']\n",
    "#hotels_df['SnapshotUnixDate']  = pd.to_datetime(hotels_df['Snapshot Date'])\n",
    "\n",
    "del df_train['Checkin Date']\n",
    "del df_train['Hotel Name']\n",
    "del df_train['SnapDate']\n",
    "del df_train['CheckingDate']\n",
    "\n",
    "\n",
    "target_columns = 'DiscountPerc'\n",
    "#target_columns = 'Discount Price'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_validate = train_test_split(df_train, test_size=0.2)\n",
    "df_train2 = df_train.copy()\n",
    "#print(df_validate.describe())\n",
    "ten_target,loader = create_data_loader(df_train2,target_columns)\n",
    "#df_validate =  df_train.sample(n = 10000)\n",
    "\n",
    "\n",
    "df_validate = df_validate.drop(\"Discount Price\" ,axis=1 )\n",
    "#df_validate = df_validate.drop(\"DiscountPerc\" ,axis=1 )\n",
    "df_validate = df_validate.drop(\"Discount\" ,axis=1 )\n",
    "validate_target, validate_loader = create_data_loader(df_validate,target_columns)\n",
    "#df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary algorithm with Deep networks\n",
    "\n",
    "1. Starting with random network with Relu activation functions.\n",
    "2. each generation take the \n",
    "    a. original\n",
    "    b. left original + random   right\n",
    "    c. left random   + original right\n",
    "    d. random (with random size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(model,train_loader,criterion,optimizer):\n",
    "    #print(1)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    running_loss=0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        #print (target)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs = model(data)\n",
    "        \n",
    "        target = target.unsqueeze(1)\n",
    "        target = target.float() \n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs,target)\n",
    "        running_loss += loss.item()\n",
    "        #print(\"Train Loss item: \", loss.item() )\n",
    "        #print (outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    running_loss /= len(train_loader)\n",
    "    print(\"Train Loss: \", running_loss, ' Time: ', end_time-start_time)\n",
    "    return running_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model,test_loader,criterion):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        #print(2)\n",
    "        total_acc =0\n",
    "        index =0\n",
    "        acc =0\n",
    "        acc1 =0\n",
    "        for batch_idx, (data,target) in enumerate(test_loader):\n",
    "            #print(3)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            #print (\"outputs 1= \", outputs[1] ) \n",
    "            #print (\"outputs  \", outputs ) \n",
    "            #print (\"target x = \", target)\n",
    "            # Predicted discount / Discount\n",
    "            acc1 = (outputs.data/target.data)\n",
    "            #print (\"acc is x= \",acc)\n",
    "            loss = criterion(outputs,target).detach()\n",
    "            running_loss += loss.item()\n",
    "            total_acc +=torch.mean(acc1[0]) \n",
    "            index = index + 1\n",
    "        acc = total_acc/index\n",
    "        running_loss /= len(test_loader)\n",
    "        \n",
    "        print (\"mean acc = \",acc,\" idx \",index )\n",
    "            \n",
    "        return running_loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98347</th>\n",
       "      <td>5075</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.940887</td>\n",
       "      <td>1</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161213</th>\n",
       "      <td>1311</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5.491991</td>\n",
       "      <td>33</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4464</th>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>15</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135669</th>\n",
       "      <td>1366</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.831625</td>\n",
       "      <td>34</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23472</th>\n",
       "      <td>3763</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8.184959</td>\n",
       "      <td>24</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>2846</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.162333</td>\n",
       "      <td>17</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87609</th>\n",
       "      <td>1905</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.908136</td>\n",
       "      <td>22</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114113</th>\n",
       "      <td>1935</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.410853</td>\n",
       "      <td>10</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81046</th>\n",
       "      <td>1775</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.380282</td>\n",
       "      <td>27</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85807</th>\n",
       "      <td>2300</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8.695652</td>\n",
       "      <td>17</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Original Price  Discount Code  Available Rooms  Hotel Stars  weekday  \\\n",
       "98347             5075              1               27            5        2   \n",
       "161213            1311              4               -1            4        2   \n",
       "4464               750              4               -1            4        3   \n",
       "135669            1366              4                1            4        5   \n",
       "23472             3763              3               48            5        6   \n",
       "...                ...            ...              ...          ...      ...   \n",
       "26771             2846              2                1            4        5   \n",
       "87609             1905              2                2            4        2   \n",
       "114113            1935              3               70            4        1   \n",
       "81046             1775              3               -1            4        2   \n",
       "85807             2300              1              101            5        3   \n",
       "\n",
       "        DiscountPerc  delta_days  hotel  \n",
       "98347       3.940887           1    482  \n",
       "161213      5.491991          33     94  \n",
       "4464       10.666667          15    199  \n",
       "135669      4.831625          34    301  \n",
       "23472       8.184959          24    341  \n",
       "...              ...         ...    ...  \n",
       "26771       3.162333          17    242  \n",
       "87609      34.908136          22    287  \n",
       "114113      3.410853          10    538  \n",
       "81046       3.380282          27    323  \n",
       "85807       8.695652          17    529  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train_bkup = df_train.copy()\n",
    "\n",
    "#df_train = df_train.drop(\"Available Rooms\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Discount Code\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Hotel Stars\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"weekday\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"Discount\" ,axis=1 )\n",
    "#df_train = df_train.drop(\"delta_days\" ,axis=1 )\n",
    "\n",
    "\n",
    "df_train = df_train.drop(\"Discount Price\" ,axis=1 )\n",
    "\n",
    "#df_train = df_train.drop(\"DiscountPerc\" ,axis=1 )\n",
    "\n",
    "\n",
    "\n",
    "df_train = df_train.drop(\"Discount\" ,axis=1 )\n",
    "\n",
    "\n",
    "df_train.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount Code</th>\n",
       "      <th>Available Rooms</th>\n",
       "      <th>Hotel Stars</th>\n",
       "      <th>weekday</th>\n",
       "      <th>DiscountPerc</th>\n",
       "      <th>delta_days</th>\n",
       "      <th>hotel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "      <td>150278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1825.561772</td>\n",
       "      <td>2.432352</td>\n",
       "      <td>14.579380</td>\n",
       "      <td>3.905375</td>\n",
       "      <td>2.919216</td>\n",
       "      <td>8.881183</td>\n",
       "      <td>17.484216</td>\n",
       "      <td>289.074522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1042.681116</td>\n",
       "      <td>1.005444</td>\n",
       "      <td>41.581135</td>\n",
       "      <td>0.846907</td>\n",
       "      <td>1.839048</td>\n",
       "      <td>6.028766</td>\n",
       "      <td>10.045060</td>\n",
       "      <td>146.139747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1160.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.852050</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1600.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.058824</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>285.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2164.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.933941</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29975.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>68.425842</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>553.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Price  Discount Code  Available Rooms    Hotel Stars  \\\n",
       "count   150278.000000  150278.000000    150278.000000  150278.000000   \n",
       "mean      1825.561772       2.432352        14.579380       3.905375   \n",
       "std       1042.681116       1.005444        41.581135       0.846907   \n",
       "min        289.000000       1.000000        -1.000000       0.000000   \n",
       "25%       1160.000000       2.000000        -1.000000       3.000000   \n",
       "50%       1600.000000       2.000000        -1.000000       4.000000   \n",
       "75%       2164.000000       3.000000         8.000000       4.000000   \n",
       "max      29975.000000       4.000000       431.000000       5.000000   \n",
       "\n",
       "             weekday   DiscountPerc     delta_days          hotel  \n",
       "count  150278.000000  150278.000000  150278.000000  150278.000000  \n",
       "mean        2.919216       8.881183      17.484216     289.074522  \n",
       "std         1.839048       6.028766      10.045060     146.139747  \n",
       "min         0.000000       0.581395       1.000000       0.000000  \n",
       "25%         2.000000       4.852050       9.000000     176.000000  \n",
       "50%         3.000000       7.058824      18.000000     285.000000  \n",
       "75%         4.000000      10.933941      26.000000     397.000000  \n",
       "max         6.000000      68.425842      34.000000     553.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  109.33008575439453  Time:  0.03171896934509277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([194])) that is different to the input size (torch.Size([194, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.1750)  idx  147\n",
      "train loss 109.33008575439453\n",
      "test loss 98.21526606553266\n",
      "================================================== gen= 0 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  91.80571556091309  Time:  0.022922039031982422\n",
      "mean acc =  tensor(0.7771)  idx  147\n",
      "train loss 91.80571556091309\n",
      "test loss 59.29568587841631\n",
      "================================================== gen= 0 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.51269817352295  Time:  0.029703855514526367\n",
      "mean acc =  tensor(1.4890)  idx  147\n",
      "train loss 65.51269817352295\n",
      "test loss 59.99422060389097\n",
      "================================================== gen= 0 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.22807598114014  Time:  0.02245807647705078\n",
      "mean acc =  tensor(0.9742)  idx  147\n",
      "train loss 62.22807598114014\n",
      "test loss 54.59658058322206\n",
      "================================================== gen= 0 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.39380931854248  Time:  0.022497892379760742\n",
      "mean acc =  tensor(0.7904)  idx  147\n",
      "train loss 64.39380931854248\n",
      "test loss 58.84528444251236\n",
      "================================================== gen= 0 index 4 vector= 0\n",
      "updating model =======  58.84528444251236\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.30042839050293  Time:  0.026062965393066406\n",
      "mean acc =  tensor(0.1048)  idx  147\n",
      "train loss 114.30042839050293\n",
      "test loss 104.34640970035476\n",
      "================================================== gen= 0 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  100.6938247680664  Time:  0.025981903076171875\n",
      "mean acc =  tensor(0.6391)  idx  147\n",
      "train loss 100.6938247680664\n",
      "test loss 64.98038259493251\n",
      "================================================== gen= 0 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.40924453735352  Time:  0.026575803756713867\n",
      "mean acc =  tensor(1.3655)  idx  147\n",
      "train loss 68.40924453735352\n",
      "test loss 61.82673136393229\n",
      "================================================== gen= 0 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.15926456451416  Time:  0.030941009521484375\n",
      "mean acc =  tensor(0.8506)  idx  147\n",
      "train loss 72.15926456451416\n",
      "test loss 57.03133309292956\n",
      "================================================== gen= 0 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.85175895690918  Time:  0.031371116638183594\n",
      "mean acc =  tensor(1.0611)  idx  147\n",
      "train loss 63.85175895690918\n",
      "test loss 54.32983273875956\n",
      "================================================== gen= 0 index 4 vector= 1\n",
      "updating model =======  54.32983273875956\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  105.73098945617676  Time:  0.024996042251586914\n",
      "mean acc =  tensor(0.1935)  idx  147\n",
      "train loss 105.73098945617676\n",
      "test loss 94.95989123493636\n",
      "================================================== gen= 0 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  76.81760025024414  Time:  0.025750160217285156\n",
      "mean acc =  tensor(0.9792)  idx  147\n",
      "train loss 76.81760025024414\n",
      "test loss 53.97029163075142\n",
      "================================================== gen= 0 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.49217414855957  Time:  0.026623964309692383\n",
      "mean acc =  tensor(1.1739)  idx  147\n",
      "train loss 62.49217414855957\n",
      "test loss 53.28870827811105\n",
      "================================================== gen= 0 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  55.45566368103027  Time:  0.023663997650146484\n",
      "mean acc =  tensor(0.7957)  idx  147\n",
      "train loss 55.45566368103027\n",
      "test loss 58.42160176906456\n",
      "================================================== gen= 0 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.32658672332764  Time:  0.02432084083557129\n",
      "mean acc =  tensor(0.9790)  idx  147\n",
      "train loss 66.32658672332764\n",
      "test loss 52.63452680419091\n",
      "================================================== gen= 0 index 4 vector= 2\n",
      "updating model =======  52.63452680419091\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.89608383178711  Time:  0.02889704704284668\n",
      "mean acc =  tensor(0.0251)  idx  147\n",
      "train loss 114.89608383178711\n",
      "test loss 112.04395802815755\n",
      "================================================== gen= 0 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.2611198425293  Time:  0.027418136596679688\n",
      "mean acc =  tensor(0.1729)  idx  147\n",
      "train loss 110.2611198425293\n",
      "test loss 98.38972208451251\n",
      "================================================== gen= 0 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  84.61389350891113  Time:  0.02485489845275879\n",
      "mean acc =  tensor(0.6777)  idx  147\n",
      "train loss 84.61389350891113\n",
      "test loss 62.169165526928545\n",
      "================================================== gen= 0 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.20876693725586  Time:  0.024149179458618164\n",
      "mean acc =  tensor(1.4852)  idx  147\n",
      "train loss 61.20876693725586\n",
      "test loss 59.52341497512091\n",
      "================================================== gen= 0 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.42273712158203  Time:  0.028918981552124023\n",
      "mean acc =  tensor(0.9086)  idx  147\n",
      "train loss 64.42273712158203\n",
      "test loss 55.50423314620038\n",
      "================================================== gen= 0 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  105.26658248901367  Time:  0.02423405647277832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.1411)  idx  147\n",
      "train loss 105.26658248901367\n",
      "test loss 100.83234255005713\n",
      "================================================== gen= 1 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  92.31113052368164  Time:  0.026335954666137695\n",
      "mean acc =  tensor(0.5669)  idx  147\n",
      "train loss 92.31113052368164\n",
      "test loss 68.14374783574318\n",
      "================================================== gen= 1 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.5594253540039  Time:  0.02665996551513672\n",
      "mean acc =  tensor(1.4999)  idx  147\n",
      "train loss 66.5594253540039\n",
      "test loss 63.93287671666567\n",
      "================================================== gen= 1 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.24763202667236  Time:  0.02583909034729004\n",
      "mean acc =  tensor(1.1015)  idx  147\n",
      "train loss 62.24763202667236\n",
      "test loss 53.55847323670679\n",
      "================================================== gen= 1 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.3474349975586  Time:  0.023881912231445312\n",
      "mean acc =  tensor(0.8127)  idx  147\n",
      "train loss 68.3474349975586\n",
      "test loss 56.31119323263363\n",
      "================================================== gen= 1 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  120.40650939941406  Time:  0.025624990463256836\n",
      "mean acc =  tensor(0.1408)  idx  147\n",
      "train loss 120.40650939941406\n",
      "test loss 100.18678091659027\n",
      "================================================== gen= 1 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  87.05065536499023  Time:  0.029085159301757812\n",
      "mean acc =  tensor(1.0210)  idx  147\n",
      "train loss 87.05065536499023\n",
      "test loss 57.71173287735505\n",
      "================================================== gen= 1 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.06633758544922  Time:  0.026098012924194336\n",
      "mean acc =  tensor(1.0309)  idx  147\n",
      "train loss 72.06633758544922\n",
      "test loss 57.16836293538412\n",
      "================================================== gen= 1 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.70902919769287  Time:  0.025521278381347656\n",
      "mean acc =  tensor(0.6937)  idx  147\n",
      "train loss 60.70902919769287\n",
      "test loss 62.72784558769797\n",
      "================================================== gen= 1 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.33847713470459  Time:  0.024428129196166992\n",
      "mean acc =  tensor(0.7961)  idx  147\n",
      "train loss 63.33847713470459\n",
      "test loss 57.946831631822654\n",
      "================================================== gen= 1 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  107.69955062866211  Time:  0.023813962936401367\n",
      "mean acc =  tensor(0.0290)  idx  147\n",
      "train loss 107.69955062866211\n",
      "test loss 111.97092370273305\n",
      "================================================== gen= 1 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  107.08768272399902  Time:  0.022982120513916016\n",
      "mean acc =  tensor(0.1438)  idx  147\n",
      "train loss 107.08768272399902\n",
      "test loss 100.29733494349888\n",
      "================================================== gen= 1 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.26033782958984  Time:  0.02403402328491211\n",
      "mean acc =  tensor(0.6073)  idx  147\n",
      "train loss 102.26033782958984\n",
      "test loss 67.18441326115408\n",
      "================================================== gen= 1 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.78921890258789  Time:  0.025428056716918945\n",
      "mean acc =  tensor(1.4703)  idx  147\n",
      "train loss 63.78921890258789\n",
      "test loss 57.94508193139316\n",
      "================================================== gen= 1 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  53.817381858825684  Time:  0.026157140731811523\n",
      "mean acc =  tensor(0.9267)  idx  147\n",
      "train loss 53.817381858825684\n",
      "test loss 53.417891366141184\n",
      "================================================== gen= 1 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.35243606567383  Time:  0.023582935333251953\n",
      "mean acc =  tensor(0.0691)  idx  147\n",
      "train loss 111.35243606567383\n",
      "test loss 108.06705204970172\n",
      "================================================== gen= 1 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.9454574584961  Time:  0.023096084594726562\n",
      "mean acc =  tensor(0.3378)  idx  147\n",
      "train loss 98.9454574584961\n",
      "test loss 84.86511767640405\n",
      "================================================== gen= 1 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.84619903564453  Time:  0.02524709701538086\n",
      "mean acc =  tensor(1.2788)  idx  147\n",
      "train loss 78.84619903564453\n",
      "test loss 55.371298082831764\n",
      "================================================== gen= 1 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.46583652496338  Time:  0.024169921875\n",
      "mean acc =  tensor(1.1989)  idx  147\n",
      "train loss 69.46583652496338\n",
      "test loss 54.04694166637602\n",
      "================================================== gen= 1 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.67187690734863  Time:  0.02397632598876953\n",
      "mean acc =  tensor(0.6568)  idx  147\n",
      "train loss 67.67187690734863\n",
      "test loss 60.886676736429436\n",
      "================================================== gen= 1 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.44186592102051  Time:  0.02221226692199707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0118)  idx  147\n",
      "train loss 116.44186592102051\n",
      "test loss 113.67373911539714\n",
      "================================================== gen= 2 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.66223335266113  Time:  0.024526119232177734\n",
      "mean acc =  tensor(0.1040)  idx  147\n",
      "train loss 112.66223335266113\n",
      "test loss 104.04068626351908\n",
      "================================================== gen= 2 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.89590072631836  Time:  0.023683786392211914\n",
      "mean acc =  tensor(0.5277)  idx  147\n",
      "train loss 98.89590072631836\n",
      "test loss 71.90595250551392\n",
      "================================================== gen= 2 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.44370937347412  Time:  0.023083209991455078\n",
      "mean acc =  tensor(1.5516)  idx  147\n",
      "train loss 65.44370937347412\n",
      "test loss 60.68772600135025\n",
      "================================================== gen= 2 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.22106647491455  Time:  0.02451610565185547\n",
      "mean acc =  tensor(1.2094)  idx  147\n",
      "train loss 62.22106647491455\n",
      "test loss 52.946446918305895\n",
      "================================================== gen= 2 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  104.84192085266113  Time:  0.02591991424560547\n",
      "mean acc =  tensor(0.2311)  idx  147\n",
      "train loss 104.84192085266113\n",
      "test loss 91.08999893292278\n",
      "================================================== gen= 2 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  77.33790588378906  Time:  0.027444839477539062\n",
      "mean acc =  tensor(1.4176)  idx  147\n",
      "train loss 77.33790588378906\n",
      "test loss 55.85382645795134\n",
      "================================================== gen= 2 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.54213237762451  Time:  0.037043094635009766\n",
      "mean acc =  tensor(1.0955)  idx  147\n",
      "train loss 60.54213237762451\n",
      "test loss 53.512073880150204\n",
      "================================================== gen= 2 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.18757343292236  Time:  0.029821157455444336\n",
      "mean acc =  tensor(0.7217)  idx  147\n",
      "train loss 58.18757343292236\n",
      "test loss 60.05032366798038\n",
      "================================================== gen= 2 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.948161125183105  Time:  0.029531002044677734\n",
      "mean acc =  tensor(0.9912)  idx  147\n",
      "train loss 61.948161125183105\n",
      "test loss 53.46741981246844\n",
      "================================================== gen= 2 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  113.03804779052734  Time:  0.02482891082763672\n",
      "mean acc =  tensor(0.0315)  idx  147\n",
      "train loss 113.03804779052734\n",
      "test loss 111.44051854140093\n",
      "================================================== gen= 2 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  115.17918586730957  Time:  0.023713111877441406\n",
      "mean acc =  tensor(0.2070)  idx  147\n",
      "train loss 115.17918586730957\n",
      "test loss 93.52917252105921\n",
      "================================================== gen= 2 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.22315406799316  Time:  0.02813577651977539\n",
      "mean acc =  tensor(0.9518)  idx  147\n",
      "train loss 75.22315406799316\n",
      "test loss 55.71060902083001\n",
      "================================================== gen= 2 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.54200553894043  Time:  0.025557994842529297\n",
      "mean acc =  tensor(1.3698)  idx  147\n",
      "train loss 67.54200553894043\n",
      "test loss 56.494665340501435\n",
      "================================================== gen= 2 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.26049613952637  Time:  0.024034976959228516\n",
      "mean acc =  tensor(0.8154)  idx  147\n",
      "train loss 62.26049613952637\n",
      "test loss 58.10973962796788\n",
      "================================================== gen= 2 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.07790565490723  Time:  0.022193193435668945\n",
      "mean acc =  tensor(0.1572)  idx  147\n",
      "train loss 111.07790565490723\n",
      "test loss 98.3062269249741\n",
      "================================================== gen= 2 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  83.31631088256836  Time:  0.023089885711669922\n",
      "mean acc =  tensor(0.7968)  idx  147\n",
      "train loss 83.31631088256836\n",
      "test loss 58.804537143837024\n",
      "================================================== gen= 2 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.69782733917236  Time:  0.02637481689453125\n",
      "mean acc =  tensor(1.3629)  idx  147\n",
      "train loss 63.69782733917236\n",
      "test loss 60.8363855323013\n",
      "================================================== gen= 2 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.80558204650879  Time:  0.027365922927856445\n",
      "mean acc =  tensor(0.7909)  idx  147\n",
      "train loss 66.80558204650879\n",
      "test loss 59.11633705606266\n",
      "================================================== gen= 2 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.60890674591064  Time:  0.031703948974609375\n",
      "mean acc =  tensor(0.7056)  idx  147\n",
      "train loss 66.60890674591064\n",
      "test loss 62.49245175212419\n",
      "================================================== gen= 2 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  95.79636573791504  Time:  0.027805089950561523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.1647)  idx  147\n",
      "train loss 95.79636573791504\n",
      "test loss 97.31842414700255\n",
      "================================================== gen= 3 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  93.21641159057617  Time:  0.027369976043701172\n",
      "mean acc =  tensor(0.6081)  idx  147\n",
      "train loss 93.21641159057617\n",
      "test loss 66.71566733535455\n",
      "================================================== gen= 3 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.62307643890381  Time:  0.023109912872314453\n",
      "mean acc =  tensor(1.7374)  idx  147\n",
      "train loss 62.62307643890381\n",
      "test loss 68.57034750698375\n",
      "================================================== gen= 3 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.91459083557129  Time:  0.026855945587158203\n",
      "mean acc =  tensor(1.0098)  idx  147\n",
      "train loss 71.91459083557129\n",
      "test loss 55.653321532165116\n",
      "================================================== gen= 3 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.73241710662842  Time:  0.03148627281188965\n",
      "mean acc =  tensor(0.6342)  idx  147\n",
      "train loss 72.73241710662842\n",
      "test loss 64.22594968315695\n",
      "================================================== gen= 3 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.19031143188477  Time:  0.030356884002685547\n",
      "mean acc =  tensor(0.0819)  idx  147\n",
      "train loss 110.19031143188477\n",
      "test loss 106.3597450515851\n",
      "================================================== gen= 3 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.99521446228027  Time:  0.0247499942779541\n",
      "mean acc =  tensor(0.4667)  idx  147\n",
      "train loss 98.99521446228027\n",
      "test loss 73.56918532183381\n",
      "================================================== gen= 3 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.62426567077637  Time:  0.02684497833251953\n",
      "mean acc =  tensor(1.5114)  idx  147\n",
      "train loss 67.62426567077637\n",
      "test loss 65.41535957492128\n",
      "================================================== gen= 3 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.94488048553467  Time:  0.02597808837890625\n",
      "mean acc =  tensor(0.8459)  idx  147\n",
      "train loss 62.94488048553467\n",
      "test loss 55.68906340307119\n",
      "================================================== gen= 3 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.591097831726074  Time:  0.026150941848754883\n",
      "mean acc =  tensor(0.7067)  idx  147\n",
      "train loss 58.591097831726074\n",
      "test loss 63.40568119490228\n",
      "================================================== gen= 3 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  91.29985809326172  Time:  0.031138896942138672\n",
      "mean acc =  tensor(0.3611)  idx  147\n",
      "train loss 91.29985809326172\n",
      "test loss 82.04146770555145\n",
      "================================================== gen= 3 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.64594650268555  Time:  0.02779984474182129\n",
      "mean acc =  tensor(1.4872)  idx  147\n",
      "train loss 72.64594650268555\n",
      "test loss 57.02772703624907\n",
      "================================================== gen= 3 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.73054885864258  Time:  0.03058624267578125\n",
      "mean acc =  tensor(1.1223)  idx  147\n",
      "train loss 68.73054885864258\n",
      "test loss 51.22755133700208\n",
      "================================================== gen= 3 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  56.73487663269043  Time:  0.030288219451904297\n",
      "mean acc =  tensor(0.7644)  idx  147\n",
      "train loss 56.73487663269043\n",
      "test loss 57.96090176640725\n",
      "================================================== gen= 3 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.21498489379883  Time:  0.03127288818359375\n",
      "mean acc =  tensor(0.8828)  idx  147\n",
      "train loss 63.21498489379883\n",
      "test loss 52.36960690037734\n",
      "================================================== gen= 3 index 4 vector= 2\n",
      "updating model =======  52.36960690037734\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  114.20321655273438  Time:  0.02802586555480957\n",
      "mean acc =  tensor(0.1075)  idx  147\n",
      "train loss 114.20321655273438\n",
      "test loss 103.28212800317881\n",
      "================================================== gen= 3 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  92.18126487731934  Time:  0.024621009826660156\n",
      "mean acc =  tensor(0.5488)  idx  147\n",
      "train loss 92.18126487731934\n",
      "test loss 68.22653834025066\n",
      "================================================== gen= 3 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.21602821350098  Time:  0.02275681495666504\n",
      "mean acc =  tensor(1.6017)  idx  147\n",
      "train loss 65.21602821350098\n",
      "test loss 64.68977695906243\n",
      "================================================== gen= 3 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.39724636077881  Time:  0.02526092529296875\n",
      "mean acc =  tensor(0.9723)  idx  147\n",
      "train loss 62.39724636077881\n",
      "test loss 53.851201764580345\n",
      "================================================== gen= 3 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.84194469451904  Time:  0.024851083755493164\n",
      "mean acc =  tensor(0.6830)  idx  147\n",
      "train loss 63.84194469451904\n",
      "test loss 60.564496487987284\n",
      "================================================== gen= 3 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.2530746459961  Time:  0.024049043655395508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0308)  idx  147\n",
      "train loss 116.2530746459961\n",
      "test loss 112.05469632310933\n",
      "================================================== gen= 4 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  119.85103607177734  Time:  0.024860858917236328\n",
      "mean acc =  tensor(0.1948)  idx  147\n",
      "train loss 119.85103607177734\n",
      "test loss 95.61435196026653\n",
      "================================================== gen= 4 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  90.72250366210938  Time:  0.024744033813476562\n",
      "mean acc =  tensor(0.7395)  idx  147\n",
      "train loss 90.72250366210938\n",
      "test loss 62.895567705842105\n",
      "================================================== gen= 4 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.00808048248291  Time:  0.024152040481567383\n",
      "mean acc =  tensor(1.2593)  idx  147\n",
      "train loss 60.00808048248291\n",
      "test loss 59.88717708457895\n",
      "================================================== gen= 4 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.66718673706055  Time:  0.022812843322753906\n",
      "mean acc =  tensor(1.0380)  idx  147\n",
      "train loss 71.66718673706055\n",
      "test loss 55.30794719773896\n",
      "================================================== gen= 4 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  104.98312568664551  Time:  0.02573108673095703\n",
      "mean acc =  tensor(0.1708)  idx  147\n",
      "train loss 104.98312568664551\n",
      "test loss 97.91819036574591\n",
      "================================================== gen= 4 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  85.31379699707031  Time:  0.02567577362060547\n",
      "mean acc =  tensor(1.2350)  idx  147\n",
      "train loss 85.31379699707031\n",
      "test loss 53.64229677161392\n",
      "================================================== gen= 4 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.48795318603516  Time:  0.025907039642333984\n",
      "mean acc =  tensor(1.0971)  idx  147\n",
      "train loss 66.48795318603516\n",
      "test loss 52.83132662091936\n",
      "================================================== gen= 4 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.89614772796631  Time:  0.02585005760192871\n",
      "mean acc =  tensor(0.7259)  idx  147\n",
      "train loss 60.89614772796631\n",
      "test loss 59.21719666565357\n",
      "================================================== gen= 4 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.40677452087402  Time:  0.02508997917175293\n",
      "mean acc =  tensor(0.9762)  idx  147\n",
      "train loss 63.40677452087402\n",
      "test loss 53.42644282749721\n",
      "================================================== gen= 4 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  127.03682327270508  Time:  0.024235963821411133\n",
      "mean acc =  tensor(0.0351)  idx  147\n",
      "train loss 127.03682327270508\n",
      "test loss 111.21686631806043\n",
      "================================================== gen= 4 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  108.12790107727051  Time:  0.02441716194152832\n",
      "mean acc =  tensor(0.1142)  idx  147\n",
      "train loss 108.12790107727051\n",
      "test loss 102.81121519958081\n",
      "================================================== gen= 4 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  97.157470703125  Time:  0.024690866470336914\n",
      "mean acc =  tensor(0.4721)  idx  147\n",
      "train loss 97.157470703125\n",
      "test loss 76.7111025959456\n",
      "================================================== gen= 4 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.5127067565918  Time:  0.02439403533935547\n",
      "mean acc =  tensor(1.3832)  idx  147\n",
      "train loss 72.5127067565918\n",
      "test loss 67.2001720610119\n",
      "================================================== gen= 4 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.38701915740967  Time:  0.02185821533203125\n",
      "mean acc =  tensor(1.0506)  idx  147\n",
      "train loss 72.38701915740967\n",
      "test loss 56.24413826352074\n",
      "================================================== gen= 4 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.48477935791016  Time:  0.023141145706176758\n",
      "mean acc =  tensor(0.0716)  idx  147\n",
      "train loss 112.48477935791016\n",
      "test loss 107.70191872525378\n",
      "================================================== gen= 4 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  98.29770851135254  Time:  0.024271011352539062\n",
      "mean acc =  tensor(0.3042)  idx  147\n",
      "train loss 98.29770851135254\n",
      "test loss 85.15526515934744\n",
      "================================================== gen= 4 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.1796760559082  Time:  0.02699112892150879\n",
      "mean acc =  tensor(1.2186)  idx  147\n",
      "train loss 78.1796760559082\n",
      "test loss 52.713026059728094\n",
      "================================================== gen= 4 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.62313461303711  Time:  0.02425384521484375\n",
      "mean acc =  tensor(1.4046)  idx  147\n",
      "train loss 61.62313461303711\n",
      "test loss 53.6000343893661\n",
      "================================================== gen= 4 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.3960599899292  Time:  0.023889780044555664\n",
      "mean acc =  tensor(0.8485)  idx  147\n",
      "train loss 60.3960599899292\n",
      "test loss 54.22178234697199\n",
      "================================================== gen= 4 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.35087966918945  Time:  0.023257970809936523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0333)  idx  147\n",
      "train loss 111.35087966918945\n",
      "test loss 111.35793164311623\n",
      "================================================== gen= 5 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  112.28393173217773  Time:  0.02246713638305664\n",
      "mean acc =  tensor(0.1735)  idx  147\n",
      "train loss 112.28393173217773\n",
      "test loss 97.09823302184644\n",
      "================================================== gen= 5 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  92.83520698547363  Time:  0.023262977600097656\n",
      "mean acc =  tensor(0.6142)  idx  147\n",
      "train loss 92.83520698547363\n",
      "test loss 64.59080705188569\n",
      "================================================== gen= 5 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.1294174194336  Time:  0.02483677864074707\n",
      "mean acc =  tensor(1.6699)  idx  147\n",
      "train loss 66.1294174194336\n",
      "test loss 63.04027035771584\n",
      "================================================== gen= 5 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.851144790649414  Time:  0.02376389503479004\n",
      "mean acc =  tensor(1.1421)  idx  147\n",
      "train loss 63.851144790649414\n",
      "test loss 52.2443306592046\n",
      "================================================== gen= 5 index 4 vector= 0\n",
      "updating model =======  52.2443306592046\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  102.80308532714844  Time:  0.028647899627685547\n",
      "mean acc =  tensor(0.1631)  idx  147\n",
      "train loss 102.80308532714844\n",
      "test loss 98.31862095424107\n",
      "================================================== gen= 5 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  86.13229751586914  Time:  0.026189088821411133\n",
      "mean acc =  tensor(1.2425)  idx  147\n",
      "train loss 86.13229751586914\n",
      "test loss 57.19535869157233\n",
      "================================================== gen= 5 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  63.627275466918945  Time:  0.02381610870361328\n",
      "mean acc =  tensor(1.0703)  idx  147\n",
      "train loss 63.627275466918945\n",
      "test loss 55.63323499718491\n",
      "================================================== gen= 5 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.79822063446045  Time:  0.028078079223632812\n",
      "mean acc =  tensor(0.7238)  idx  147\n",
      "train loss 65.79822063446045\n",
      "test loss 62.05372362720723\n",
      "================================================== gen= 5 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  58.901883125305176  Time:  0.024352073669433594\n",
      "mean acc =  tensor(0.9176)  idx  147\n",
      "train loss 58.901883125305176\n",
      "test loss 55.514685883813975\n",
      "================================================== gen= 5 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  109.65867805480957  Time:  0.023663997650146484\n",
      "mean acc =  tensor(0.1030)  idx  147\n",
      "train loss 109.65867805480957\n",
      "test loss 103.63329195814067\n",
      "================================================== gen= 5 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  88.44197654724121  Time:  0.023307085037231445\n",
      "mean acc =  tensor(0.5590)  idx  147\n",
      "train loss 88.44197654724121\n",
      "test loss 69.40727402563809\n",
      "================================================== gen= 5 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  68.46595573425293  Time:  0.02352595329284668\n",
      "mean acc =  tensor(1.6619)  idx  147\n",
      "train loss 68.46595573425293\n",
      "test loss 60.11629470513792\n",
      "================================================== gen= 5 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.43959426879883  Time:  0.023090839385986328\n",
      "mean acc =  tensor(1.0280)  idx  147\n",
      "train loss 60.43959426879883\n",
      "test loss 52.071503671659094\n",
      "================================================== gen= 5 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.35004615783691  Time:  0.02410721778869629\n",
      "mean acc =  tensor(0.9420)  idx  147\n",
      "train loss 67.35004615783691\n",
      "test loss 53.400808477077355\n",
      "================================================== gen= 5 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  115.59522247314453  Time:  0.024925947189331055\n",
      "mean acc =  tensor(0.0308)  idx  147\n",
      "train loss 115.59522247314453\n",
      "test loss 112.04856291271392\n",
      "================================================== gen= 5 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  104.68008422851562  Time:  0.024044036865234375\n",
      "mean acc =  tensor(0.1726)  idx  147\n",
      "train loss 104.68008422851562\n",
      "test loss 98.04732461527091\n",
      "================================================== gen= 5 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  93.88279914855957  Time:  0.02434086799621582\n",
      "mean acc =  tensor(0.7480)  idx  147\n",
      "train loss 93.88279914855957\n",
      "test loss 63.48398564137569\n",
      "================================================== gen= 5 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.48836421966553  Time:  0.024852991104125977\n",
      "mean acc =  tensor(1.6287)  idx  147\n",
      "train loss 71.48836421966553\n",
      "test loss 70.47091443846826\n",
      "================================================== gen= 5 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.06774711608887  Time:  0.024321794509887695\n",
      "mean acc =  tensor(0.8593)  idx  147\n",
      "train loss 69.06774711608887\n",
      "test loss 58.539416669988306\n",
      "================================================== gen= 5 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.01359748840332  Time:  0.022262096405029297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0328)  idx  147\n",
      "train loss 116.01359748840332\n",
      "test loss 111.41425780211988\n",
      "================================================== gen= 6 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.6138801574707  Time:  0.022756099700927734\n",
      "mean acc =  tensor(0.0964)  idx  147\n",
      "train loss 101.6138801574707\n",
      "test loss 105.08603159586589\n",
      "================================================== gen= 6 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.35391998291016  Time:  0.022705078125\n",
      "mean acc =  tensor(0.3204)  idx  147\n",
      "train loss 101.35391998291016\n",
      "test loss 85.01440447852725\n",
      "================================================== gen= 6 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.5598497390747  Time:  0.023579835891723633\n",
      "mean acc =  tensor(1.1349)  idx  147\n",
      "train loss 72.5598497390747\n",
      "test loss 55.366814879333084\n",
      "================================================== gen= 6 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.06912517547607  Time:  0.022349834442138672\n",
      "mean acc =  tensor(1.2031)  idx  147\n",
      "train loss 65.06912517547607\n",
      "test loss 55.5473318294603\n",
      "================================================== gen= 6 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.43310737609863  Time:  0.02503204345703125\n",
      "mean acc =  tensor(0.2401)  idx  147\n",
      "train loss 111.43310737609863\n",
      "test loss 92.31305310515319\n",
      "================================================== gen= 6 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.3538408279419  Time:  0.0245821475982666\n",
      "mean acc =  tensor(1.6300)  idx  147\n",
      "train loss 78.3538408279419\n",
      "test loss 65.69315177242773\n",
      "================================================== gen= 6 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.9154634475708  Time:  0.023741960525512695\n",
      "mean acc =  tensor(0.8501)  idx  147\n",
      "train loss 66.9154634475708\n",
      "test loss 57.69478158237172\n",
      "================================================== gen= 6 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.6890344619751  Time:  0.024369001388549805\n",
      "mean acc =  tensor(0.6896)  idx  147\n",
      "train loss 66.6890344619751\n",
      "test loss 62.22632803884493\n",
      "================================================== gen= 6 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.0009994506836  Time:  0.02474498748779297\n",
      "mean acc =  tensor(1.2391)  idx  147\n",
      "train loss 64.0009994506836\n",
      "test loss 53.565555857963304\n",
      "================================================== gen= 6 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  118.22006034851074  Time:  0.022490262985229492\n",
      "mean acc =  tensor(0.0313)  idx  147\n",
      "train loss 118.22006034851074\n",
      "test loss 111.62617492675781\n",
      "================================================== gen= 6 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  106.63536262512207  Time:  0.02408599853515625\n",
      "mean acc =  tensor(0.0855)  idx  147\n",
      "train loss 106.63536262512207\n",
      "test loss 105.76233470683195\n",
      "================================================== gen= 6 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  104.31803894042969  Time:  0.022008657455444336\n",
      "mean acc =  tensor(0.3369)  idx  147\n",
      "train loss 104.31803894042969\n",
      "test loss 83.75591329976815\n",
      "================================================== gen= 6 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.59120845794678  Time:  0.0219271183013916\n",
      "mean acc =  tensor(1.3284)  idx  147\n",
      "train loss 64.59120845794678\n",
      "test loss 52.26443606006856\n",
      "================================================== gen= 6 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.02696418762207  Time:  0.0231170654296875\n",
      "mean acc =  tensor(1.3652)  idx  147\n",
      "train loss 62.02696418762207\n",
      "test loss 53.00622374346467\n",
      "================================================== gen= 6 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  107.17648315429688  Time:  0.024054765701293945\n",
      "mean acc =  tensor(0.0746)  idx  147\n",
      "train loss 107.17648315429688\n",
      "test loss 107.10605662858406\n",
      "================================================== gen= 6 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  101.48715019226074  Time:  0.022801876068115234\n",
      "mean acc =  tensor(0.3569)  idx  147\n",
      "train loss 101.48715019226074\n",
      "test loss 81.92303824911312\n",
      "================================================== gen= 6 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.85923767089844  Time:  0.02364063262939453\n",
      "mean acc =  tensor(1.5008)  idx  147\n",
      "train loss 72.85923767089844\n",
      "test loss 59.6576485666288\n",
      "================================================== gen= 6 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.696852684021  Time:  0.025092124938964844\n",
      "mean acc =  tensor(1.1429)  idx  147\n",
      "train loss 66.696852684021\n",
      "test loss 53.37412897745768\n",
      "================================================== gen= 6 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.45950889587402  Time:  0.025434255599975586\n",
      "mean acc =  tensor(0.8872)  idx  147\n",
      "train loss 64.45950889587402\n",
      "test loss 54.8480123403121\n",
      "================================================== gen= 6 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.72129440307617  Time:  0.022140979766845703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0957)  idx  147\n",
      "train loss 116.72129440307617\n",
      "test loss 104.64453021198715\n",
      "================================================== gen= 7 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  90.94810485839844  Time:  0.021737098693847656\n",
      "mean acc =  tensor(0.2966)  idx  147\n",
      "train loss 90.94810485839844\n",
      "test loss 85.67702203867387\n",
      "================================================== gen= 7 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  76.28665733337402  Time:  0.023119211196899414\n",
      "mean acc =  tensor(0.9296)  idx  147\n",
      "train loss 76.28665733337402\n",
      "test loss 55.15595769557823\n",
      "================================================== gen= 7 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.06854820251465  Time:  0.023432254791259766\n",
      "mean acc =  tensor(1.4837)  idx  147\n",
      "train loss 72.06854820251465\n",
      "test loss 62.69136101858957\n",
      "================================================== gen= 7 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.77739143371582  Time:  0.023031949996948242\n",
      "mean acc =  tensor(0.9934)  idx  147\n",
      "train loss 59.77739143371582\n",
      "test loss 53.483814602806454\n",
      "================================================== gen= 7 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  103.25289726257324  Time:  0.02611708641052246\n",
      "mean acc =  tensor(0.1967)  idx  147\n",
      "train loss 103.25289726257324\n",
      "test loss 94.1024612115354\n",
      "================================================== gen= 7 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  78.97201728820801  Time:  0.024385929107666016\n",
      "mean acc =  tensor(1.3181)  idx  147\n",
      "train loss 78.97201728820801\n",
      "test loss 56.9498513928887\n",
      "================================================== gen= 7 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.6535758972168  Time:  0.024838924407958984\n",
      "mean acc =  tensor(1.1183)  idx  147\n",
      "train loss 66.6535758972168\n",
      "test loss 53.22612043627265\n",
      "================================================== gen= 7 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  57.390265464782715  Time:  0.025595903396606445\n",
      "mean acc =  tensor(0.8758)  idx  147\n",
      "train loss 57.390265464782715\n",
      "test loss 54.91519084593066\n",
      "================================================== gen= 7 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  54.935404777526855  Time:  0.02554774284362793\n",
      "mean acc =  tensor(1.3469)  idx  147\n",
      "train loss 54.935404777526855\n",
      "test loss 52.6211221915524\n",
      "================================================== gen= 7 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  111.5527515411377  Time:  0.02353692054748535\n",
      "mean acc =  tensor(0.2070)  idx  147\n",
      "train loss 111.5527515411377\n",
      "test loss 95.1897208804176\n",
      "================================================== gen= 7 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  76.75512504577637  Time:  0.022990942001342773\n",
      "mean acc =  tensor(1.0242)  idx  147\n",
      "train loss 76.75512504577637\n",
      "test loss 54.4464243934268\n",
      "================================================== gen= 7 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.86571502685547  Time:  0.024312973022460938\n",
      "mean acc =  tensor(1.3017)  idx  147\n",
      "train loss 71.86571502685547\n",
      "test loss 55.18724070438722\n",
      "================================================== gen= 7 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  56.09498405456543  Time:  0.023410797119140625\n",
      "mean acc =  tensor(0.8701)  idx  147\n",
      "train loss 56.09498405456543\n",
      "test loss 57.24916320593179\n",
      "================================================== gen= 7 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.53569030761719  Time:  0.02390599250793457\n",
      "mean acc =  tensor(1.0214)  idx  147\n",
      "train loss 64.53569030761719\n",
      "test loss 53.15907440704553\n",
      "================================================== gen= 7 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  108.6817398071289  Time:  0.022616863250732422\n",
      "mean acc =  tensor(0.0788)  idx  147\n",
      "train loss 108.6817398071289\n",
      "test loss 106.6908801331812\n",
      "================================================== gen= 7 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  86.47930717468262  Time:  0.02228379249572754\n",
      "mean acc =  tensor(0.3554)  idx  147\n",
      "train loss 86.47930717468262\n",
      "test loss 83.47839721368284\n",
      "================================================== gen= 7 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  73.66937828063965  Time:  0.024196863174438477\n",
      "mean acc =  tensor(1.4322)  idx  147\n",
      "train loss 73.66937828063965\n",
      "test loss 56.352630641184696\n",
      "================================================== gen= 7 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.85731506347656  Time:  0.024068117141723633\n",
      "mean acc =  tensor(1.2510)  idx  147\n",
      "train loss 71.85731506347656\n",
      "test loss 54.753815735278486\n",
      "================================================== gen= 7 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.290343284606934  Time:  0.02493596076965332\n",
      "mean acc =  tensor(0.7351)  idx  147\n",
      "train loss 60.290343284606934\n",
      "test loss 58.320441992104456\n",
      "================================================== gen= 7 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  113.33986854553223  Time:  0.02294611930847168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.0436)  idx  147\n",
      "train loss 113.33986854553223\n",
      "test loss 110.20830115493463\n",
      "================================================== gen= 8 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  105.80594825744629  Time:  0.022619962692260742\n",
      "mean acc =  tensor(0.2773)  idx  147\n",
      "train loss 105.80594825744629\n",
      "test loss 88.09672530816526\n",
      "================================================== gen= 8 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  79.78704071044922  Time:  0.02265477180480957\n",
      "mean acc =  tensor(1.0764)  idx  147\n",
      "train loss 79.78704071044922\n",
      "test loss 57.109925743673934\n",
      "================================================== gen= 8 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  71.54425048828125  Time:  0.022436141967773438\n",
      "mean acc =  tensor(1.2507)  idx  147\n",
      "train loss 71.54425048828125\n",
      "test loss 57.09999722850566\n",
      "================================================== gen= 8 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.63464450836182  Time:  0.02278614044189453\n",
      "mean acc =  tensor(0.8740)  idx  147\n",
      "train loss 64.63464450836182\n",
      "test loss 58.85533910219361\n",
      "================================================== gen= 8 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  110.6272201538086  Time:  0.025838851928710938\n",
      "mean acc =  tensor(0.2559)  idx  147\n",
      "train loss 110.6272201538086\n",
      "test loss 89.86577144285448\n",
      "================================================== gen= 8 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  70.0848798751831  Time:  0.0246889591217041\n",
      "mean acc =  tensor(1.5574)  idx  147\n",
      "train loss 70.0848798751831\n",
      "test loss 58.62543277351224\n",
      "================================================== gen= 8 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  66.57999897003174  Time:  0.02580714225769043\n",
      "mean acc =  tensor(1.0426)  idx  147\n",
      "train loss 66.57999897003174\n",
      "test loss 52.66980442384473\n",
      "================================================== gen= 8 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.995140075683594  Time:  0.0245208740234375\n",
      "mean acc =  tensor(0.7439)  idx  147\n",
      "train loss 59.995140075683594\n",
      "test loss 59.647116005826156\n",
      "================================================== gen= 8 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  55.31973648071289  Time:  0.024993896484375\n",
      "mean acc =  tensor(1.1320)  idx  147\n",
      "train loss 55.31973648071289\n",
      "test loss 51.75351258362232\n",
      "================================================== gen= 8 index 4 vector= 1\n",
      "updating model =======  51.75351258362232\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  117.08197021484375  Time:  0.024111032485961914\n",
      "mean acc =  tensor(0.1162)  idx  147\n",
      "train loss 117.08197021484375\n",
      "test loss 101.92671421595982\n",
      "================================================== gen= 8 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  83.75431251525879  Time:  0.023045778274536133\n",
      "mean acc =  tensor(0.5554)  idx  147\n",
      "train loss 83.75431251525879\n",
      "test loss 69.15827586375126\n",
      "================================================== gen= 8 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.24079036712646  Time:  0.02365708351135254\n",
      "mean acc =  tensor(1.6098)  idx  147\n",
      "train loss 65.24079036712646\n",
      "test loss 65.76444994349058\n",
      "================================================== gen= 8 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  60.56370258331299  Time:  0.023509979248046875\n",
      "mean acc =  tensor(1.0985)  idx  147\n",
      "train loss 60.56370258331299\n",
      "test loss 53.14190435085167\n",
      "================================================== gen= 8 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.85958862304688  Time:  0.024311065673828125\n",
      "mean acc =  tensor(0.8891)  idx  147\n",
      "train loss 64.85958862304688\n",
      "test loss 56.57719758740899\n",
      "================================================== gen= 8 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  109.67848205566406  Time:  0.023947954177856445\n",
      "mean acc =  tensor(0.0743)  idx  147\n",
      "train loss 109.67848205566406\n",
      "test loss 106.60715764882613\n",
      "================================================== gen= 8 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  92.17257881164551  Time:  0.023296833038330078\n",
      "mean acc =  tensor(0.4635)  idx  147\n",
      "train loss 92.17257881164551\n",
      "test loss 73.23358128346554\n",
      "================================================== gen= 8 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.44697570800781  Time:  0.023829936981201172\n",
      "mean acc =  tensor(1.4866)  idx  147\n",
      "train loss 64.44697570800781\n",
      "test loss 58.53052868486262\n",
      "================================================== gen= 8 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.12194538116455  Time:  0.023529052734375\n",
      "mean acc =  tensor(0.8366)  idx  147\n",
      "train loss 67.12194538116455\n",
      "test loss 58.377533971046915\n",
      "================================================== gen= 8 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  59.80777931213379  Time:  0.023383617401123047\n",
      "mean acc =  tensor(0.7729)  idx  147\n",
      "train loss 59.80777931213379\n",
      "test loss 59.016499499885406\n",
      "================================================== gen= 8 index 4 vector= 3\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 112, 9, 89, 44, 12, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=112, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=112, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=44, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=44, out_features=12, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  109.09031295776367  Time:  0.021270036697387695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc =  tensor(0.1095)  idx  147\n",
      "train loss 109.09031295776367\n",
      "test loss 103.44905152612803\n",
      "================================================== gen= 9 index 0 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  93.87698936462402  Time:  0.02445697784423828\n",
      "mean acc =  tensor(0.5059)  idx  147\n",
      "train loss 93.87698936462402\n",
      "test loss 74.00206587914707\n",
      "================================================== gen= 9 index 1 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  75.81063079833984  Time:  0.022105932235717773\n",
      "mean acc =  tensor(1.4493)  idx  147\n",
      "train loss 75.81063079833984\n",
      "test loss 59.54752098135397\n",
      "================================================== gen= 9 index 2 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.96648979187012  Time:  0.02266693115234375\n",
      "mean acc =  tensor(1.3661)  idx  147\n",
      "train loss 69.96648979187012\n",
      "test loss 56.515529684469\n",
      "================================================== gen= 9 index 3 vector= 0\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  64.65702056884766  Time:  0.02290797233581543\n",
      "mean acc =  tensor(0.8127)  idx  147\n",
      "train loss 64.65702056884766\n",
      "test loss 58.53978760388433\n",
      "================================================== gen= 9 index 4 vector= 0\n",
      "create_model =  [7, 16, 131, 106, 112, 80, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=131, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=131, out_features=106, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=106, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=80, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  99.02558898925781  Time:  0.025543928146362305\n",
      "mean acc =  tensor(0.3897)  idx  147\n",
      "train loss 99.02558898925781\n",
      "test loss 79.29802529990268\n",
      "================================================== gen= 9 index 0 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.14888572692871  Time:  0.024353981018066406\n",
      "mean acc =  tensor(1.5185)  idx  147\n",
      "train loss 72.14888572692871\n",
      "test loss 65.95062468652011\n",
      "================================================== gen= 9 index 1 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  62.858704566955566  Time:  0.02475595474243164\n",
      "mean acc =  tensor(0.7315)  idx  147\n",
      "train loss 62.858704566955566\n",
      "test loss 61.16282332024607\n",
      "================================================== gen= 9 index 2 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  65.44981575012207  Time:  0.02596306800842285\n",
      "mean acc =  tensor(0.7802)  idx  147\n",
      "train loss 65.44981575012207\n",
      "test loss 59.96020173053352\n",
      "================================================== gen= 9 index 3 vector= 1\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  52.154282569885254  Time:  0.02515578269958496\n",
      "mean acc =  tensor(1.2240)  idx  147\n",
      "train loss 52.154282569885254\n",
      "test loss 52.95827637237756\n",
      "================================================== gen= 9 index 4 vector= 1\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  103.13613319396973  Time:  0.024684906005859375\n",
      "mean acc =  tensor(0.1597)  idx  147\n",
      "train loss 103.13613319396973\n",
      "test loss 99.23737776360544\n",
      "================================================== gen= 9 index 0 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  87.89665412902832  Time:  0.023801088333129883\n",
      "mean acc =  tensor(0.6174)  idx  147\n",
      "train loss 87.89665412902832\n",
      "test loss 64.63591265516216\n",
      "================================================== gen= 9 index 1 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  67.18486499786377  Time:  0.023334264755249023\n",
      "mean acc =  tensor(1.4570)  idx  147\n",
      "train loss 67.18486499786377\n",
      "test loss 60.398417933457566\n",
      "================================================== gen= 9 index 2 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  69.61268138885498  Time:  0.06512093544006348\n",
      "mean acc =  tensor(0.9258)  idx  147\n",
      "train loss 69.61268138885498\n",
      "test loss 53.45161373112477\n",
      "================================================== gen= 9 index 3 vector= 2\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  56.63314914703369  Time:  0.023505210876464844\n",
      "mean acc =  tensor(0.8747)  idx  147\n",
      "train loss 56.63314914703369\n",
      "test loss 55.86347380138579\n",
      "================================================== gen= 9 index 4 vector= 2\n",
      "create_model =  [7, 38, 44, 46, 28, 120, 36, 124, 45, 42, 140, 1]\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=38, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=28, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=28, out_features=120, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=120, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=124, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=124, out_features=45, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=45, out_features=42, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=42, out_features=140, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=140, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  116.01074600219727  Time:  0.02385687828063965\n",
      "mean acc =  tensor(0.0280)  idx  147\n",
      "train loss 116.01074600219727\n",
      "test loss 111.92079510331965\n",
      "================================================== gen= 9 index 0 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  118.13599014282227  Time:  0.023764848709106445\n",
      "mean acc =  tensor(0.1873)  idx  147\n",
      "train loss 118.13599014282227\n",
      "test loss 95.57212554516435\n",
      "================================================== gen= 9 index 1 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  80.21159362792969  Time:  0.023538827896118164\n",
      "mean acc =  tensor(0.9553)  idx  147\n",
      "train loss 80.21159362792969\n",
      "test loss 55.68613745241749\n",
      "================================================== gen= 9 index 2 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  72.12006568908691  Time:  0.0221712589263916\n",
      "mean acc =  tensor(1.3237)  idx  147\n",
      "train loss 72.12006568908691\n",
      "test loss 55.04825885279649\n",
      "================================================== gen= 9 index 3 vector= 3\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  61.78233528137207  Time:  0.022639036178588867\n",
      "mean acc =  tensor(0.9065)  idx  147\n",
      "train loss 61.78233528137207\n",
      "test loss 54.658390356569875\n",
      "================================================== gen= 9 index 4 vector= 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epoch = 5\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "#model2,optimizer2 = create_model()\n",
    "acc1 =0\n",
    "acc2 =0\n",
    "\n",
    "arx = create_ar(8)\n",
    "best , ar1,ar2 = next_gen(arx)\n",
    "#This is the 4 models that we are working on \n",
    "loops = [best,ar1,ar2,arx]\n",
    "\n",
    "target_columns='DiscountPerc'\n",
    "#target_columns = 'perc'\n",
    "results =[]\n",
    "best_score =100000000\n",
    "best_index =-1\n",
    "a,b,best =[],[],[]\n",
    "for generations in range (10):\n",
    "    #plt.plot(Test_acc)\n",
    "    #Loop over the models and choose the best one.\n",
    "    for index in range(4):\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        model ,optimizer ,ar = create_model(loops[index])\n",
    "        temp_model = model\n",
    "        temp_optimizer = optimizer\n",
    "       \n",
    "        for i in range(n_epoch):\n",
    "            #print (df_train.head())\n",
    "            train_loader = create_set(1000,df_train,target_columns)\n",
    "            #print(df_train.head())\n",
    "            train_loss = train_epoch(temp_model,train_loader,criterion,temp_optimizer)\n",
    "            test_loss ,acc= test_epoch(temp_model,validate_loader,criterion)\n",
    "            \n",
    "            #Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "            print(\"train loss\",train_loss)\n",
    "            print(\"test loss\" , test_loss)\n",
    "            #Test_acc.append(test_acc)\n",
    "            print('='*50,'gen=',generations,'index',i,'vector=',index)\n",
    "        \n",
    "        if (test_loss < best_score):\n",
    "            print (\"updating model ======= \", test_loss)\n",
    "            best_model = temp_model\n",
    "            best_optimizer = temp_optimizer\n",
    "            best_score = test_loss\n",
    "            best_index = index\n",
    "        \n",
    "        results.append(test_loss)\n",
    "    \n",
    "    \n",
    "    a,b,best = next_gen(loops[index])\n",
    "    c=create_ar(8)\n",
    "    results =[]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target column is  DiscountPerc\n",
      "Train Loss:  47.72405126143475  Time:  1.314159870147705\n",
      "mean acc =  tensor(1.0153)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  38.01370271371336  Time:  1.2075538635253906\n",
      "mean acc =  tensor(1.3824)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  34.886505584327544  Time:  1.2237937450408936\n",
      "mean acc =  tensor(1.5142)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  34.24807393794157  Time:  1.3080320358276367\n",
      "mean acc =  tensor(1.4587)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  33.32326328511141  Time:  1.3051528930664062\n",
      "mean acc =  tensor(1.4646)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  33.548216002328054  Time:  1.307692050933838\n",
      "mean acc =  tensor(1.5108)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  33.569760089017905  Time:  1.3510689735412598\n",
      "mean acc =  tensor(1.4646)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.55534246989659  Time:  1.3334758281707764\n",
      "mean acc =  tensor(1.5060)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.719926493508474  Time:  1.3640398979187012\n",
      "mean acc =  tensor(1.4061)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.66074470597871  Time:  1.3304221630096436\n",
      "mean acc =  tensor(1.4413)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.74019337673577  Time:  1.369488000869751\n",
      "mean acc =  tensor(1.4328)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.43729639053345  Time:  1.367232084274292\n",
      "mean acc =  tensor(1.4545)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.73104729944346  Time:  1.2905511856079102\n",
      "mean acc =  tensor(1.5513)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.814479214804514  Time:  1.3364357948303223\n",
      "mean acc =  tensor(1.4960)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  32.542062496652406  Time:  1.327096939086914\n",
      "mean acc =  tensor(1.4710)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.913549588651072  Time:  1.3739571571350098\n",
      "mean acc =  tensor(1.5093)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.79983579869173  Time:  1.3285210132598877\n",
      "mean acc =  tensor(1.4644)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.961340982086803  Time:  1.361496925354004\n",
      "mean acc =  tensor(1.3871)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.808758482641103  Time:  1.3303120136260986\n",
      "mean acc =  tensor(1.3992)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.665592534201487  Time:  1.3262100219726562\n",
      "mean acc =  tensor(1.5789)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.63394125140443  Time:  1.2845969200134277\n",
      "mean acc =  tensor(1.5089)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.429953399969605  Time:  1.303351879119873\n",
      "mean acc =  tensor(1.4566)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.888492360406993  Time:  1.3455548286437988\n",
      "mean acc =  tensor(1.4397)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.43731689453125  Time:  1.3364951610565186\n",
      "mean acc =  tensor(1.4325)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.70908887045724  Time:  1.2912049293518066\n",
      "mean acc =  tensor(1.3519)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.45544418023557  Time:  1.3360300064086914\n",
      "mean acc =  tensor(1.5477)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.885148875567378  Time:  1.3679227828979492\n",
      "mean acc =  tensor(1.3570)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.744358909373382  Time:  1.354963779449463\n",
      "mean acc =  tensor(1.5020)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.326599724438726  Time:  1.3012721538543701\n",
      "mean acc =  tensor(1.4840)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  31.78948264219323  Time:  1.2908148765563965\n",
      "mean acc =  tensor(1.4714)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.26348352432251  Time:  1.3386828899383545\n",
      "mean acc =  tensor(1.4207)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.717031829211177  Time:  1.287687063217163\n",
      "mean acc =  tensor(1.3557)  idx  147\n",
      " target column is  DiscountPerc\n",
      "Train Loss:  30.90259614282725  Time:  1.2854537963867188\n",
      "mean acc =  tensor(1.4384)  idx  147\n",
      "[tensor(1.0153), tensor(1.3824), tensor(1.5142), tensor(1.4587), tensor(1.4646), tensor(1.5108), tensor(1.4646), tensor(1.5060), tensor(1.4061), tensor(1.4413), tensor(1.4328), tensor(1.4545), tensor(1.5513), tensor(1.4960), tensor(1.4710), tensor(1.5093), tensor(1.4644), tensor(1.3871), tensor(1.3992), tensor(1.5789), tensor(1.5089), tensor(1.4566), tensor(1.4397), tensor(1.4325), tensor(1.3519), tensor(1.5477), tensor(1.3570), tensor(1.5020), tensor(1.4840), tensor(1.4714), tensor(1.4207), tensor(1.3557), tensor(1.4384)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(33):\n",
    "            train_loader = create_set(50000,df_train,target_columns)\n",
    "            train_loss = train_epoch(best_model,train_loader,criterion,best_optimizer)\n",
    "            test_loss ,acc= test_epoch(best_model,validate_loader,criterion)\n",
    "\n",
    "            Test_acc.append(acc)\n",
    "            Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "      \n",
    "            #print('='*50,i)\n",
    "\n",
    "print (Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.0153), tensor(1.3824), tensor(1.5142), tensor(1.4587), tensor(1.4646), tensor(1.5108), tensor(1.4646), tensor(1.5060), tensor(1.4061), tensor(1.4413), tensor(1.4328), tensor(1.4545), tensor(1.5513), tensor(1.4960), tensor(1.4710), tensor(1.5093), tensor(1.4644), tensor(1.3871), tensor(1.3992), tensor(1.5789), tensor(1.5089), tensor(1.4566), tensor(1.4397), tensor(1.4325), tensor(1.3519), tensor(1.5477), tensor(1.3570), tensor(1.5020), tensor(1.4840), tensor(1.4714), tensor(1.4207), tensor(1.3557), tensor(1.4384)]\n"
     ]
    }
   ],
   "source": [
    "print (Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda31ed49d0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss ')\n",
    "plt.plot(Test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda31dd0310>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss ')\n",
    "plt.plot(Train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda31eecb10>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_epoch(model,data):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        results = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        for batch_idx, data in enumerate(data):\n",
    "          \n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            print (outputs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.3849])\n",
      "tensor([9.1241])\n",
      "tensor([14.2351])\n",
      "tensor([8.6868])\n",
      "tensor([9.4235])\n",
      "tensor([7.0301])\n",
      "tensor([10.2613])\n",
      "tensor([7.9240])\n",
      "tensor([7.1547])\n",
      "tensor([9.1880])\n",
      "tensor([8.4165])\n",
      "tensor([9.1858])\n",
      "tensor([7.9015])\n",
      "tensor([8.7741])\n",
      "tensor([8.4093])\n",
      "tensor([10.1271])\n",
      "tensor([7.5150])\n",
      "tensor([9.9688])\n",
      "tensor([6.9941])\n",
      "tensor([9.2114])\n",
      "tensor([9.0556])\n",
      "tensor([10.2584])\n",
      "tensor([8.9732])\n",
      "tensor([9.0631])\n",
      "tensor([7.0985])\n",
      "tensor([8.6198])\n",
      "tensor([7.2656])\n",
      "tensor([8.7722])\n",
      "tensor([8.8034])\n",
      "tensor([9.0905])\n",
      "tensor([7.7248])\n",
      "tensor([8.0636])\n",
      "tensor([7.2804])\n",
      "tensor([9.6007])\n",
      "tensor([7.1495])\n",
      "tensor([9.9366])\n",
      "tensor([8.5295])\n",
      "tensor([10.0546])\n",
      "tensor([8.9145])\n",
      "tensor([7.6236])\n",
      "tensor([6.7978])\n",
      "tensor([8.8466])\n",
      "tensor([9.8116])\n",
      "tensor([9.3323])\n",
      "tensor([8.9645])\n",
      "tensor([10.1289])\n",
      "tensor([7.7619])\n",
      "tensor([11.2688])\n",
      "tensor([8.3871])\n",
      "tensor([11.6405])\n",
      "tensor([8.6625])\n",
      "tensor([6.8721])\n",
      "tensor([8.8643])\n",
      "tensor([8.3185])\n",
      "tensor([9.7484])\n",
      "tensor([7.4722])\n",
      "tensor([9.3612])\n",
      "tensor([9.1561])\n",
      "tensor([8.4569])\n",
      "tensor([8.2634])\n",
      "tensor([9.0869])\n",
      "tensor([8.0444])\n",
      "tensor([10.1996])\n",
      "tensor([7.6426])\n",
      "tensor([8.1074])\n",
      "tensor([8.8243])\n",
      "tensor([8.1347])\n",
      "tensor([10.1170])\n",
      "tensor([9.9597])\n",
      "tensor([8.0714])\n",
      "tensor([9.2331])\n",
      "tensor([7.9082])\n",
      "tensor([8.5021])\n",
      "tensor([9.1367])\n",
      "tensor([8.6023])\n",
      "tensor([9.9131])\n",
      "tensor([10.2378])\n",
      "tensor([9.2288])\n",
      "tensor([8.4557])\n",
      "tensor([9.7587])\n",
      "tensor([8.8637])\n",
      "tensor([8.4030])\n",
      "tensor([15.0727])\n",
      "tensor([8.3633])\n",
      "tensor([14.4037])\n",
      "tensor([7.5314])\n",
      "tensor([8.2554])\n",
      "tensor([9.9741])\n",
      "tensor([9.2593])\n",
      "tensor([9.5341])\n",
      "tensor([9.4365])\n",
      "tensor([8.9889])\n",
      "tensor([9.5082])\n",
      "tensor([6.8290])\n",
      "tensor([7.6763])\n",
      "tensor([8.0663])\n",
      "tensor([8.0170])\n",
      "tensor([8.0205])\n",
      "tensor([8.7571])\n",
      "tensor([7.9782])\n"
     ]
    }
   ],
   "source": [
    "X = df_train.sample(n=10000)\n",
    "\n",
    "X.describe()\n",
    "\n",
    "newset = create_set2(100,X,target_columns)\n",
    "submit_epoch(best_model,newset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

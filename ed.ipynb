{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "pic_size = 256\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./data',train=True,download=True,transform =transforms.ToTensor())\n",
    "test = MNIST('./data',train=False,download=True,transform =transforms.ToTensor())\n",
    "train_data = train.data\n",
    "train_data = train.transform(train_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "train.data.size()\n",
    "print(len(test.data.numpy() ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd6b47b3090>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train.data.cpu().numpy()[923])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index].float()\n",
    "        Y = self.Y[index].long()\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import seaborn as sns\n",
    "#sns.set_style('whitegrid')\n",
    "from datetime import datetime\n",
    "sys.platform\n",
    "df_train = pandas.read_csv('/Users/bebik/Downloads/data/train.csv',header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    #device = get_device()\n",
    "    return torch.from_numpy(df.values).float() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "0      1\n",
      "1      0\n",
      "2      1\n",
      "3      0\n",
      "4      0\n",
      "...   ..\n",
      "49995  0\n",
      "49996  1\n",
      "49997  0\n",
      "49998  1\n",
      "49999  1\n",
      "\n",
      "[50000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader(df):\n",
    "    target = pd.DataFrame(df[df.columns[0]] )\n",
    "    print(target)\n",
    "    ten_target = torch.from_numpy(target[target.columns[0]].values)\n",
    "    del df[df.columns[0] ]\n",
    "    ten_data = df_to_tensor(df)\n",
    "\n",
    "\n",
    "    _dataset = MyDataset(ten_data,ten_target)\n",
    "\n",
    "\n",
    "\n",
    "    test_loader_args = dict(shuffle=True,batch_size=pic_size,num_workers=0,pin_memory=True) if cuda\\\n",
    "    else dict(shuffle=True,batch_size=pic_size)\n",
    "    loader = data.DataLoader(_dataset,**test_loader_args)\n",
    "    return ten_target,loader\n",
    "\n",
    "\n",
    "\n",
    "df_validate = pandas.read_csv('/Users/bebik/Downloads/data/validate.csv',header=None)\n",
    "\n",
    "\n",
    "validate_target, validate_loader = create_data_loader(df_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pandas.read_csv('/Users/bebik/Downloads/data/test.csv',header=None)\n",
    "del df_test[df_test.columns[0] ]\n",
    "test_data = df_to_tensor(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set(num,df):\n",
    "\n",
    "    if (num == 0):\n",
    "        df_train_sample = df\n",
    "    else :\n",
    "        df_train_sample = df.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[df_train_sample.columns[0]] )\n",
    "    del df_train_sample[df_train_sample.columns[0]]\n",
    "    ten_train_target = torch.from_numpy(target_sample[target_sample.columns[0]].values)\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = MyDataset(ten_train_data,ten_train_target)\n",
    "\n",
    "\n",
    "    train_loader_args = dict(shuffle=True,batch_size=pic_size,num_workers=0,pin_memory=True) if cuda\\\n",
    "    else dict(shuffle=True,batch_size=pic_size)\n",
    "    train_loader = data.DataLoader(train_dataset,**train_loader_args)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple_MLP([784,100,50,10])\n",
    "\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self,size_list):\n",
    "        super(Simple_MLP,self).__init__()\n",
    "        layers=[]\n",
    "        self.size_list = size_list\n",
    "        for i in range(len(size_list) -2):\n",
    "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "            op = random.randint(0,2) \n",
    "            if (op==0):\n",
    "                layers.append(nn.ReLU())\n",
    "            if (op ==1): \n",
    "                layers.append(nn.ReLU())\n",
    "            if (op ==2):\n",
    "                layers.append(nn.ReLU())\n",
    "        #layers.append(nn.Linear(size_list[-3],size_list[-2]))\n",
    "        #layers.append(nn.Softmax(dim=1))\n",
    "        layers.append(nn.Linear(size_list[-2],size_list[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=102, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=102, out_features=15, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=15, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=20, out_features=12, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=12, out_features=69, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=69, out_features=71, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=71, out_features=86, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=86, out_features=34, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=34, out_features=57, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=57, out_features=113, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=113, out_features=60, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=60, out_features=39, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=39, out_features=96, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=96, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the network for training\n",
    "#model = Simple_MLP([784,16,50,8,200,10])\n",
    "import random\n",
    "#model = Simple_MLP([120,16,200,10])\n",
    "#model = Simple_MLP([120,80,40,20,12,2])\n",
    "\n",
    "#model = Simple_MLP([120,80,60,40,20,2])\n",
    "#model = Simple_MLP([120,110,106,104,100,90,88,82,80,70,60,50,40,60,80,30,20,10,8,6,4,4,2])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device( \"cpu\")\n",
    "\n",
    "def create_model():\n",
    "    r1 = 160\n",
    "    r2 = 160\n",
    "    ar = [120]\n",
    "    for i in range(random.randint(1,10)):\n",
    "        r1 = random.randint(4,140)\n",
    "        ar.append( r1  )\n",
    "        r2 = random.randint(4,140)\n",
    "        ar.append(  r2 )\n",
    "    ar.append(2)    \n",
    "    model = Simple_MLP(ar)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    print(model)\n",
    "    return model,optimizer\n",
    "model,optimizer = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(model,train_loader,criterion,optimizer):\n",
    "    #print(1)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    running_loss=0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        #print(2)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        #print(3)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs,target)\n",
    "        running_loss += loss.item()\n",
    "        #print (outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    running_loss /= len(train_loader)\n",
    "    print(\"Train Loss: \", running_loss, ' Time: ', end_time-start_time)\n",
    "    return running_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model,test_loader,criterion):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        #print(2)\n",
    "        \n",
    "        for batch_idx, (data,target) in enumerate(test_loader):\n",
    "            #print(3)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            #print(predicted)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted==target).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs,target).detach()\n",
    "            running_loss += loss.item()\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Test Loss: ',running_loss)\n",
    "        print('Test Acc: ',acc,'%')\n",
    "        return running_loss,acc,predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_epoch(model,data):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        results = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        for batch_idx, data in enumerate(data):\n",
    "            #print(3)\n",
    "            data = data.to(device)\n",
    "            #target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            #print(outputs)\n",
    "            #predicted = torch.max(outputs.data)\n",
    "            #print (predicted)\n",
    "            #result += predicted\n",
    "            #total_predictions += target.size(0)\n",
    "            #correct_predictions += (predicted==target).sum().item()\n",
    "#            print(outputs.data)\n",
    "            predicted_source, predicted = torch.max(outputs.data,0)\n",
    "            #print (predicted)\n",
    "            xxx = predicted.item()\n",
    "            results.append(xxx)\n",
    "            #loss = criterion(outputs,target).detach()\n",
    "            #running_loss += loss.item()\n",
    "        #running_loss /= len(test_loader)\n",
    "        #acc = (correct_predictions/total_predictions)*100.0\n",
    "        #print('Test Loss: ',running_loss)\n",
    "        #print('Test Acc: ',acc,'%')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=84, out_features=106, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=106, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=10, out_features=118, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=118, out_features=126, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=126, out_features=49, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=49, out_features=13, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=13, out_features=83, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=83, out_features=118, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=118, out_features=90, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=90, out_features=100, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=100, out_features=35, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=35, out_features=95, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=95, out_features=120, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=120, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6893156201152478  Time:  0.9672098159790039\n",
      "Test Loss:  0.6921418929586605\n",
      "Test Acc:  51.681999999999995 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6857801221184812  Time:  0.9122481346130371\n",
      "Test Loss:  0.6862007124083382\n",
      "Test Acc:  53.742000000000004 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6831121914467569  Time:  0.908883810043335\n",
      "Test Loss:  0.6821999729287868\n",
      "Test Acc:  55.720000000000006 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6832362150741835  Time:  0.9116590023040771\n",
      "Test Loss:  0.6768389401995406\n",
      "Test Acc:  55.872 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6820823843196288  Time:  0.9116547107696533\n",
      "Test Loss:  0.6810864505110955\n",
      "Test Acc:  55.01200000000001 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=57, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=57, out_features=132, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=132, out_features=36, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=36, out_features=138, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=138, out_features=63, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=63, out_features=52, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=52, out_features=17, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=17, out_features=70, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=70, out_features=81, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=81, out_features=80, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=80, out_features=88, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=88, out_features=21, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=21, out_features=119, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=119, out_features=80, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=80, out_features=8, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.691853091878406  Time:  0.9588210582733154\n",
      "Test Loss:  0.6899767530207731\n",
      "Test Acc:  51.534 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6888457683183379  Time:  0.9543900489807129\n",
      "Test Loss:  0.6893690973520279\n",
      "Test Acc:  53.956 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6875753215814041  Time:  0.9908123016357422\n",
      "Test Loss:  0.6860057468316993\n",
      "Test Acc:  54.796 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6858427458900517  Time:  0.9631209373474121\n",
      "Test Loss:  0.6854919547937355\n",
      "Test Acc:  54.976 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6839568786701914  Time:  0.968621015548706\n",
      "Test Loss:  0.6833974630857\n",
      "Test Acc:  56.702 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=137, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=137, out_features=127, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=127, out_features=101, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=101, out_features=7, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=7, out_features=38, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=38, out_features=122, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=122, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.7827405570927313  Time:  0.6271300315856934\n",
      "Test Loss:  0.6931870205669987\n",
      "Test Acc:  49.946 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6931542754173279  Time:  0.5922098159790039\n",
      "Test Loss:  0.6931417228616014\n",
      "Test Acc:  49.946 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6934294887518478  Time:  0.6386070251464844\n",
      "Test Loss:  0.6933522747487438\n",
      "Test Acc:  50.056 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6932414648896557  Time:  0.593289852142334\n",
      "Test Loss:  0.6931846433756302\n",
      "Test Acc:  50.056 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6931873201313665  Time:  0.6203877925872803\n",
      "Test Loss:  0.6932277399666456\n",
      "Test Acc:  50.056 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=34, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=34, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4, out_features=129, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=129, out_features=135, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=135, out_features=125, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=125, out_features=6, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=6, out_features=74, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=74, out_features=121, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=121, out_features=43, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=43, out_features=65, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=65, out_features=66, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=66, out_features=108, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=108, out_features=100, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=100, out_features=75, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=75, out_features=95, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=95, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6903929104239254  Time:  1.0892722606658936\n",
      "Test Loss:  0.6914600793804441\n",
      "Test Acc:  49.522 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6890100548833104  Time:  1.0397379398345947\n",
      "Test Loss:  0.6860269895013498\n",
      "Test Acc:  54.676 %\n",
      "================================================== 1\n",
      "Train Loss:  0.689059947001732  Time:  1.137002944946289\n",
      "Test Loss:  0.6829602104060504\n",
      "Test Acc:  56.211999999999996 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6862713921878297  Time:  1.0908432006835938\n",
      "Test Loss:  0.6847031022212944\n",
      "Test Acc:  54.466 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6847939910524983  Time:  1.134216070175171\n",
      "Test Loss:  0.6791164534432548\n",
      "Test Acc:  56.05 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=105, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=105, out_features=98, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=98, out_features=46, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=46, out_features=52, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=52, out_features=27, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=27, out_features=90, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=90, out_features=65, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=65, out_features=53, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=53, out_features=11, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=11, out_features=24, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=24, out_features=43, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=43, out_features=42, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=42, out_features=93, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=93, out_features=105, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=105, out_features=89, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=89, out_features=123, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=123, out_features=139, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=139, out_features=57, bias=True)\n",
      "    (35): ReLU()\n",
      "    (36): Linear(in_features=57, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.6918630352464773  Time:  1.1451351642608643\n",
      "Test Loss:  0.6909786067446884\n",
      "Test Acc:  52.32 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6892312679250362  Time:  1.1079089641571045\n",
      "Test Loss:  0.6886503912356435\n",
      "Test Acc:  53.786 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6862884446726008  Time:  1.1062407493591309\n",
      "Test Loss:  0.684207304703946\n",
      "Test Acc:  55.776 %\n",
      "================================================== 2\n",
      "Train Loss:  0.683758794756259  Time:  1.1685528755187988\n",
      "Test Loss:  0.6797474349031642\n",
      "Test Acc:  55.862 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6803108388084477  Time:  1.2125036716461182\n",
      "Test Loss:  0.6764833933236648\n",
      "Test Acc:  57.352000000000004 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=34, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=34, out_features=126, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=126, out_features=22, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=22, out_features=125, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=125, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=19, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=19, out_features=40, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=40, out_features=126, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=126, out_features=17, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=17, out_features=20, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=20, out_features=24, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=24, out_features=131, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=131, out_features=83, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=83, out_features=29, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=29, out_features=55, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=55, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6898413219694364  Time:  0.9394848346710205\n",
      "Test Loss:  0.6864468549587288\n",
      "Test Acc:  54.04 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6837241599115275  Time:  0.8683271408081055\n",
      "Test Loss:  0.680015331324266\n",
      "Test Acc:  55.855999999999995 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6826663401167271  Time:  0.9037179946899414\n",
      "Test Loss:  0.6746101324655571\n",
      "Test Acc:  56.994 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6800373277421725  Time:  0.8748488426208496\n",
      "Test Loss:  0.6758938951759922\n",
      "Test Acc:  56.83200000000001 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6797620765233444  Time:  0.8975811004638672\n",
      "Test Loss:  0.6731990052729236\n",
      "Test Acc:  57.412 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=60, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=60, out_features=23, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=23, out_features=19, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=19, out_features=127, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=127, out_features=111, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=111, out_features=13, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=13, out_features=76, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=76, out_features=20, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=20, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=35, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=35, out_features=80, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=80, out_features=127, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=127, out_features=5, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=5, out_features=53, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=53, out_features=19, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=19, out_features=94, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=94, out_features=118, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): Linear(in_features=118, out_features=45, bias=True)\n",
      "    (35): ReLU()\n",
      "    (36): Linear(in_features=45, out_features=12, bias=True)\n",
      "    (37): ReLU()\n",
      "    (38): Linear(in_features=12, out_features=53, bias=True)\n",
      "    (39): ReLU()\n",
      "    (40): Linear(in_features=53, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6924207412590415  Time:  1.1059253215789795\n",
      "Test Loss:  0.6907583131473891\n",
      "Test Acc:  50.946000000000005 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6885123732736556  Time:  1.1986520290374756\n",
      "Test Loss:  0.6875531287217627\n",
      "Test Acc:  54.098 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6853887888334566  Time:  1.1384410858154297\n",
      "Test Loss:  0.6910143525016551\n",
      "Test Acc:  52.262 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6836982990725565  Time:  1.0799140930175781\n",
      "Test Loss:  0.6829482614994049\n",
      "Test Acc:  55.620000000000005 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6837472415576547  Time:  1.2202808856964111\n",
      "Test Loss:  0.6835645887316489\n",
      "Test Acc:  55.662 %\n",
      "================================================== 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epoch = 5\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "#model2,optimizer2 = create_model()\n",
    "acc1 =0\n",
    "acc2 =0\n",
    "for runs in range(7):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model ,optimizer = create_model()\n",
    "    best_model = model\n",
    "    best_optimizer = optimizer\n",
    "    best_score = 0\n",
    "    for i in range(n_epoch):\n",
    "        train_loader = create_set(30000,df_train)\n",
    "        train_loss = train_epoch(best_model,train_loader,criterion,best_optimizer)\n",
    "        test_loss,test_acc,pre = test_epoch(best_model,validate_loader,criterion)\n",
    "        \n",
    "        #train_loss = train_epoch(model,train_loader,criterion,optimizer2)\n",
    "        #test_loss2,test_acc2,pre2 = test_epoch(model2,test_loader,criterion)\n",
    "        #if (test_acc2 > test_acc):\n",
    "        #    best_model = model2\n",
    "        #    best_optimizer = optimizer2\n",
    "        #else:\n",
    "        #    best_model = model\n",
    "        #    best_optimizer = optimizer\n",
    "        Train_loss.append(train_loss)\n",
    "        Test_loss.append(test_loss)\n",
    "        Test_acc.append(test_acc)\n",
    "        print('='*50,i)\n",
    "        #print(pre)\n",
    "    #model2 ,optimizer2 = create_model()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_res = submit_epoch(best_model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(predicted_res)\n",
    "df.to_csv('/Users/bebik/Downloads/data/70_1.csv',index=False,header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss/Criterion')\n",
    "plt.plot(Train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss/Criterion')\n",
    "plt.plot(Test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

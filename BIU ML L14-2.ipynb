{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "pic_size = 256\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./data',train=True,download=True,transform =transforms.ToTensor())\n",
    "test = MNIST('./data',train=False,download=True,transform =transforms.ToTensor())\n",
    "train_data = train.data\n",
    "train_data = train.transform(train_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "train.data.size()\n",
    "print(len(test.data.numpy() ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f940e8e1ad0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train.data.cpu().numpy()[923])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index].float()\n",
    "        Y = self.Y[index].long()\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebik/anaconda3/lib/python3.7/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import seaborn as sns\n",
    "#sns.set_style('whitegrid')\n",
    "from datetime import datetime\n",
    "sys.platform\n",
    "df_train = pandas.read_csv('/Users/bebik/Downloads/data/train.csv',header=None)#,nrows=700000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    #device = get_device()\n",
    "    return torch.from_numpy(df.values).float() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def normalize1(X_train):\n",
    "    import math\n",
    "    same  = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    #X_train = df_test\n",
    "    df1 =pd.DataFrame()\n",
    "    df2 =pd.DataFrame()\n",
    "    df3 =pd.DataFrame()\n",
    "    df4 =pd.DataFrame()\n",
    "\n",
    "    #df_pca = pd.DataFrame()\n",
    "    #dft['result']  = X_train[X_train.columns[0]] \n",
    "    #X_train[1] = X_train[3]-X_train[1]\n",
    "    \n",
    "    for i in range(0,29):\n",
    "        df_temp = pd.DataFrame()\n",
    "        x = i*4\n",
    "        #print(i)\n",
    "        #df_temp[0] = X_train[X_train.columns[x+1]]-X_train[X_train.columns[x+5]]\n",
    "        #df_temp[1] = X_train[X_train.columns[x+2]]-X_train[X_train.columns[x+6]]\n",
    "        #df_temp[2] = X_train[X_train.columns[x+3]]-X_train[X_train.columns[x+7]]\n",
    "        #df_temp[3] = X_train[X_train.columns[x+4]]-X_train[X_train.columns[x+8]]\n",
    "       \n",
    "        df1[i] = X_train[X_train.columns[x+5]]-X_train[X_train.columns[x+1]]\n",
    "        df2[i] = X_train[X_train.columns[x+6]]-X_train[X_train.columns[x+2]]\n",
    "        df3[i] = X_train[X_train.columns[x+7]]-X_train[X_train.columns[x+3]]\n",
    "        df4[i] = X_train[X_train.columns[x+8]]-X_train[X_train.columns[x+4]]\n",
    "        \n",
    "#        X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
    "        #pca = PCA(n_components=1)\n",
    "        #pca.fit(df_temp)\n",
    "        #df_pca = pca.transform(df_temp)\n",
    "        #print(\"PCA VARIANCE \",pca.explained_variance_ratio_)\n",
    "\n",
    "        #df33 = pd.DataFrame(data=df_pca)\n",
    "        #dft[i] = df33[0]\n",
    "    \n",
    "    return X_train,df1,df2,df3,df4\n",
    "\n",
    "#df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 res            2z            6z           10z           14z  \\\n",
      "count  700000.000000  7.000000e+05  7.000000e+05  7.000000e+05  7.000000e+05   \n",
      "mean        0.499653  2.070697e-17  3.187467e-17  1.413099e-17 -5.000601e-17   \n",
      "std         0.500000  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min         0.000000 -1.358319e+02 -1.573637e+02 -2.320619e+02 -1.767994e+02   \n",
      "25%         0.000000 -2.920642e-02 -2.555831e-02 -2.546232e-02 -2.886290e-02   \n",
      "50%         0.000000 -6.328446e-04 -1.651408e-03  7.112844e-04 -4.319368e-04   \n",
      "75%         1.000000  2.580837e-02  2.072300e-02  2.482441e-02  2.560484e-02   \n",
      "max         1.000000  2.711047e+02  2.453320e+02  1.162502e+02  2.663445e+02   \n",
      "\n",
      "                18z           22z           26z           30z           34z  \\\n",
      "count  7.000000e+05  7.000000e+05  7.000000e+05  7.000000e+05  7.000000e+05   \n",
      "mean  -1.582115e-17 -1.962909e-16  4.112387e-18 -2.734945e-17  9.587943e-18   \n",
      "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min   -1.744103e+02 -1.843808e+02 -1.810166e+02 -1.760720e+02 -2.587068e+02   \n",
      "25%   -2.826745e-02 -2.797623e-02 -2.820130e-02 -2.877039e-02 -2.934903e-02   \n",
      "50%   -1.600922e-03  9.529639e-05  9.693269e-04 -5.160368e-04  1.593335e-05   \n",
      "75%    2.319725e-02  2.661062e-02  2.787249e-02  2.583085e-02  2.720802e-02   \n",
      "max    2.519354e+02  1.335956e+02  1.352588e+02  2.862766e+02  1.385693e+02   \n",
      "\n",
      "       ...           75z           79z           83z           87z  \\\n",
      "count  ...  7.000000e+05  7.000000e+05  7.000000e+05  7.000000e+05   \n",
      "mean   ... -2.745664e-16 -1.617258e-15 -4.396683e-16 -1.946579e-16   \n",
      "std    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min    ... -9.181039e+01 -9.200795e+01 -9.037239e+01 -9.048943e+01   \n",
      "25%    ... -4.215324e-01 -4.198165e-01 -4.271839e-01 -4.312174e-01   \n",
      "50%    ...  5.599380e-04  5.049546e-03  5.091571e-03  1.989290e-03   \n",
      "75%    ...  4.053598e-01  4.095098e-01  4.117346e-01  4.158310e-01   \n",
      "max    ...  7.749871e+01  3.472408e+01  3.505003e+01  3.506800e+01   \n",
      "\n",
      "                91z           95z           99z          103z          107z  \\\n",
      "count  7.000000e+05  7.000000e+05  7.000000e+05  7.000000e+05  7.000000e+05   \n",
      "mean   5.798693e-16  7.579550e-16 -2.866334e-16  4.658998e-17 -3.232004e-16   \n",
      "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min   -2.257498e+01 -2.294937e+01 -2.291594e+01 -9.260805e+01 -8.990063e+01   \n",
      "25%   -4.350722e-01 -4.408295e-01 -4.421593e-01 -4.398260e-01 -4.390907e-01   \n",
      "50%    1.520776e-03  6.752822e-05  9.754453e-04  4.118350e-03  2.315518e-03   \n",
      "75%    4.189152e-01  4.240318e-01  4.252356e-01  4.201265e-01  4.218146e-01   \n",
      "max    3.516809e+01  3.497530e+01  3.362359e+01  3.353612e+01  2.206224e+01   \n",
      "\n",
      "               111z  \n",
      "count  7.000000e+05  \n",
      "mean   5.682363e-16  \n",
      "std    1.000000e+00  \n",
      "min   -9.217678e+01  \n",
      "25%   -4.356026e-01  \n",
      "50%    3.369901e-03  \n",
      "75%    4.198854e-01  \n",
      "max    2.979751e+01  \n",
      "\n",
      "[8 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "def normalize2(df,series,newdf):\n",
    "    for i in range(28):\n",
    "        newcol = str(i*4+series)+'z'\n",
    "        newdf[newcol] =(df[df.columns[i] ]-df[df.columns[i]].mean() ) /df[df.columns[i]].std()\n",
    "    return newdf\n",
    "\n",
    "def normalize3(df_train):\n",
    "    df_train ,df1,df2,df3,df4= normalize1(df_train)\n",
    "    newdf =pd.DataFrame()\n",
    "    newdf['res']= df_train[df_train.columns[0] ]\n",
    "    #newdf = normalize2(df1,0,newdf)\n",
    "    #newdf = normalize2(df2,1,newdf)\n",
    "    newdf = normalize2(df3,2,newdf)\n",
    "    newdf = normalize2(df4,3,newdf)\n",
    "\n",
    "    return newdf\n",
    "df_train = normalize3(df_train)\n",
    "print (df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                res            2z            6z           10z           14z  \\\n",
      "count  50000.000000  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04   \n",
      "mean       0.500560  2.734757e-17 -1.766087e-17 -6.862705e-18 -9.216308e-18   \n",
      "std        0.500005  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min        0.000000 -2.884449e+01 -2.724074e+01 -3.472153e+01 -3.693919e+01   \n",
      "25%        0.000000 -4.680297e-02 -4.646409e-02 -4.364539e-02 -4.830021e-02   \n",
      "50%        1.000000 -1.374756e-03 -3.641369e-04  2.215469e-03 -2.990512e-03   \n",
      "75%        1.000000  4.057570e-02  4.504776e-02  4.448153e-02  4.036337e-02   \n",
      "max        1.000000  4.837794e+01  3.577733e+01  4.849435e+01  3.632376e+01   \n",
      "\n",
      "                18z           22z           26z           30z           34z  \\\n",
      "count  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04   \n",
      "mean  -6.668138e-18  1.766517e-17  2.885470e-18  2.325924e-17  1.796306e-17   \n",
      "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min   -2.882844e+01 -5.061131e+01 -2.599325e+01 -4.966475e+01 -4.277490e+01   \n",
      "25%   -4.178625e-02 -4.709965e-02 -5.049603e-02 -4.574903e-02 -4.650316e-02   \n",
      "50%    3.435155e-03 -2.161367e-03 -5.530991e-03 -2.711857e-03 -3.746164e-03   \n",
      "75%    4.534570e-02  3.953618e-02  3.735537e-02  3.979530e-02  3.559831e-02   \n",
      "max    5.036769e+01  4.808548e+01  5.116972e+01  4.433947e+01  4.694515e+01   \n",
      "\n",
      "       ...           75z           79z           83z           87z  \\\n",
      "count  ...  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04   \n",
      "mean   ... -1.296391e-16  1.364862e-16 -1.697581e-16  3.435918e-17   \n",
      "std    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min    ... -3.877180e+01 -1.435014e+01 -8.909755e+00 -1.385812e+01   \n",
      "25%    ... -5.100671e-01 -5.148850e-01 -5.213711e-01 -5.096175e-01   \n",
      "50%    ...  1.192300e-02  3.859648e-03  6.012700e-03  1.890198e-02   \n",
      "75%    ...  4.857060e-01  4.830010e-01  4.860469e-01  4.795739e-01   \n",
      "max    ...  8.251053e+00  1.048944e+01  1.620750e+01  1.173905e+01   \n",
      "\n",
      "                91z           95z           99z          103z          107z  \\\n",
      "count  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04   \n",
      "mean  -1.027955e-16  3.801504e-16  6.970202e-17 -4.428014e-17 -2.312828e-16   \n",
      "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "min   -1.347898e+01 -1.020385e+01 -3.823162e+01 -1.413952e+01 -1.647291e+01   \n",
      "25%   -5.183635e-01 -5.178608e-01 -5.158669e-01 -5.184476e-01 -5.193919e-01   \n",
      "50%    1.777966e-02  1.062651e-02  8.869346e-03  8.103767e-03  4.842687e-03   \n",
      "75%    4.910105e-01  5.001921e-01  4.868937e-01  4.905007e-01  4.906558e-01   \n",
      "max    1.614951e+01  1.613513e+01  1.157797e+01  1.169501e+01  1.178184e+01   \n",
      "\n",
      "               111z  \n",
      "count  5.000000e+04  \n",
      "mean  -1.153166e-16  \n",
      "std    1.000000e+00  \n",
      "min   -1.519496e+01  \n",
      "25%   -5.211549e-01  \n",
      "50%    4.326264e-03  \n",
      "75%    5.016852e-01  \n",
      "max    1.182288e+01  \n",
      "\n",
      "[8 rows x 57 columns]\n",
      "       res\n",
      "0        1\n",
      "1        0\n",
      "2        1\n",
      "3        0\n",
      "4        0\n",
      "...    ...\n",
      "49995    0\n",
      "49996    1\n",
      "49997    0\n",
      "49998    1\n",
      "49999    1\n",
      "\n",
      "[50000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader(df):\n",
    "    target = pd.DataFrame(df[df.columns[0]] )\n",
    "    print(target)\n",
    "    ten_target = torch.from_numpy(target[target.columns[0]].values)\n",
    "    del df[df.columns[0] ]\n",
    "    ten_data = df_to_tensor(df)\n",
    "\n",
    "\n",
    "    _dataset = MyDataset(ten_data,ten_target)\n",
    "\n",
    "\n",
    "\n",
    "    test_loader_args = dict(shuffle=True,batch_size=pic_size,num_workers=0,pin_memory=True) if cuda\\\n",
    "    else dict(shuffle=True,batch_size=pic_size)\n",
    "    loader = data.DataLoader(_dataset,**test_loader_args)\n",
    "    return ten_target,loader\n",
    "\n",
    "\n",
    "\n",
    "df_validate = pandas.read_csv('/Users/bebik/Downloads/data/validate.csv',header=None)\n",
    "df_validate = normalize3(df_validate)\n",
    "print (df_validate.describe())\n",
    "validate_target, validate_loader = create_data_loader(df_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pandas.read_csv('/Users/bebik/Downloads/data/test.csv',header=None)\n",
    "ttt = normalize3(df_test)\n",
    "del ttt[ttt.columns[0] ]\n",
    "test_data = df_to_tensor(ttt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set(num,df):\n",
    "\n",
    "    if (num == 0):\n",
    "        df_train_sample = df\n",
    "    else :\n",
    "        df_train_sample = df.sample( n = num)\n",
    "    target_sample = pd.DataFrame(df_train_sample[df_train_sample.columns[0]] )\n",
    "    del df_train_sample[df_train_sample.columns[0]]\n",
    "    ten_train_target = torch.from_numpy(target_sample[target_sample.columns[0]].values)\n",
    "    #df_to_tensor(target)\n",
    "    ten_train_data = df_to_tensor(df_train_sample)\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = MyDataset(ten_train_data,ten_train_target)\n",
    "\n",
    "\n",
    "    train_loader_args = dict(shuffle=True,batch_size=pic_size,num_workers=0,pin_memory=True) if cuda\\\n",
    "    else dict(shuffle=True,batch_size=pic_size)\n",
    "    train_loader = data.DataLoader(train_dataset,**train_loader_args)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple_MLP([784,100,50,10])\n",
    "\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self,size_list):\n",
    "        super(Simple_MLP,self).__init__()\n",
    "        layers=[]\n",
    "        self.size_list = size_list\n",
    "        for i in range(len(size_list) -2):\n",
    "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "            op = random.randint(0,2) \n",
    "            if (op==0):\n",
    "                layers.append(nn.ReLU())\n",
    "            if (op ==1): \n",
    "                layers.append(nn.LeakyReLU())\n",
    "            if (op ==2):\n",
    "                layers.append(nn.ReLU())\n",
    "        #layers.append(nn.Linear(size_list[-3],size_list[-2]))\n",
    "        #layers.append(nn.Softmax(dim=1))\n",
    "        layers.append(nn.Linear(size_list[-2],size_list[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=103, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=103, out_features=59, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=59, out_features=4, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=4, out_features=59, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=59, out_features=31, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=31, out_features=111, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=111, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([56, 103, 49, 139, 48, 105, 65, 2],\n",
       " [56, 104, 59, 4, 59, 31, 111, 2],\n",
       " [56, 103, 59, 4, 59, 31, 111, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the network for training\n",
    "#model = Simple_MLP([784,16,50,8,200,10])\n",
    "\n",
    "\n",
    "#model = Simple_MLP([120,16,200,10])\n",
    "#model = Simple_MLP([120,80,40,20,12,2])\n",
    "\n",
    "#model = Simple_MLP([120,80,60,40,20,2])\n",
    "#model = Simple_MLP([120,110,106,104,100,90,88,82,80,70,60,50,40,60,80,30,20,10,8,6,4,4,2])\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "first_round = 56\n",
    "\n",
    "device = torch.device( \"cpu\")\n",
    "def create_ar(lens):\n",
    "    \n",
    "    r1 = 160\n",
    "    r2 = 160\n",
    "    ar = [first_round]\n",
    "    for i in range(random.randint(1,lens)):\n",
    "        r1 = random.randint(4,140)\n",
    "        ar.append( r1  )\n",
    "        r2 = random.randint(4,140)\n",
    "        ar.append(  r2 )\n",
    "    ar.append(2)\n",
    "    return ar\n",
    "def next_gen(best_ar):\n",
    "    \n",
    "    ar_left = []\n",
    "    ar_right = []\n",
    "    for index,item in enumerate(best_ar):\n",
    "        r1 = random.randint(4,140)\n",
    "        if (index is 0 or index is len(best_ar)-1):\n",
    "            r1=item\n",
    "        if (index < len(best_ar)/4 ):\n",
    "            ar_left.append(item)\n",
    "            ar_right.append(r1)\n",
    "        else:\n",
    "            ar_left.append(r1)\n",
    "            ar_right.append(item)\n",
    "    \n",
    "    return ar_left,ar_right,best_ar\n",
    "\n",
    "def create_model(ar):   \n",
    "    model = Simple_MLP(ar)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    print(model)\n",
    "    return model,optimizer ,ar\n",
    "ar2=create_ar(10)\n",
    "model,optimizer ,ar2= create_model(ar2)\n",
    "next_gen(ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(model,train_loader,criterion,optimizer):\n",
    "    #print(1)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    running_loss=0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        #print(2)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        #print(3)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs,target)\n",
    "        running_loss += loss.item()\n",
    "        #print (outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()   \n",
    "    running_loss /= len(train_loader)\n",
    "    print(\"Train Loss: \", running_loss, ' Time: ', end_time-start_time)\n",
    "    return running_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model,test_loader,criterion):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        #print(2)\n",
    "        \n",
    "        for batch_idx, (data,target) in enumerate(test_loader):\n",
    "            #print(3)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            #print(predicted)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted==target).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs,target).detach()\n",
    "            running_loss += loss.item()\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Test Loss: ',running_loss)\n",
    "        print('Test Acc: ',acc,'%')\n",
    "        return running_loss,acc,predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_epoch(model,data):\n",
    "    with torch.no_grad():\n",
    "        #print(1)\n",
    "        results = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        for batch_idx, data in enumerate(data):\n",
    "            #print(3)\n",
    "            data = data.to(device)\n",
    "            #target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            #print(outputs)\n",
    "            #predicted = torch.max(outputs.data)\n",
    "            #print (predicted)\n",
    "            #result += predicted\n",
    "            #total_predictions += target.size(0)\n",
    "            #correct_predictions += (predicted==target).sum().item()\n",
    "#            print(outputs.data)\n",
    "            predicted_source, predicted = torch.max(outputs.data,0)\n",
    "            #print (predicted)\n",
    "            xxx = predicted.item()\n",
    "            results.append(xxx)\n",
    "            #loss = criterion(outputs,target).detach()\n",
    "            #running_loss += loss.item()\n",
    "        #running_loss /= len(test_loader)\n",
    "        #acc = (correct_predictions/total_predictions)*100.0\n",
    "        #print('Test Loss: ',running_loss)\n",
    "        #print('Test Acc: ',acc,'%')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=55, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=55, out_features=118, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=118, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6905690059609657  Time:  1.005028247833252\n",
      "Test Loss:  0.6750730957303729\n",
      "Test Acc:  58.866 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6866521646071525  Time:  1.063413143157959\n",
      "Test Loss:  0.6737069104398999\n",
      "Test Acc:  59.194 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6855462056441899  Time:  1.0517208576202393\n",
      "Test Loss:  0.6721295522791999\n",
      "Test Acc:  59.504000000000005 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6842814348909977  Time:  1.0340511798858643\n",
      "Test Loss:  0.6725345804375045\n",
      "Test Acc:  58.358 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6826565872578725  Time:  1.1063261032104492\n",
      "Test Loss:  0.6706606335177714\n",
      "Test Acc:  58.642 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6807600899334372  Time:  1.1131041049957275\n",
      "Test Loss:  0.6709311741347216\n",
      "Test Acc:  59.144 %\n",
      "================================================== 0\n",
      "Train Loss:  0.681014198021297  Time:  1.0578110218048096\n",
      "Test Loss:  0.670083534048528\n",
      "Test Acc:  59.355999999999995 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6787115248450397  Time:  0.9973611831665039\n",
      "Test Loss:  0.669493801739751\n",
      "Test Acc:  58.548 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6800777122487117  Time:  0.9979712963104248\n",
      "Test Loss:  0.6713108146677211\n",
      "Test Acc:  58.044 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6787341854868144  Time:  1.005295991897583\n",
      "Test Loss:  0.6699666289650664\n",
      "Test Acc:  58.604 %\n",
      "================================================== 0\n",
      "updating model =======  58.604\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=33, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=33, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6905295670467572  Time:  1.0307731628417969\n",
      "Test Loss:  0.6772146690256742\n",
      "Test Acc:  58.68 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6854067851592155  Time:  1.0528371334075928\n",
      "Test Loss:  0.6713922717133347\n",
      "Test Acc:  60.15200000000001 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6855698863085169  Time:  1.0420401096343994\n",
      "Test Loss:  0.6731114944024962\n",
      "Test Acc:  59.321999999999996 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6832154723849609  Time:  1.0356831550598145\n",
      "Test Loss:  0.672378415355877\n",
      "Test Acc:  58.872 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6813559227616247  Time:  0.9945559501647949\n",
      "Test Loss:  0.6716555308322517\n",
      "Test Acc:  58.08 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6805842244712106  Time:  1.009690284729004\n",
      "Test Loss:  0.6727012328955592\n",
      "Test Acc:  58.29 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6793080498702335  Time:  0.9943790435791016\n",
      "Test Loss:  0.6707039077062996\n",
      "Test Acc:  58.324 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6792122997941762  Time:  0.9904148578643799\n",
      "Test Loss:  0.670663657845283\n",
      "Test Acc:  58.422 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6803858856650165  Time:  1.0132081508636475\n",
      "Test Loss:  0.6698199489286968\n",
      "Test Acc:  58.52 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6782458951003361  Time:  1.035447120666504\n",
      "Test Loss:  0.6680166344253384\n",
      "Test Acc:  58.912 %\n",
      "================================================== 0\n",
      "updating model =======  58.912\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=33, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=33, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6912986999445588  Time:  1.0485591888427734\n",
      "Test Loss:  0.6791779733434016\n",
      "Test Acc:  58.068 %\n",
      "================================================== 0\n",
      "Train Loss:  0.687686594080751  Time:  1.0316662788391113\n",
      "Test Loss:  0.6739663408727062\n",
      "Test Acc:  59.744 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6851852447012045  Time:  1.0280449390411377\n",
      "Test Loss:  0.6725576909220948\n",
      "Test Acc:  58.372 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6834434286086228  Time:  0.9920310974121094\n",
      "Test Loss:  0.6707012178946514\n",
      "Test Acc:  58.652 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6813209300493672  Time:  0.9993679523468018\n",
      "Test Loss:  0.6727633561406817\n",
      "Test Acc:  58.114 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6813957060340547  Time:  0.9893200397491455\n",
      "Test Loss:  0.66962297017477\n",
      "Test Acc:  58.940000000000005 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6780689913426002  Time:  0.9913501739501953\n",
      "Test Loss:  0.6711666206924283\n",
      "Test Acc:  58.635999999999996 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6786848615555867  Time:  1.0157692432403564\n",
      "Test Loss:  0.671069153717586\n",
      "Test Acc:  57.626 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6788409976628575  Time:  1.025527000427246\n",
      "Test Loss:  0.6707725293782293\n",
      "Test Acc:  57.862 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6783280679344261  Time:  1.0310001373291016\n",
      "Test Loss:  0.6688261667684633\n",
      "Test Acc:  58.467999999999996 %\n",
      "================================================== 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=33, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=33, out_features=87, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=87, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.691175555878312  Time:  1.026724100112915\n",
      "Test Loss:  0.6793919062736083\n",
      "Test Acc:  57.382 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6874557733535767  Time:  1.0323257446289062\n",
      "Test Loss:  0.6735361893566287\n",
      "Test Acc:  59.44199999999999 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6850640358280962  Time:  0.991736888885498\n",
      "Test Loss:  0.6721059521850274\n",
      "Test Acc:  59.428000000000004 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6837138200763369  Time:  0.9961230754852295\n",
      "Test Loss:  0.6701729127338955\n",
      "Test Acc:  58.967999999999996 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6823588938173586  Time:  0.9929380416870117\n",
      "Test Loss:  0.6713700866212651\n",
      "Test Acc:  58.596000000000004 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6813846611193497  Time:  0.9835879802703857\n",
      "Test Loss:  0.6711405771119254\n",
      "Test Acc:  57.86599999999999 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6798243990344722  Time:  1.0236432552337646\n",
      "Test Loss:  0.6719866194287125\n",
      "Test Acc:  58.508 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6797608744053945  Time:  1.0286309719085693\n",
      "Test Loss:  0.6722377319725192\n",
      "Test Acc:  58.142 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6778652378677452  Time:  1.03277587890625\n",
      "Test Loss:  0.6695027999123748\n",
      "Test Acc:  58.07 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6794640169526539  Time:  1.026737928390503\n",
      "Test Loss:  0.6708976653765659\n",
      "Test Acc:  57.82000000000001 %\n",
      "================================================== 0\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=74, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=74, out_features=34, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=34, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6904079287591642  Time:  0.9755799770355225\n",
      "Test Loss:  0.6766737574825481\n",
      "Test Acc:  59.653999999999996 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6868738510312825  Time:  0.9686579704284668\n",
      "Test Loss:  0.6720926716011397\n",
      "Test Acc:  59.806000000000004 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6835206340264229  Time:  0.9738609790802002\n",
      "Test Loss:  0.6693416861247043\n",
      "Test Acc:  60.07599999999999 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6834636402391169  Time:  0.9727318286895752\n",
      "Test Loss:  0.668027789312966\n",
      "Test Acc:  60.394000000000005 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6829548103965982  Time:  0.9435861110687256\n",
      "Test Loss:  0.6699649889250191\n",
      "Test Acc:  59.292 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6801132786012914  Time:  1.0082778930664062\n",
      "Test Loss:  0.6691109078879259\n",
      "Test Acc:  58.9 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6797439060071959  Time:  1.0188260078430176\n",
      "Test Loss:  0.6690480961483352\n",
      "Test Acc:  58.599999999999994 %\n",
      "================================================== 1\n",
      "Train Loss:  0.679110126338736  Time:  1.0200510025024414\n",
      "Test Loss:  0.6702532680058966\n",
      "Test Acc:  58.142 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6790088839339514  Time:  1.0141880512237549\n",
      "Test Loss:  0.6685239176969139\n",
      "Test Acc:  58.821999999999996 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6787626854259602  Time:  0.9786489009857178\n",
      "Test Loss:  0.6694872382344031\n",
      "Test Acc:  58.053999999999995 %\n",
      "================================================== 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=33, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=33, out_features=87, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=87, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6911201785950765  Time:  0.9910588264465332\n",
      "Test Loss:  0.6805773438239584\n",
      "Test Acc:  56.608000000000004 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6868970572513385  Time:  0.9787161350250244\n",
      "Test Loss:  0.6734820540462222\n",
      "Test Acc:  59.162000000000006 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6850040239574265  Time:  0.9848828315734863\n",
      "Test Loss:  0.675061437244318\n",
      "Test Acc:  57.948 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6820387383447076  Time:  1.0271530151367188\n",
      "Test Loss:  0.6724003976096913\n",
      "Test Acc:  58.486000000000004 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6808290910111726  Time:  1.026965856552124\n",
      "Test Loss:  0.6719344218774718\n",
      "Test Acc:  58.102 %\n",
      "================================================== 1\n",
      "Train Loss:  0.680303698256068  Time:  1.0400187969207764\n",
      "Test Loss:  0.6729397679470024\n",
      "Test Acc:  58.384 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6799578912501788  Time:  1.029163122177124\n",
      "Test Loss:  0.6716547915521933\n",
      "Test Acc:  58.286 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6803947530523704  Time:  1.0261790752410889\n",
      "Test Loss:  0.6695158405571567\n",
      "Test Acc:  58.467999999999996 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6784320088633655  Time:  0.9739389419555664\n",
      "Test Loss:  0.6719732883633399\n",
      "Test Acc:  57.472 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6777126101681786  Time:  0.9853990077972412\n",
      "Test Loss:  0.6712960877588817\n",
      "Test Acc:  58.211999999999996 %\n",
      "================================================== 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=33, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=33, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6913987848445446  Time:  0.9875612258911133\n",
      "Test Loss:  0.6806333688448887\n",
      "Test Acc:  57.972 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6878032351497316  Time:  0.9823141098022461\n",
      "Test Loss:  0.677680006440805\n",
      "Test Acc:  56.884 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6834968761371  Time:  1.022068977355957\n",
      "Test Loss:  0.6733773630492541\n",
      "Test Acc:  57.977999999999994 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6822826786197885  Time:  1.0257458686828613\n",
      "Test Loss:  0.6704422232447839\n",
      "Test Acc:  58.708000000000006 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6816340756677363  Time:  1.020561933517456\n",
      "Test Loss:  0.6724002145382822\n",
      "Test Acc:  58.288 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6801358098096221  Time:  1.0245580673217773\n",
      "Test Loss:  0.671733351082218\n",
      "Test Acc:  58.374 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6793050913915147  Time:  0.9863219261169434\n",
      "Test Loss:  0.6716831834340582\n",
      "Test Acc:  57.831999999999994 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6787416991526193  Time:  0.9821991920471191\n",
      "Test Loss:  0.6694818665178455\n",
      "Test Acc:  57.79600000000001 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6788527413006247  Time:  0.991588830947876\n",
      "Test Loss:  0.6688178285044066\n",
      "Test Acc:  58.550000000000004 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6765902768956483  Time:  0.9880311489105225\n",
      "Test Loss:  0.6706476898825898\n",
      "Test Acc:  58.438 %\n",
      "================================================== 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=122, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=122, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=29, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=29, out_features=91, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6887547306335755  Time:  1.143388032913208\n",
      "Test Loss:  0.6679380828020524\n",
      "Test Acc:  58.324 %\n",
      "================================================== 1\n",
      "Train Loss:  0.680847156004314  Time:  1.1952388286590576\n",
      "Test Loss:  0.6678729130297291\n",
      "Test Acc:  58.418000000000006 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6778387198918057  Time:  1.2144389152526855\n",
      "Test Loss:  0.6691358801053495\n",
      "Test Acc:  59.708000000000006 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6786999893884589  Time:  1.2168939113616943\n",
      "Test Loss:  0.6662976571491787\n",
      "Test Acc:  58.728 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6791596108109411  Time:  1.2106921672821045\n",
      "Test Loss:  0.6692216125678043\n",
      "Test Acc:  57.50999999999999 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6777398003278857  Time:  1.162818193435669\n",
      "Test Loss:  0.6654566879175148\n",
      "Test Acc:  59.43000000000001 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6775288544867161  Time:  1.1629390716552734\n",
      "Test Loss:  0.666338075180443\n",
      "Test Acc:  58.346 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6755372440292887  Time:  1.1512479782104492\n",
      "Test Loss:  0.6691852023406905\n",
      "Test Acc:  57.977999999999994 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6767447495112454  Time:  1.1665310859680176\n",
      "Test Loss:  0.6674149456072827\n",
      "Test Acc:  57.374 %\n",
      "================================================== 1\n",
      "Train Loss:  0.6748294075475122  Time:  1.191521167755127\n",
      "Test Loss:  0.6640530560089617\n",
      "Test Acc:  58.611999999999995 %\n",
      "================================================== 1\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=122, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=122, out_features=118, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=118, out_features=98, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=98, out_features=61, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=61, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6893766113441356  Time:  1.2243919372558594\n",
      "Test Loss:  0.6732159649230995\n",
      "Test Acc:  59.178 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6809874370585393  Time:  1.2306969165802002\n",
      "Test Loss:  0.6698713673620807\n",
      "Test Acc:  58.436 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6783546094041671  Time:  1.2336630821228027\n",
      "Test Loss:  0.668710149976672\n",
      "Test Acc:  57.647999999999996 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6782027937635018  Time:  1.2339308261871338\n",
      "Test Loss:  0.6685890968965025\n",
      "Test Acc:  56.726 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6760912283058584  Time:  1.19474196434021\n",
      "Test Loss:  0.6693821598072441\n",
      "Test Acc:  57.809999999999995 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6758769724490868  Time:  1.2101211547851562\n",
      "Test Loss:  0.6659151072404823\n",
      "Test Acc:  58.496 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6769048093009169  Time:  1.2293188571929932\n",
      "Test Loss:  0.6705691081528761\n",
      "Test Acc:  57.158 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6758883021173686  Time:  1.2224090099334717\n",
      "Test Loss:  0.6694175786509806\n",
      "Test Acc:  57.51199999999999 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6768043574190488  Time:  1.2574429512023926\n",
      "Test Loss:  0.670398165376819\n",
      "Test Acc:  58.331999999999994 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6761973843957386  Time:  1.2496771812438965\n",
      "Test Loss:  0.6684768011375349\n",
      "Test Acc:  57.494 %\n",
      "================================================== 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=35, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=35, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=29, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=29, out_features=91, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6897095293894301  Time:  1.1624932289123535\n",
      "Test Loss:  0.673013976033853\n",
      "Test Acc:  58.812 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6809297332363408  Time:  1.1741888523101807\n",
      "Test Loss:  0.6684937693026601\n",
      "Test Acc:  60.258 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6794264468833477  Time:  1.1147620677947998\n",
      "Test Loss:  0.6698739197181196\n",
      "Test Acc:  58.056 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6790356731762851  Time:  1.114151954650879\n",
      "Test Loss:  0.6682385032882496\n",
      "Test Acc:  59.092 %\n",
      "================================================== 2\n",
      "Train Loss:  0.677600976977035  Time:  1.121161937713623\n",
      "Test Loss:  0.6686630699099326\n",
      "Test Acc:  58.331999999999994 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6781078766297249  Time:  1.1187639236450195\n",
      "Test Loss:  0.6712072224033122\n",
      "Test Acc:  56.962 %\n",
      "================================================== 2\n",
      "Train Loss:  0.678832890343492  Time:  1.1205191612243652\n",
      "Test Loss:  0.6685353064415406\n",
      "Test Acc:  58.282000000000004 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6771106998415759  Time:  1.1645760536193848\n",
      "Test Loss:  0.6685096645841793\n",
      "Test Acc:  58.192 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6771024839721457  Time:  1.1578240394592285\n",
      "Test Loss:  0.6677880481797822\n",
      "Test Acc:  57.730000000000004 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6774138116488492  Time:  1.1631836891174316\n",
      "Test Loss:  0.6701249455919072\n",
      "Test Acc:  59.526 %\n",
      "================================================== 2\n",
      "updating model =======  59.526\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=122, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=122, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=29, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=29, out_features=91, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6887437886130201  Time:  1.184384822845459\n",
      "Test Loss:  0.6695119467925053\n",
      "Test Acc:  59.416000000000004 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6801914835933351  Time:  1.1577479839324951\n",
      "Test Loss:  0.6710007692478142\n",
      "Test Acc:  56.97 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6794401964131933  Time:  1.167194128036499\n",
      "Test Loss:  0.6662531458601659\n",
      "Test Acc:  58.138 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6783472660684238  Time:  1.1745188236236572\n",
      "Test Loss:  0.6679146256373854\n",
      "Test Acc:  58.068 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6785856370073166  Time:  1.15224289894104\n",
      "Test Loss:  0.6690924885321636\n",
      "Test Acc:  60.638000000000005 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6764281927234065  Time:  1.198390007019043\n",
      "Test Loss:  0.6677541039427932\n",
      "Test Acc:  58.087999999999994 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6767883885950938  Time:  1.1938848495483398\n",
      "Test Loss:  0.6670152432456309\n",
      "Test Acc:  58.24 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6768558769330492  Time:  1.189342975616455\n",
      "Test Loss:  0.6691114029714039\n",
      "Test Acc:  58.221999999999994 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6757945975247961  Time:  1.1853959560394287\n",
      "Test Loss:  0.6660499752176051\n",
      "Test Acc:  59.51 %\n",
      "================================================== 2\n",
      "Train Loss:  0.675800023070217  Time:  1.1860921382904053\n",
      "Test Loss:  0.6682009572277263\n",
      "Test Acc:  58.812 %\n",
      "================================================== 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=6, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=6, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=57, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=57, out_features=23, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=23, out_features=31, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=31, out_features=49, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=49, out_features=8, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=8, out_features=61, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=61, out_features=98, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=98, out_features=11, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=11, out_features=5, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=5, out_features=104, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=104, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6934021013061495  Time:  1.5648391246795654\n",
      "Test Loss:  0.6931698279721397\n",
      "Test Acc:  49.944 %\n",
      "================================================== 2\n",
      "Train Loss:  0.689961508025218  Time:  1.5743250846862793\n",
      "Test Loss:  0.6839596501418522\n",
      "Test Acc:  53.87 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6841845956161945  Time:  1.5689959526062012\n",
      "Test Loss:  0.6814824205880262\n",
      "Test Acc:  54.172 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6842811823326306  Time:  1.5820989608764648\n",
      "Test Loss:  0.6806877915348325\n",
      "Test Acc:  54.368 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6826650082629963  Time:  1.6194229125976562\n",
      "Test Loss:  0.6846394502386755\n",
      "Test Acc:  55.788000000000004 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6845715487525411  Time:  1.6339819431304932\n",
      "Test Loss:  0.6835324408448472\n",
      "Test Acc:  54.336 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6826271221150447  Time:  1.6742479801177979\n",
      "Test Loss:  0.6811069970836445\n",
      "Test Acc:  55.657999999999994 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6844318889788468  Time:  1.6638462543487549\n",
      "Test Loss:  0.6801862120628357\n",
      "Test Acc:  56.03 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6831672715879705  Time:  1.5835611820220947\n",
      "Test Loss:  0.6792575704808138\n",
      "Test Acc:  55.01200000000001 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6831682167784141  Time:  1.5811419486999512\n",
      "Test Loss:  0.6796526857176606\n",
      "Test Acc:  54.96 %\n",
      "================================================== 2\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=35, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=35, out_features=71, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=71, out_features=44, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=44, out_features=122, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=122, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6899800537711512  Time:  1.1174042224884033\n",
      "Test Loss:  0.6782291446413312\n",
      "Test Acc:  57.542 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6824252485793872  Time:  1.118232011795044\n",
      "Test Loss:  0.6725214141972211\n",
      "Test Acc:  58.848 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6800986552325479  Time:  1.1223039627075195\n",
      "Test Loss:  0.6711990748132978\n",
      "Test Acc:  57.635999999999996 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6789366465850468  Time:  1.1618609428405762\n",
      "Test Loss:  0.6704608128995312\n",
      "Test Acc:  58.675999999999995 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6785973947848717  Time:  1.1652641296386719\n",
      "Test Loss:  0.669440260347055\n",
      "Test Acc:  58.80800000000001 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6776610032050279  Time:  1.1678800582885742\n",
      "Test Loss:  0.6693170827870466\n",
      "Test Acc:  57.18 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6784492091540872  Time:  1.1625869274139404\n",
      "Test Loss:  0.6689886234852732\n",
      "Test Acc:  57.782 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6788840924736357  Time:  1.1282548904418945\n",
      "Test Loss:  0.6701315534966332\n",
      "Test Acc:  59.775999999999996 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6768826589967213  Time:  1.1137852668762207\n",
      "Test Loss:  0.6688396477577637\n",
      "Test Acc:  58.782000000000004 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6784471421346178  Time:  1.1243751049041748\n",
      "Test Loss:  0.6684287242135223\n",
      "Test Acc:  58.884 %\n",
      "================================================== 3\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=29, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=29, out_features=91, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6888658386947465  Time:  1.1355080604553223\n",
      "Test Loss:  0.6763103440099832\n",
      "Test Acc:  56.657999999999994 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6818951137744598  Time:  1.1625111103057861\n",
      "Test Loss:  0.6709242940557246\n",
      "Test Acc:  58.040000000000006 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6795546675685549  Time:  1.1672871112823486\n",
      "Test Loss:  0.6709247930925719\n",
      "Test Acc:  56.86600000000001 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6783670217451387  Time:  1.188183069229126\n",
      "Test Loss:  0.6676146804678197\n",
      "Test Acc:  59.550000000000004 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6773753377207874  Time:  1.2062289714813232\n",
      "Test Loss:  0.6698494894163949\n",
      "Test Acc:  58.042 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6778261467923213  Time:  1.2041327953338623\n",
      "Test Loss:  0.6670115076157511\n",
      "Test Acc:  59.297999999999995 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6773657579056538  Time:  1.142348051071167\n",
      "Test Loss:  0.6668370642832347\n",
      "Test Acc:  59.216 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6780639960818047  Time:  1.137200117111206\n",
      "Test Loss:  0.6685665988800477\n",
      "Test Acc:  59.606 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6767948602672911  Time:  1.1341629028320312\n",
      "Test Loss:  0.668130946402647\n",
      "Test Acc:  59.8 %\n",
      "================================================== 3\n",
      "Train Loss:  0.676439144533046  Time:  1.133803129196167\n",
      "Test Loss:  0.6692517259899451\n",
      "Test Acc:  57.486000000000004 %\n",
      "================================================== 3\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=35, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=35, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=29, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=29, out_features=91, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6906958785805389  Time:  1.1556882858276367\n",
      "Test Loss:  0.6771621004659303\n",
      "Test Acc:  57.428000000000004 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6824564329028999  Time:  1.1580758094787598\n",
      "Test Loss:  0.6717428543737957\n",
      "Test Acc:  58.443999999999996 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6798444499064537  Time:  1.1559278964996338\n",
      "Test Loss:  0.6692898370781724\n",
      "Test Acc:  57.726 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6794869340684292  Time:  1.177860975265503\n",
      "Test Loss:  0.6726832034028306\n",
      "Test Acc:  58.912 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6790588839645804  Time:  1.1329050064086914\n",
      "Test Loss:  0.6694697670790614\n",
      "Test Acc:  58.126 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6777506757826701  Time:  1.1187560558319092\n",
      "Test Loss:  0.673803272904182\n",
      "Test Acc:  56.072 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6777260362231818  Time:  1.1333200931549072\n",
      "Test Loss:  0.6658988780513102\n",
      "Test Acc:  57.89 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6790332657142277  Time:  1.1162481307983398\n",
      "Test Loss:  0.6681434849695284\n",
      "Test Acc:  58.126 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6779551358118544  Time:  1.1247270107269287\n",
      "Test Loss:  0.6678680330514908\n",
      "Test Acc:  58.879999999999995 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6769344436861303  Time:  1.1526710987091064\n",
      "Test Loss:  0.6697026743572585\n",
      "Test Acc:  58.338 %\n",
      "================================================== 3\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=7, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=7, out_features=8, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=8, out_features=72, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=72, out_features=96, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=96, out_features=75, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=75, out_features=134, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=134, out_features=7, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=7, out_features=86, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=86, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.690547453008429  Time:  1.5702579021453857\n",
      "Test Loss:  0.6786640611838322\n",
      "Test Acc:  59.484 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6808758589038013  Time:  1.5668506622314453\n",
      "Test Loss:  0.6673363033606081\n",
      "Test Acc:  57.872 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6786074275082915  Time:  1.5781009197235107\n",
      "Test Loss:  0.6681386585138283\n",
      "Test Acc:  59.534 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6774464891339741  Time:  1.509411096572876\n",
      "Test Loss:  0.6649703882178482\n",
      "Test Acc:  59.06 %\n",
      "================================================== 3\n",
      "Train Loss:  0.678258400328838  Time:  1.5472640991210938\n",
      "Test Loss:  0.6687999468068687\n",
      "Test Acc:  60.916000000000004 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6771653155340766  Time:  1.533790111541748\n",
      "Test Loss:  0.666420440892784\n",
      "Test Acc:  59.44199999999999 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6772738573324941  Time:  1.5454611778259277\n",
      "Test Loss:  0.6694693817776076\n",
      "Test Acc:  56.69800000000001 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6785660594919302  Time:  1.6223831176757812\n",
      "Test Loss:  0.667376202892284\n",
      "Test Acc:  60.187999999999995 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6768804288258518  Time:  1.6196348667144775\n",
      "Test Loss:  0.6691739291560893\n",
      "Test Acc:  61.23 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6775911650518431  Time:  1.574962854385376\n",
      "Test Loss:  0.6672146612284134\n",
      "Test Acc:  61.25000000000001 %\n",
      "================================================== 3\n",
      "updating model =======  61.25000000000001\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=125, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=125, out_features=62, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=62, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=64, out_features=9, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=9, out_features=98, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=98, out_features=106, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=106, out_features=9, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=9, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6924250523974426  Time:  1.5919947624206543\n",
      "Test Loss:  0.6863966502097189\n",
      "Test Acc:  53.408 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6840058756570746  Time:  1.5756983757019043\n",
      "Test Loss:  0.6748703067400017\n",
      "Test Acc:  58.152 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6805357456642346  Time:  1.5665862560272217\n",
      "Test Loss:  0.6753721586903747\n",
      "Test Acc:  56.618 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6792962257444424  Time:  1.611142873764038\n",
      "Test Loss:  0.6696837206884306\n",
      "Test Acc:  59.704 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6779062671818002  Time:  1.6136538982391357\n",
      "Test Loss:  0.6680375191630149\n",
      "Test Acc:  59.816 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6772645779334716  Time:  1.6020090579986572\n",
      "Test Loss:  0.6690054003681455\n",
      "Test Acc:  58.275999999999996 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6759019861691189  Time:  1.6599421501159668\n",
      "Test Loss:  0.6696019522389587\n",
      "Test Acc:  56.489999999999995 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6778627764569581  Time:  1.644251823425293\n",
      "Test Loss:  0.6707961866442038\n",
      "Test Acc:  61.77 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6767826034639873  Time:  1.6496829986572266\n",
      "Test Loss:  0.6655264861729681\n",
      "Test Acc:  60.26200000000001 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6771740711083377  Time:  1.6576769351959229\n",
      "Test Loss:  0.6668922192588145\n",
      "Test Acc:  59.858 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=109, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=109, out_features=44, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=44, out_features=7, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=7, out_features=8, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=8, out_features=72, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=72, out_features=96, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=96, out_features=75, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=75, out_features=134, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=134, out_features=7, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=7, out_features=86, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=86, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933587902653826  Time:  1.549339771270752\n",
      "Test Loss:  0.693157752861782\n",
      "Test Acc:  50.056 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6876075720264964  Time:  1.5492939949035645\n",
      "Test Loss:  0.6723653245337156\n",
      "Test Acc:  57.858 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6797302256535439  Time:  1.5453591346740723\n",
      "Test Loss:  0.6668386620526411\n",
      "Test Acc:  59.194 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6778398341070997  Time:  1.5561790466308594\n",
      "Test Loss:  0.6672652287750828\n",
      "Test Acc:  58.577999999999996 %\n",
      "================================================== 4\n",
      "Train Loss:  0.678212992248744  Time:  1.5734241008758545\n",
      "Test Loss:  0.6678802073001862\n",
      "Test Acc:  60.606 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6777371348690813  Time:  1.5690927505493164\n",
      "Test Loss:  0.6675663444460654\n",
      "Test Acc:  58.182 %\n",
      "================================================== 4\n",
      "Train Loss:  0.677039747255562  Time:  1.6051881313323975\n",
      "Test Loss:  0.667680199961273\n",
      "Test Acc:  60.192 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6769866667089671  Time:  1.6127171516418457\n",
      "Test Loss:  0.6680741279709096\n",
      "Test Acc:  57.316 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6766015134588645  Time:  1.6322970390319824\n",
      "Test Loss:  0.669104092279259\n",
      "Test Acc:  60.846000000000004 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6767958095909035  Time:  1.6132416725158691\n",
      "Test Loss:  0.6662168782584521\n",
      "Test Acc:  57.824 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=7, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=7, out_features=8, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=8, out_features=72, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=72, out_features=96, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=96, out_features=75, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=75, out_features=134, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=134, out_features=7, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=7, out_features=86, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=86, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932910946163818  Time:  1.5156638622283936\n",
      "Test Loss:  0.6931898980116358\n",
      "Test Acc:  49.944 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6850830612391451  Time:  1.5226480960845947\n",
      "Test Loss:  0.6691490095488879\n",
      "Test Acc:  58.618 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6799277641477376  Time:  1.5420911312103271\n",
      "Test Loss:  0.6706772431427118\n",
      "Test Acc:  61.036 %\n",
      "================================================== 4\n",
      "Train Loss:  0.680167436164661  Time:  1.6291017532348633\n",
      "Test Loss:  0.6679902855230837\n",
      "Test Acc:  59.330000000000005 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6782228004758375  Time:  1.6349067687988281\n",
      "Test Loss:  0.6668153979948589\n",
      "Test Acc:  59.384 %\n",
      "================================================== 4\n",
      "Train Loss:  0.678502550742922  Time:  1.6280932426452637\n",
      "Test Loss:  0.6698393663581537\n",
      "Test Acc:  57.766 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6781810021748508  Time:  1.6297149658203125\n",
      "Test Loss:  0.6687407067843846\n",
      "Test Acc:  59.53000000000001 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6772549898520003  Time:  1.617302656173706\n",
      "Test Loss:  0.669028715211518\n",
      "Test Acc:  56.818000000000005 %\n",
      "================================================== 4\n",
      "Train Loss:  0.677319766831224  Time:  1.5962259769439697\n",
      "Test Loss:  0.6654528850803569\n",
      "Test Acc:  58.498000000000005 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6777053192584184  Time:  1.5985660552978516\n",
      "Test Loss:  0.6668134848682248\n",
      "Test Acc:  60.494 %\n",
      "================================================== 4\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=30, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=30, out_features=88, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=88, out_features=118, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=118, out_features=88, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=88, out_features=85, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=85, out_features=63, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=63, out_features=54, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=54, out_features=108, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=108, out_features=60, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=60, out_features=23, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=23, out_features=31, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=31, out_features=137, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=137, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6923104370162435  Time:  1.8078091144561768\n",
      "Test Loss:  0.6803657865645935\n",
      "Test Acc:  56.564 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6848191081607429  Time:  1.772453784942627\n",
      "Test Loss:  0.6749901576917998\n",
      "Test Acc:  57.98 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6809232489471018  Time:  1.8627898693084717\n",
      "Test Loss:  0.6696063836618346\n",
      "Test Acc:  59.01 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6806485487596832  Time:  1.940458059310913\n",
      "Test Loss:  0.6714999237839057\n",
      "Test Acc:  57.274 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6791270011967986  Time:  1.923367977142334\n",
      "Test Loss:  0.6718631484070603\n",
      "Test Acc:  57.709999999999994 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6784293856063899  Time:  1.9303898811340332\n",
      "Test Loss:  0.6684806869954479\n",
      "Test Acc:  60.309999999999995 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6800544196671813  Time:  1.8977930545806885\n",
      "Test Loss:  0.6723409690419022\n",
      "Test Acc:  57.089999999999996 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6788533747631268  Time:  1.9119579792022705\n",
      "Test Loss:  0.6706479027563211\n",
      "Test Acc:  59.001999999999995 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6780448706045638  Time:  1.9242937564849854\n",
      "Test Loss:  0.6695243390847225\n",
      "Test Acc:  60.18 %\n",
      "================================================== 4\n",
      "Train Loss:  0.6769885597002767  Time:  1.9040498733520508\n",
      "Test Loss:  0.6693548769975195\n",
      "Test Acc:  56.862 %\n",
      "================================================== 4\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=26, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=26, out_features=114, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=114, out_features=109, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=109, out_features=36, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=36, out_features=77, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=46, out_features=82, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=82, out_features=26, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=26, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.687954324005294  Time:  1.5403101444244385\n",
      "Test Loss:  0.6717609063703187\n",
      "Test Acc:  58.652 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6808528204033845  Time:  1.598132848739624\n",
      "Test Loss:  0.6677248794205335\n",
      "Test Acc:  59.158 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6784863232696143  Time:  1.5895862579345703\n",
      "Test Loss:  0.6682501453526166\n",
      "Test Acc:  58.582 %\n",
      "================================================== 5\n",
      "Train Loss:  0.678395602607379  Time:  1.6037518978118896\n",
      "Test Loss:  0.6715765276125499\n",
      "Test Acc:  58.64 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6788934310422327  Time:  1.6420307159423828\n",
      "Test Loss:  0.6678105884668778\n",
      "Test Acc:  57.452000000000005 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6772921787996362  Time:  1.605895757675171\n",
      "Test Loss:  0.6681626709748287\n",
      "Test Acc:  58.838 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6779344060125142  Time:  1.5650758743286133\n",
      "Test Loss:  0.6760641388138946\n",
      "Test Acc:  59.28600000000001 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6770620550552424  Time:  1.5564298629760742\n",
      "Test Loss:  0.6680592283302423\n",
      "Test Acc:  58.37 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6779429222980555  Time:  1.5578739643096924\n",
      "Test Loss:  0.6694343172165812\n",
      "Test Acc:  57.272 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6770351859774902  Time:  1.5994501113891602\n",
      "Test Loss:  0.6687075693388375\n",
      "Test Acc:  59.21999999999999 %\n",
      "================================================== 5\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=68, out_features=7, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=7, out_features=8, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=8, out_features=72, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=72, out_features=96, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=96, out_features=75, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=75, out_features=134, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=134, out_features=7, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=7, out_features=86, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=86, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6935255512268874  Time:  1.5871801376342773\n",
      "Test Loss:  0.6886239994545372\n",
      "Test Acc:  53.272 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6827877392298984  Time:  1.5827679634094238\n",
      "Test Loss:  0.6683444484156005\n",
      "Test Acc:  58.587999999999994 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6783530070398845  Time:  1.60502290725708\n",
      "Test Loss:  0.67088064125606\n",
      "Test Acc:  60.672000000000004 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6790688072677946  Time:  1.6375186443328857\n",
      "Test Loss:  0.669975474172709\n",
      "Test Acc:  59.008 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6789624967279225  Time:  1.5796551704406738\n",
      "Test Loss:  0.6668376867868462\n",
      "Test Acc:  60.006 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6786962912900605  Time:  1.5926482677459717\n",
      "Test Loss:  0.6706923009181509\n",
      "Test Acc:  59.56 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6784940074830159  Time:  1.5945351123809814\n",
      "Test Loss:  0.666665436357868\n",
      "Test Acc:  59.318000000000005 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6764555036151496  Time:  1.5929186344146729\n",
      "Test Loss:  0.6710995566480014\n",
      "Test Acc:  57.252 %\n",
      "================================================== 5\n",
      "Train Loss:  0.676361529061394  Time:  1.6366288661956787\n",
      "Test Loss:  0.6665468772455138\n",
      "Test Acc:  57.394 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6764643881877843  Time:  1.6254231929779053\n",
      "Test Loss:  0.6663627077122124\n",
      "Test Acc:  56.989999999999995 %\n",
      "================================================== 5\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=12, out_features=7, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=7, out_features=8, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=8, out_features=72, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=72, out_features=96, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=96, out_features=75, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=75, out_features=134, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=134, out_features=7, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=7, out_features=86, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=86, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6913448965897525  Time:  1.5715110301971436\n",
      "Test Loss:  0.6750243436925265\n",
      "Test Acc:  58.95399999999999 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6813450462191645  Time:  1.583801031112671\n",
      "Test Loss:  0.6679588462017021\n",
      "Test Acc:  59.187999999999995 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6794111297948517  Time:  1.54215407371521\n",
      "Test Loss:  0.6707613465129113\n",
      "Test Acc:  59.102 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6792361969060271  Time:  1.555619716644287\n",
      "Test Loss:  0.6703981790615587\n",
      "Test Acc:  59.065999999999995 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6778993928519479  Time:  1.544219970703125\n",
      "Test Loss:  0.6683479717799595\n",
      "Test Acc:  57.199999999999996 %\n",
      "================================================== 5\n",
      "Train Loss:  0.677280755156148  Time:  1.5719220638275146\n",
      "Test Loss:  0.6695723831653595\n",
      "Test Acc:  57.32000000000001 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6784598975285997  Time:  1.5742559432983398\n",
      "Test Loss:  0.6695057944375642\n",
      "Test Acc:  58.448 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6777546853914748  Time:  1.60294508934021\n",
      "Test Loss:  0.6673155065093722\n",
      "Test Acc:  60.966 %\n",
      "================================================== 5\n",
      "Train Loss:  0.677325231532981  Time:  1.6026058197021484\n",
      "Test Loss:  0.6673166894791077\n",
      "Test Acc:  60.418000000000006 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6771439875129366  Time:  1.5959069728851318\n",
      "Test Loss:  0.6658569689916105\n",
      "Test Acc:  58.833999999999996 %\n",
      "================================================== 5\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=7, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=7, out_features=60, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=60, out_features=54, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=54, out_features=17, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=17, out_features=77, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=77, out_features=10, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=10, out_features=74, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=74, out_features=124, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=124, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6934606250185166  Time:  1.4233672618865967\n",
      "Test Loss:  0.6931461378627893\n",
      "Test Acc:  50.056 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6911834933026864  Time:  1.373457908630371\n",
      "Test Loss:  0.6836687879902976\n",
      "Test Acc:  55.48800000000001 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6855242861448413  Time:  1.3921077251434326\n",
      "Test Loss:  0.6803226112102976\n",
      "Test Acc:  55.510000000000005 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6847597343208146  Time:  1.3926210403442383\n",
      "Test Loss:  0.6812031004501848\n",
      "Test Acc:  54.722 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6840273265856026  Time:  1.4065699577331543\n",
      "Test Loss:  0.680455421306649\n",
      "Test Acc:  54.425999999999995 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6831535873186849  Time:  1.4656741619110107\n",
      "Test Loss:  0.6788812805803455\n",
      "Test Acc:  55.644000000000005 %\n",
      "================================================== 5\n",
      "Train Loss:  0.682761126626147  Time:  1.4471991062164307\n",
      "Test Loss:  0.678521670553149\n",
      "Test Acc:  55.132000000000005 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6825987503041316  Time:  1.4567172527313232\n",
      "Test Loss:  0.6802056170239741\n",
      "Test Acc:  55.523999999999994 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6828449307567012  Time:  1.4445979595184326\n",
      "Test Loss:  0.6784539508576296\n",
      "Test Acc:  55.318 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6820184397001336  Time:  1.4500160217285156\n",
      "Test Loss:  0.6807189650681554\n",
      "Test Acc:  56.518 %\n",
      "================================================== 5\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=12, out_features=19, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=19, out_features=71, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=71, out_features=128, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=6, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=26, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=26, out_features=53, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=53, out_features=138, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=138, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6878805465071741  Time:  1.5364031791687012\n",
      "Test Loss:  0.6767764112779072\n",
      "Test Acc:  56.733999999999995 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6807764479278648  Time:  1.5369651317596436\n",
      "Test Loss:  0.6677148098848305\n",
      "Test Acc:  60.682 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6805527419069387  Time:  1.550417184829712\n",
      "Test Loss:  0.6706490778193182\n",
      "Test Acc:  56.562 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6792220551602162  Time:  1.5782639980316162\n",
      "Test Loss:  0.6716299999733361\n",
      "Test Acc:  56.903999999999996 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6796338492936461  Time:  1.5979151725769043\n",
      "Test Loss:  0.6670784147418275\n",
      "Test Acc:  57.628 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6774935607057418  Time:  1.595731258392334\n",
      "Test Loss:  0.6697608579178246\n",
      "Test Acc:  58.465999999999994 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6783521917179554  Time:  1.6109569072723389\n",
      "Test Loss:  0.6665038061993462\n",
      "Test Acc:  60.202 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6780158105122782  Time:  1.6473569869995117\n",
      "Test Loss:  0.6677434100788466\n",
      "Test Acc:  57.284 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6762123421160844  Time:  1.5939271450042725\n",
      "Test Loss:  0.6663427997608574\n",
      "Test Acc:  57.272 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6774416692065497  Time:  1.569688081741333\n",
      "Test Loss:  0.6685374026395836\n",
      "Test Acc:  60.306000000000004 %\n",
      "================================================== 6\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=117, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=117, out_features=75, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=75, out_features=26, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=26, out_features=114, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=114, out_features=109, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=109, out_features=36, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=36, out_features=77, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=46, out_features=82, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=82, out_features=26, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=26, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6909522455539147  Time:  1.580190896987915\n",
      "Test Loss:  0.6738121138543499\n",
      "Test Acc:  59.336 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6814233523650761  Time:  1.6067309379577637\n",
      "Test Loss:  0.6699211059176192\n",
      "Test Acc:  57.142 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6813613422160602  Time:  1.6344051361083984\n",
      "Test Loss:  0.6675010807051951\n",
      "Test Acc:  59.275999999999996 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6790568809004596  Time:  1.6895740032196045\n",
      "Test Loss:  0.667096485592881\n",
      "Test Acc:  59.314 %\n",
      "================================================== 6\n",
      "Train Loss:  0.678168348152272  Time:  1.6818430423736572\n",
      "Test Loss:  0.6687391534143564\n",
      "Test Acc:  60.422 %\n",
      "================================================== 6\n",
      "Train Loss:  0.678228015447185  Time:  1.6692161560058594\n",
      "Test Loss:  0.6680015483681037\n",
      "Test Acc:  58.17400000000001 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6771781235715769  Time:  1.6746361255645752\n",
      "Test Loss:  0.6672068232176255\n",
      "Test Acc:  59.150000000000006 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6772926197434864  Time:  1.6514480113983154\n",
      "Test Loss:  0.6670668888456968\n",
      "Test Acc:  59.5 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6764665504876715  Time:  1.6447067260742188\n",
      "Test Loss:  0.6658964613262488\n",
      "Test Acc:  60.78 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6765296006724782  Time:  1.6503901481628418\n",
      "Test Loss:  0.6669487928857609\n",
      "Test Acc:  58.452000000000005 %\n",
      "================================================== 6\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=26, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=26, out_features=114, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=114, out_features=109, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=109, out_features=36, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=36, out_features=77, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=46, out_features=82, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=82, out_features=26, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=26, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6924380831909875  Time:  1.5582079887390137\n",
      "Test Loss:  0.6736221042822819\n",
      "Test Acc:  59.646 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6828590931683561  Time:  1.5895280838012695\n",
      "Test Loss:  0.671321226321921\n",
      "Test Acc:  57.65 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6800431909352324  Time:  1.604853868484497\n",
      "Test Loss:  0.6665116028518093\n",
      "Test Acc:  59.660000000000004 %\n",
      "================================================== 6\n",
      "Train Loss:  0.679483531165297  Time:  1.6178569793701172\n",
      "Test Loss:  0.6709502877629533\n",
      "Test Acc:  57.48799999999999 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6788680794900351  Time:  1.633404016494751\n",
      "Test Loss:  0.6696919753235213\n",
      "Test Acc:  59.97 %\n",
      "================================================== 6\n",
      "Train Loss:  0.678323444223752  Time:  1.6159448623657227\n",
      "Test Loss:  0.6698130268831642\n",
      "Test Acc:  57.440000000000005 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6784357054390177  Time:  1.5824170112609863\n",
      "Test Loss:  0.6711093293769019\n",
      "Test Acc:  58.614 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6779662003047275  Time:  1.5942909717559814\n",
      "Test Loss:  0.6681486283029828\n",
      "Test Acc:  59.582 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6769021125170436  Time:  1.5767180919647217\n",
      "Test Loss:  0.6670100910931217\n",
      "Test Acc:  58.984 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6762777388530926  Time:  1.6668121814727783\n",
      "Test Loss:  0.6697759461037966\n",
      "Test Acc:  59.211999999999996 %\n",
      "================================================== 6\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=105, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=105, out_features=66, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=66, out_features=65, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=65, out_features=135, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=135, out_features=85, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=85, out_features=47, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=47, out_features=101, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=101, out_features=130, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=130, out_features=109, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=109, out_features=104, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=104, out_features=19, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=19, out_features=92, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=92, out_features=59, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=59, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932772928780883  Time:  2.460400104522705\n",
      "Test Loss:  0.6931548772417769\n",
      "Test Acc:  49.944 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6923706494543674  Time:  2.32184100151062\n",
      "Test Loss:  0.689184310789011\n",
      "Test Acc:  52.138 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6871627153706377  Time:  2.255857229232788\n",
      "Test Loss:  0.6811564567745948\n",
      "Test Acc:  54.11 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6859018154387927  Time:  2.4101901054382324\n",
      "Test Loss:  0.6818785798184726\n",
      "Test Acc:  54.154 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6850146538149702  Time:  2.5559802055358887\n",
      "Test Loss:  0.6824709380767784\n",
      "Test Acc:  54.35 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6844975663362628  Time:  2.628603935241699\n",
      "Test Loss:  0.6792405475767291\n",
      "Test Acc:  55.467999999999996 %\n",
      "================================================== 6\n",
      "Train Loss:  0.683173242929208  Time:  2.6960208415985107\n",
      "Test Loss:  0.6814845514540769\n",
      "Test Acc:  55.518 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6839957979038684  Time:  2.590547800064087\n",
      "Test Loss:  0.6813409553498638\n",
      "Test Acc:  54.901999999999994 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6824370272403216  Time:  2.488191843032837\n",
      "Test Loss:  0.6803810769806102\n",
      "Test Acc:  55.19 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6831817518185525  Time:  2.510918140411377\n",
      "Test Loss:  0.6792196543241034\n",
      "Test Acc:  55.458 %\n",
      "================================================== 6\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=12, out_features=73, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=73, out_features=98, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=98, out_features=49, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=49, out_features=41, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=41, out_features=54, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=54, out_features=81, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=81, out_features=6, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=6, out_features=115, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933940912250185  Time:  1.6995999813079834\n",
      "Test Loss:  0.6910237654739496\n",
      "Test Acc:  53.766000000000005 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6846118867397308  Time:  1.6618931293487549\n",
      "Test Loss:  0.6705020124814949\n",
      "Test Acc:  58.955999999999996 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6804854706691129  Time:  1.6525490283966064\n",
      "Test Loss:  0.6676059292287243\n",
      "Test Acc:  60.318000000000005 %\n",
      "================================================== 7\n",
      "Train Loss:  0.678491746422148  Time:  1.5520319938659668\n",
      "Test Loss:  0.6689363024672683\n",
      "Test Acc:  58.43000000000001 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6783184054559165  Time:  1.5730080604553223\n",
      "Test Loss:  0.667899812362632\n",
      "Test Acc:  58.56 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6774340504712432  Time:  1.5819218158721924\n",
      "Test Loss:  0.6683610848018101\n",
      "Test Acc:  57.916000000000004 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6775167092789699  Time:  2.035261869430542\n",
      "Test Loss:  0.6655460538304582\n",
      "Test Acc:  58.948 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6775738170547206  Time:  2.0323450565338135\n",
      "Test Loss:  0.6673092887717851\n",
      "Test Acc:  58.158 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6777422312837448  Time:  1.7700319290161133\n",
      "Test Loss:  0.6669187430216341\n",
      "Test Acc:  58.699999999999996 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6771660784300226  Time:  2.119040012359619\n",
      "Test Loss:  0.6677680420024055\n",
      "Test Acc:  59.794000000000004 %\n",
      "================================================== 7\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=77, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=77, out_features=19, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=19, out_features=71, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=71, out_features=128, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=6, out_features=100, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=100, out_features=26, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=26, out_features=53, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=53, out_features=138, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=138, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6893585992120478  Time:  1.7107930183410645\n",
      "Test Loss:  0.679547135623134\n",
      "Test Acc:  56.77799999999999 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6823773904003366  Time:  1.7213733196258545\n",
      "Test Loss:  0.6712654032269303\n",
      "Test Acc:  56.98800000000001 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6803252914526167  Time:  1.6438071727752686\n",
      "Test Loss:  0.6687003118651254\n",
      "Test Acc:  60.150000000000006 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6794322346248766  Time:  1.7519888877868652\n",
      "Test Loss:  0.6690332676683154\n",
      "Test Acc:  60.338 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6793207371756979  Time:  1.8064770698547363\n",
      "Test Loss:  0.6705315791222514\n",
      "Test Acc:  58.462 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6784017157815668  Time:  1.970552682876587\n",
      "Test Loss:  0.6691406116801866\n",
      "Test Acc:  57.45400000000001 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6780571952788499  Time:  1.682084083557129\n",
      "Test Loss:  0.6683318192253307\n",
      "Test Acc:  58.650000000000006 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6784470440262426  Time:  1.7581031322479248\n",
      "Test Loss:  0.6690330283374203\n",
      "Test Acc:  58.955999999999996 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6767368727791918  Time:  1.6725831031799316\n",
      "Test Loss:  0.6701862885027515\n",
      "Test Acc:  57.128 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6769945456163726  Time:  1.7310051918029785\n",
      "Test Loss:  0.6649177238649252\n",
      "Test Acc:  60.824 %\n",
      "================================================== 7\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=120, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=19, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=19, out_features=71, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=71, out_features=128, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=6, out_features=100, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=100, out_features=26, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=26, out_features=53, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=53, out_features=138, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=138, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.69338653557492  Time:  1.768782138824463\n",
      "Test Loss:  0.6931450777516073\n",
      "Test Acc:  50.056 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6932735380030026  Time:  1.6456680297851562\n",
      "Test Loss:  0.6932860339174465\n",
      "Test Acc:  50.056 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6933185893253688  Time:  1.6865968704223633\n",
      "Test Loss:  0.6932563669219309\n",
      "Test Acc:  49.944 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6931862702769954  Time:  1.7044281959533691\n",
      "Test Loss:  0.6931937485933304\n",
      "Test Acc:  49.944 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6932084499484431  Time:  1.7197678089141846\n",
      "Test Loss:  0.6931455637119255\n",
      "Test Acc:  50.056 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6931783165374812  Time:  1.6939671039581299\n",
      "Test Loss:  0.6931634968032643\n",
      "Test Acc:  49.944 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6931700008193942  Time:  1.8036789894104004\n",
      "Test Loss:  0.6931795626878738\n",
      "Test Acc:  49.944 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6931860549171476  Time:  1.9759299755096436\n",
      "Test Loss:  0.6931600062822809\n",
      "Test Acc:  49.944 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6931879083170508  Time:  1.712209701538086\n",
      "Test Loss:  0.693150955499435\n",
      "Test Acc:  50.056 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6931849631079792  Time:  1.6619441509246826\n",
      "Test Loss:  0.6931564363898063\n",
      "Test Acc:  49.944 %\n",
      "================================================== 7\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=29, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=29, out_features=25, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=25, out_features=7, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=7, out_features=40, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=40, out_features=44, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=44, out_features=15, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=15, out_features=106, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=106, out_features=24, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=24, out_features=65, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=65, out_features=43, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=43, out_features=53, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=53, out_features=109, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=109, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933383125893391  Time:  1.643423318862915\n",
      "Test Loss:  0.6925857365131378\n",
      "Test Acc:  53.096 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6858115418113931  Time:  1.646785020828247\n",
      "Test Loss:  0.6756350100040436\n",
      "Test Acc:  57.19800000000001 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6816438660569435  Time:  1.6145269870758057\n",
      "Test Loss:  0.6707143817020922\n",
      "Test Acc:  56.442 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6796198039159288  Time:  1.6700520515441895\n",
      "Test Loss:  0.6695303719262687\n",
      "Test Acc:  60.397999999999996 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6791760827938136  Time:  1.6847219467163086\n",
      "Test Loss:  0.6712805598366017\n",
      "Test Acc:  56.477999999999994 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6796383018041179  Time:  3.7422730922698975\n",
      "Test Loss:  0.6713595283882958\n",
      "Test Acc:  60.087999999999994 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6794186826169926  Time:  1.7140679359436035\n",
      "Test Loss:  0.6736059870038714\n",
      "Test Acc:  57.816 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6789310822521684  Time:  1.7178239822387695\n",
      "Test Loss:  0.6706326576519985\n",
      "Test Acc:  57.424 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6775377440626604  Time:  1.6910791397094727\n",
      "Test Loss:  0.6706964403999095\n",
      "Test Acc:  59.79 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6772568234562004  Time:  1.6573748588562012\n",
      "Test Loss:  0.6670622372505616\n",
      "Test Acc:  59.202 %\n",
      "================================================== 7\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=89, out_features=77, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=77, out_features=129, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=129, out_features=31, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=31, out_features=128, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=64, out_features=51, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=51, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=103, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=103, out_features=114, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=114, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6909956525277047  Time:  1.7925539016723633\n",
      "Test Loss:  0.6862634399107524\n",
      "Test Acc:  55.52 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6833069700829304  Time:  1.7755591869354248\n",
      "Test Loss:  0.6745192892089182\n",
      "Test Acc:  56.654 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6796512351418934  Time:  1.8284239768981934\n",
      "Test Loss:  0.6724947481131067\n",
      "Test Acc:  59.870000000000005 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6796433357861791  Time:  2.3398361206054688\n",
      "Test Loss:  0.6690129616430828\n",
      "Test Acc:  58.378 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6774646378781674  Time:  2.2238030433654785\n",
      "Test Loss:  0.6739839719874519\n",
      "Test Acc:  56.288000000000004 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6784973009659426  Time:  2.782543182373047\n",
      "Test Loss:  0.668000262306661\n",
      "Test Acc:  58.984 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6782306554543711  Time:  2.30787992477417\n",
      "Test Loss:  0.6716957825179003\n",
      "Test Acc:  58.858 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6769270953470773  Time:  2.0987772941589355\n",
      "Test Loss:  0.6671618083301856\n",
      "Test Acc:  59.016000000000005 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6766189080085198  Time:  2.092540979385376\n",
      "Test Loss:  0.6694042293392882\n",
      "Test Acc:  58.64 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6775540737775121  Time:  2.103868007659912\n",
      "Test Loss:  0.6672602262423963\n",
      "Test Acc:  60.086 %\n",
      "================================================== 8\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=19, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=19, out_features=71, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=71, out_features=128, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=6, out_features=100, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=100, out_features=26, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=26, out_features=53, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=53, out_features=138, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=138, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6901881279301469  Time:  1.6668708324432373\n",
      "Test Loss:  0.6792690522816717\n",
      "Test Acc:  55.764 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6816372421101062  Time:  1.6252429485321045\n",
      "Test Loss:  0.6710342810470231\n",
      "Test Acc:  58.501999999999995 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6794950350792739  Time:  1.629992961883545\n",
      "Test Loss:  0.6693758815526962\n",
      "Test Acc:  57.02 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6799521687692099  Time:  1.6297800540924072\n",
      "Test Loss:  0.6660709000971853\n",
      "Test Acc:  58.18 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6781669265597406  Time:  1.67097806930542\n",
      "Test Loss:  0.6671174633867887\n",
      "Test Acc:  59.848 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6794696191366572  Time:  1.6124749183654785\n",
      "Test Loss:  0.668578642363451\n",
      "Test Acc:  60.672000000000004 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6785138962912733  Time:  1.6070847511291504\n",
      "Test Loss:  0.6708652678192878\n",
      "Test Acc:  57.684000000000005 %\n",
      "================================================== 8\n",
      "Train Loss:  0.677830524253149  Time:  1.6492817401885986\n",
      "Test Loss:  0.6685715716104118\n",
      "Test Acc:  57.02199999999999 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6769785176228432  Time:  1.6222429275512695\n",
      "Test Loss:  0.6665475362417649\n",
      "Test Acc:  61.244 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6761526532851867  Time:  1.708677053451538\n",
      "Test Loss:  0.669017376948376\n",
      "Test Acc:  60.928000000000004 %\n",
      "================================================== 8\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=77, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=77, out_features=19, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=19, out_features=71, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=71, out_features=128, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=6, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=26, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=26, out_features=53, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=53, out_features=138, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=138, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6907984188003261  Time:  1.69160795211792\n",
      "Test Loss:  0.677970078526711\n",
      "Test Acc:  55.754000000000005 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6807246049390222  Time:  1.7211380004882812\n",
      "Test Loss:  0.6720996304434173\n",
      "Test Acc:  56.852000000000004 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6798585185604374  Time:  1.748317003250122\n",
      "Test Loss:  0.6685581608694426\n",
      "Test Acc:  59.684000000000005 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6775017459027088  Time:  1.8472599983215332\n",
      "Test Loss:  0.6696827597155863\n",
      "Test Acc:  57.824 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6775941794371083  Time:  1.774332046508789\n",
      "Test Loss:  0.6670637933575377\n",
      "Test Acc:  57.745999999999995 %\n",
      "================================================== 8\n",
      "Train Loss:  0.677686470703487  Time:  1.7766263484954834\n",
      "Test Loss:  0.669049509325806\n",
      "Test Acc:  57.50999999999999 %\n",
      "================================================== 8\n",
      "Train Loss:  0.677887417756728  Time:  1.662320852279663\n",
      "Test Loss:  0.6662460647675456\n",
      "Test Acc:  60.39600000000001 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6768058377460842  Time:  1.6612207889556885\n",
      "Test Loss:  0.6669245158531227\n",
      "Test Acc:  59.012 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6765798958983734  Time:  1.697908878326416\n",
      "Test Loss:  0.6688960559514104\n",
      "Test Acc:  56.796 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6766879053881568  Time:  1.6825940608978271\n",
      "Test Loss:  0.6664014090688861\n",
      "Test Acc:  58.384 %\n",
      "================================================== 8\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=52, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=52, out_features=45, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=45, out_features=78, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=78, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=16, out_features=21, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=21, out_features=87, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=87, out_features=119, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=119, out_features=71, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=71, out_features=101, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=101, out_features=15, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=15, out_features=47, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=47, out_features=50, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=50, out_features=52, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=52, out_features=56, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=56, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6931679857908374  Time:  2.121123790740967\n",
      "Test Loss:  0.6897872963121959\n",
      "Test Acc:  53.300000000000004 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6861081434427386  Time:  2.0255062580108643\n",
      "Test Loss:  0.6741616689428991\n",
      "Test Acc:  58.326 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6813647388106715  Time:  2.007512092590332\n",
      "Test Loss:  0.6773399917446837\n",
      "Test Acc:  55.888000000000005 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6798906822274201  Time:  1.9872360229492188\n",
      "Test Loss:  0.6679160163110617\n",
      "Test Acc:  59.474000000000004 %\n",
      "================================================== 8\n",
      "Train Loss:  0.679290735591067  Time:  2.055056095123291\n",
      "Test Loss:  0.6700933174211152\n",
      "Test Acc:  59.663999999999994 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6779525079866395  Time:  2.0451300144195557\n",
      "Test Loss:  0.6682650085006442\n",
      "Test Acc:  57.63 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6778901134529253  Time:  2.030825138092041\n",
      "Test Loss:  0.6671956053801945\n",
      "Test Acc:  59.644 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6780065552596628  Time:  2.0567429065704346\n",
      "Test Loss:  0.6687581040421311\n",
      "Test Acc:  58.17400000000001 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6776157354351378  Time:  2.08215594291687\n",
      "Test Loss:  0.6671126381475099\n",
      "Test Acc:  59.17 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6780718876062518  Time:  2.096039056777954\n",
      "Test Loss:  0.668286141996481\n",
      "Test Acc:  58.76 %\n",
      "================================================== 8\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=24, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=24, out_features=52, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=52, out_features=101, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=101, out_features=63, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=63, out_features=74, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=74, out_features=112, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=80, out_features=24, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=24, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6898625673603838  Time:  1.6488099098205566\n",
      "Test Loss:  0.6792694530924972\n",
      "Test Acc:  60.589999999999996 %\n",
      "================================================== 9\n",
      "Train Loss:  0.681778689153003  Time:  1.6038942337036133\n",
      "Test Loss:  0.6722934285596925\n",
      "Test Acc:  56.436 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6811902503462604  Time:  1.6789257526397705\n",
      "Test Loss:  0.6707858832515016\n",
      "Test Acc:  56.523999999999994 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6786721435776593  Time:  1.7072598934173584\n",
      "Test Loss:  0.6705406195655161\n",
      "Test Acc:  56.01199999999999 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6794080860423346  Time:  1.6610119342803955\n",
      "Test Loss:  0.6696297806136462\n",
      "Test Acc:  58.768 %\n",
      "================================================== 9\n",
      "Train Loss:  0.677713071995408  Time:  1.7157361507415771\n",
      "Test Loss:  0.6715694024246566\n",
      "Test Acc:  59.206 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6781656670309332  Time:  1.7471051216125488\n",
      "Test Loss:  0.6680912828567077\n",
      "Test Acc:  57.647999999999996 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6771468185160282  Time:  1.7320542335510254\n",
      "Test Loss:  0.66878136505886\n",
      "Test Acc:  58.382 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6775159894549934  Time:  1.7280209064483643\n",
      "Test Loss:  0.6670541453118227\n",
      "Test Acc:  58.040000000000006 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6776563148863994  Time:  1.715029001235962\n",
      "Test Loss:  0.6665592756198377\n",
      "Test Acc:  61.448 %\n",
      "================================================== 9\n",
      "updating model =======  61.448\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=96, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=96, out_features=117, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=117, out_features=19, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=19, out_features=71, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=71, out_features=128, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=6, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=26, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=26, out_features=53, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=53, out_features=138, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=138, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6910909866764597  Time:  1.6503190994262695\n",
      "Test Loss:  0.678719373685973\n",
      "Test Acc:  55.98 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6817868900125044  Time:  1.6210601329803467\n",
      "Test Loss:  0.6716801259590655\n",
      "Test Acc:  58.577999999999996 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6802088433373583  Time:  1.6932621002197266\n",
      "Test Loss:  0.6688759515480119\n",
      "Test Acc:  58.814 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6783751706137274  Time:  1.6516058444976807\n",
      "Test Loss:  0.6670677981206349\n",
      "Test Acc:  58.209999999999994 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6785618380038407  Time:  1.6911370754241943\n",
      "Test Loss:  0.6659257886361103\n",
      "Test Acc:  57.422 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6781238513271304  Time:  1.6886119842529297\n",
      "Test Loss:  0.6675235370592195\n",
      "Test Acc:  58.062000000000005 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6782317899004386  Time:  1.7017850875854492\n",
      "Test Loss:  0.6682884261316183\n",
      "Test Acc:  57.558 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6778765651431397  Time:  1.6983191967010498\n",
      "Test Loss:  0.6689614249127251\n",
      "Test Acc:  60.653999999999996 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6762696428455576  Time:  1.6430530548095703\n",
      "Test Loss:  0.6685802741926543\n",
      "Test Acc:  56.764 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6762383612838104  Time:  1.654223918914795\n",
      "Test Loss:  0.6687082487101458\n",
      "Test Acc:  61.232 %\n",
      "================================================== 9\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=19, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=19, out_features=71, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=71, out_features=128, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=6, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=26, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=26, out_features=53, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=53, out_features=138, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=138, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6909142434597015  Time:  1.511112928390503\n",
      "Test Loss:  0.6762437732244024\n",
      "Test Acc:  55.336 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6819659915283649  Time:  1.6109619140625\n",
      "Test Loss:  0.6724629323093259\n",
      "Test Acc:  59.275999999999996 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6790119387807637  Time:  1.595839023590088\n",
      "Test Loss:  0.6722749699743427\n",
      "Test Acc:  57.052 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6797972358491299  Time:  1.6392147541046143\n",
      "Test Loss:  0.6722399665384876\n",
      "Test Acc:  60.492000000000004 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6788078509543064  Time:  1.6288161277770996\n",
      "Test Loss:  0.6694696624668277\n",
      "Test Acc:  57.69 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6782732201318671  Time:  1.6185510158538818\n",
      "Test Loss:  0.6687414773872921\n",
      "Test Acc:  57.132000000000005 %\n",
      "================================================== 9\n",
      "Train Loss:  0.677797773676197  Time:  1.6306731700897217\n",
      "Test Loss:  0.668576821380732\n",
      "Test Acc:  57.018 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6774843885515728  Time:  1.596864938735962\n",
      "Test Loss:  0.6687500166649721\n",
      "Test Acc:  59.040000000000006 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6770141170407734  Time:  1.561692237854004\n",
      "Test Loss:  0.6664432852852101\n",
      "Test Acc:  59.65200000000001 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6774378207913281  Time:  1.6015448570251465\n",
      "Test Loss:  0.6687167472377116\n",
      "Test Acc:  60.372 %\n",
      "================================================== 9\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=123, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=123, out_features=52, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=52, out_features=121, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=121, out_features=41, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=41, out_features=56, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=56, out_features=37, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=37, out_features=89, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=89, out_features=70, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=70, out_features=133, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=133, out_features=31, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=31, out_features=56, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=56, out_features=11, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=11, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6906681826514919  Time:  1.8931519985198975\n",
      "Test Loss:  0.6778195618974919\n",
      "Test Acc:  55.778000000000006 %\n",
      "================================================== 9\n",
      "Train Loss:  0.68087044923845  Time:  1.8858239650726318\n",
      "Test Loss:  0.6726299177627174\n",
      "Test Acc:  57.550000000000004 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6796278714263526  Time:  1.9377968311309814\n",
      "Test Loss:  0.6705524273672883\n",
      "Test Acc:  60.27 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6785276151486557  Time:  1.982867956161499\n",
      "Test Loss:  0.6651462100598277\n",
      "Test Acc:  59.458 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6792104307317386  Time:  1.944498062133789\n",
      "Test Loss:  0.6711188545640634\n",
      "Test Acc:  57.994 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6778768272730555  Time:  1.9645850658416748\n",
      "Test Loss:  0.6669283192984912\n",
      "Test Acc:  58.914 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6769355583364947  Time:  1.9055769443511963\n",
      "Test Loss:  0.6723658953394208\n",
      "Test Acc:  57.269999999999996 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6780522548369248  Time:  1.9593029022216797\n",
      "Test Loss:  0.6701285379881762\n",
      "Test Acc:  56.916 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6779845633210927  Time:  1.9417738914489746\n",
      "Test Loss:  0.6686505334717887\n",
      "Test Acc:  57.252 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6768494985834526  Time:  1.9647409915924072\n",
      "Test Loss:  0.6685220510983954\n",
      "Test Acc:  57.50999999999999 %\n",
      "================================================== 9\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=102, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=102, out_features=26, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=26, out_features=37, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=37, out_features=88, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=88, out_features=25, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=25, out_features=21, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=21, out_features=23, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=23, out_features=7, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=7, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.692730430051358  Time:  1.5447869300842285\n",
      "Test Loss:  0.6853537063817589\n",
      "Test Acc:  53.815999999999995 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6847576207488123  Time:  1.519516944885254\n",
      "Test Loss:  0.6722876891797903\n",
      "Test Acc:  58.158 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6792548037793514  Time:  1.5173687934875488\n",
      "Test Loss:  0.6746177141155515\n",
      "Test Acc:  58.321999999999996 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6801135150620538  Time:  1.5156002044677734\n",
      "Test Loss:  0.6716912011710965\n",
      "Test Acc:  56.379999999999995 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6781460220796348  Time:  1.5690009593963623\n",
      "Test Loss:  0.6695011513573783\n",
      "Test Acc:  61.022 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6770112494917682  Time:  1.5150823593139648\n",
      "Test Loss:  0.6649696024096742\n",
      "Test Acc:  58.384 %\n",
      "================================================== 10\n",
      "Train Loss:  0.677292102444781  Time:  1.5378952026367188\n",
      "Test Loss:  0.6680672071417983\n",
      "Test Acc:  58.720000000000006 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6769774706259261  Time:  1.5099542140960693\n",
      "Test Loss:  0.6701644759397117\n",
      "Test Acc:  57.302 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6764018746623157  Time:  1.5040650367736816\n",
      "Test Loss:  0.6657590093661327\n",
      "Test Acc:  57.76 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6764732855949959  Time:  1.5570781230926514\n",
      "Test Loss:  0.6641028307530344\n",
      "Test Acc:  60.706 %\n",
      "================================================== 10\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=112, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=112, out_features=24, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=24, out_features=52, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=52, out_features=101, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=101, out_features=63, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=63, out_features=74, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=74, out_features=112, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=80, out_features=24, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=24, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6913515459882081  Time:  1.6112339496612549\n",
      "Test Loss:  0.6775663349093223\n",
      "Test Acc:  56.104 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6823107452288161  Time:  1.6738359928131104\n",
      "Test Loss:  0.6771589502388117\n",
      "Test Acc:  56.401999999999994 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6811231999066625  Time:  1.6325600147247314\n",
      "Test Loss:  0.672759439264025\n",
      "Test Acc:  57.548 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6800079058556661  Time:  1.6813440322875977\n",
      "Test Loss:  0.6722269645150827\n",
      "Test Acc:  59.378 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6795925785155192  Time:  1.625180721282959\n",
      "Test Loss:  0.667266704902357\n",
      "Test Acc:  59.397999999999996 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6784573377919023  Time:  1.6872320175170898\n",
      "Test Loss:  0.6698612923524818\n",
      "Test Acc:  60.228 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6788125603738493  Time:  1.6617820262908936\n",
      "Test Loss:  0.6688658239281907\n",
      "Test Acc:  56.702 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6779478172751239  Time:  1.7164280414581299\n",
      "Test Loss:  0.6702292105373071\n",
      "Test Acc:  59.486000000000004 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6771419004802286  Time:  1.6876039505004883\n",
      "Test Loss:  0.6680263803929699\n",
      "Test Acc:  58.19 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6781612160432078  Time:  1.6930139064788818\n",
      "Test Loss:  0.6684494724079054\n",
      "Test Acc:  59.628 %\n",
      "================================================== 10\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=24, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=24, out_features=52, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=52, out_features=101, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=101, out_features=63, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=63, out_features=74, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=74, out_features=112, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=112, out_features=80, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=80, out_features=24, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=24, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6918177761300637  Time:  1.68534517288208\n",
      "Test Loss:  0.6748648255455251\n",
      "Test Acc:  58.004 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6821016556155073  Time:  1.652728796005249\n",
      "Test Loss:  0.6691383719444275\n",
      "Test Acc:  59.126 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6792999036120673  Time:  1.6229138374328613\n",
      "Test Loss:  0.6703690485078462\n",
      "Test Acc:  57.56 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6790594784882817  Time:  1.6787219047546387\n",
      "Test Loss:  0.6709423299346652\n",
      "Test Acc:  57.730000000000004 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6785479938461833  Time:  1.7196407318115234\n",
      "Test Loss:  0.6712833080364733\n",
      "Test Acc:  57.848 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6784636787254444  Time:  1.6840829849243164\n",
      "Test Loss:  0.6705610822049939\n",
      "Test Acc:  57.330000000000005 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6767101925219933  Time:  1.755629062652588\n",
      "Test Loss:  0.6675293901745154\n",
      "Test Acc:  58.772000000000006 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6773274490868089  Time:  1.7354719638824463\n",
      "Test Loss:  0.6705895628856153\n",
      "Test Acc:  57.19800000000001 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6771930701976275  Time:  1.7683441638946533\n",
      "Test Loss:  0.6668897499235309\n",
      "Test Acc:  59.902 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6774856874542515  Time:  1.7113149166107178\n",
      "Test Loss:  0.6668458158264354\n",
      "Test Acc:  59.014 %\n",
      "================================================== 10\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=115, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=115, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=10, out_features=93, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=93, out_features=112, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=112, out_features=134, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=134, out_features=28, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=28, out_features=94, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=94, out_features=87, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=87, out_features=77, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=77, out_features=88, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=88, out_features=42, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=42, out_features=109, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=109, out_features=35, bias=True)\n",
      "    (27): LeakyReLU(negative_slope=0.01)\n",
      "    (28): Linear(in_features=35, out_features=30, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=30, out_features=118, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=118, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933233972883572  Time:  2.292879104614258\n",
      "Test Loss:  0.6931758893995869\n",
      "Test Acc:  49.944 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6933271689136533  Time:  2.246709108352661\n",
      "Test Loss:  0.6932929757298255\n",
      "Test Acc:  50.056 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6932710622348924  Time:  2.2721331119537354\n",
      "Test Loss:  0.693330851440527\n",
      "Test Acc:  50.056 %\n",
      "================================================== 10\n",
      "Train Loss:  0.693230012907599  Time:  2.2969462871551514\n",
      "Test Loss:  0.6932289311472251\n",
      "Test Acc:  49.944 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6932870075650459  Time:  2.3655202388763428\n",
      "Test Loss:  0.6931793412991932\n",
      "Test Acc:  50.056 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6932376309902999  Time:  2.401304006576538\n",
      "Test Loss:  0.6931467418159757\n",
      "Test Acc:  49.944 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6932385853172218  Time:  2.3602371215820312\n",
      "Test Loss:  0.6932761790801067\n",
      "Test Acc:  49.944 %\n",
      "================================================== 10\n",
      "Train Loss:  0.693196724801168  Time:  2.4127800464630127\n",
      "Test Loss:  0.6932314421449389\n",
      "Test Acc:  49.944 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6931816350369557  Time:  2.408038854598999\n",
      "Test Loss:  0.6932308397122792\n",
      "Test Acc:  49.944 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6932101208363136  Time:  2.3114960193634033\n",
      "Test Loss:  0.6931487327935745\n",
      "Test Acc:  49.944 %\n",
      "================================================== 10\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=123, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=123, out_features=122, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=122, out_features=25, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=25, out_features=76, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=28, out_features=43, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=43, out_features=46, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=46, out_features=35, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=35, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6918467977186189  Time:  1.602492094039917\n",
      "Test Loss:  0.68723712466201\n",
      "Test Acc:  53.528 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6853534198590439  Time:  1.5743238925933838\n",
      "Test Loss:  0.6761548777624052\n",
      "Test Acc:  58.348 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6804938353326199  Time:  1.5960240364074707\n",
      "Test Loss:  0.6754814650939436\n",
      "Test Acc:  57.36 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6802421740806885  Time:  1.597208023071289\n",
      "Test Loss:  0.6748757584362614\n",
      "Test Acc:  57.346 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6791221210121239  Time:  1.6549949645996094\n",
      "Test Loss:  0.6766640258078672\n",
      "Test Acc:  56.314 %\n",
      "================================================== 11\n",
      "Train Loss:  0.679350894733067  Time:  1.624239206314087\n",
      "Test Loss:  0.6745205786155195\n",
      "Test Acc:  57.821999999999996 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6791053995163772  Time:  1.635193109512329\n",
      "Test Loss:  0.6733213060972641\n",
      "Test Acc:  58.989999999999995 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6787695802041214  Time:  1.6586449146270752\n",
      "Test Loss:  0.6724987750758931\n",
      "Test Acc:  58.288 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6788637481466697  Time:  1.591407060623169\n",
      "Test Loss:  0.6732993633771429\n",
      "Test Acc:  57.894 %\n",
      "================================================== 11\n",
      "Train Loss:  0.677913504360366  Time:  1.6114311218261719\n",
      "Test Loss:  0.6688753229622938\n",
      "Test Acc:  57.93000000000001 %\n",
      "================================================== 11\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=99, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=99, out_features=102, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=102, out_features=26, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=26, out_features=37, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=37, out_features=88, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=88, out_features=25, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=25, out_features=21, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=21, out_features=23, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=23, out_features=7, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=7, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6925071519221703  Time:  1.5083999633789062\n",
      "Test Loss:  0.6869932376608556\n",
      "Test Acc:  54.37 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6886994853942063  Time:  1.5542809963226318\n",
      "Test Loss:  0.6853644899567779\n",
      "Test Acc:  52.538 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6876158897024002  Time:  1.5617649555206299\n",
      "Test Loss:  0.6841519414162149\n",
      "Test Acc:  53.806 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6854943574345025  Time:  1.5611820220947266\n",
      "Test Loss:  0.6813790618765111\n",
      "Test Acc:  53.961999999999996 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6839708150738347  Time:  1.563429832458496\n",
      "Test Loss:  0.682223299632267\n",
      "Test Acc:  53.934000000000005 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6827271073839091  Time:  1.5342800617218018\n",
      "Test Loss:  0.6830023618376985\n",
      "Test Acc:  54.147999999999996 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6838812434325253  Time:  1.6166269779205322\n",
      "Test Loss:  0.6813581598048307\n",
      "Test Acc:  56.58 %\n",
      "================================================== 11\n",
      "Train Loss:  0.683086990657514  Time:  1.5229761600494385\n",
      "Test Loss:  0.6796205092449578\n",
      "Test Acc:  54.896 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6829515412776139  Time:  1.5620439052581787\n",
      "Test Loss:  0.6802119740418026\n",
      "Test Acc:  56.306 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6824571464183556  Time:  1.5269267559051514\n",
      "Test Loss:  0.6811169276432115\n",
      "Test Acc:  54.884 %\n",
      "================================================== 11\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=102, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=102, out_features=26, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=26, out_features=37, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=37, out_features=88, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=88, out_features=25, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=25, out_features=21, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=21, out_features=23, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=23, out_features=7, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=7, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6911286459351978  Time:  1.5046250820159912\n",
      "Test Loss:  0.6763957310087827\n",
      "Test Acc:  57.058 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6805288724220582  Time:  1.5411009788513184\n",
      "Test Loss:  0.6727024474922492\n",
      "Test Acc:  56.87 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6782985943077254  Time:  1.5369598865509033\n",
      "Test Loss:  0.6704430437209655\n",
      "Test Acc:  58.516 %\n",
      "================================================== 11\n",
      "Train Loss:  0.678038078503017  Time:  1.551365852355957\n",
      "Test Loss:  0.6692656485401854\n",
      "Test Acc:  58.74 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6787806342553048  Time:  1.5775389671325684\n",
      "Test Loss:  0.6675316551510169\n",
      "Test Acc:  59.684000000000005 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6778342693391508  Time:  1.4821350574493408\n",
      "Test Loss:  0.6685110929669166\n",
      "Test Acc:  58.699999999999996 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6768279127831006  Time:  1.4862949848175049\n",
      "Test Loss:  0.6698717186037375\n",
      "Test Acc:  57.642 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6769709756774623  Time:  1.5139610767364502\n",
      "Test Loss:  0.6680942399769413\n",
      "Test Acc:  58.242000000000004 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6777524389054653  Time:  1.5418941974639893\n",
      "Test Loss:  0.6667440068356845\n",
      "Test Acc:  59.660000000000004 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6770089083779467  Time:  1.570601224899292\n",
      "Test Loss:  0.6669708484289597\n",
      "Test Acc:  59.778 %\n",
      "================================================== 11\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=64, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=64, out_features=62, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=62, out_features=119, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=119, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6876449437037001  Time:  1.208582878112793\n",
      "Test Loss:  0.6718353635194351\n",
      "Test Acc:  56.934 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6803676633939256  Time:  1.2030272483825684\n",
      "Test Loss:  0.6717647350564295\n",
      "Test Acc:  59.738 %\n",
      "================================================== 11\n",
      "Train Loss:  0.678283069038043  Time:  1.1999778747558594\n",
      "Test Loss:  0.6683204535927091\n",
      "Test Acc:  56.912 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6782790559486751  Time:  1.2038729190826416\n",
      "Test Loss:  0.6676666663617504\n",
      "Test Acc:  59.938 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6782402533249263  Time:  1.1695880889892578\n",
      "Test Loss:  0.6678990937617361\n",
      "Test Acc:  58.146 %\n",
      "================================================== 11\n",
      "Train Loss:  0.677696524310286  Time:  1.1643881797790527\n",
      "Test Loss:  0.6663004366718993\n",
      "Test Acc:  59.004 %\n",
      "================================================== 11\n",
      "Train Loss:  0.677588707774225  Time:  1.14677095413208\n",
      "Test Loss:  0.6672264164199635\n",
      "Test Acc:  59.036 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6775986397788473  Time:  1.1378538608551025\n",
      "Test Loss:  0.668635909350551\n",
      "Test Acc:  59.318000000000005 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6772476610911153  Time:  1.2247579097747803\n",
      "Test Loss:  0.6652570746991099\n",
      "Test Acc:  57.943999999999996 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6765594319270475  Time:  1.2134499549865723\n",
      "Test Loss:  0.6693138528843315\n",
      "Test Acc:  58.164 %\n",
      "================================================== 11\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6904081291090833  Time:  1.7531399726867676\n",
      "Test Loss:  0.6744452149284129\n",
      "Test Acc:  58.513999999999996 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6823180802982219  Time:  1.7982311248779297\n",
      "Test Loss:  0.6705418819067429\n",
      "Test Acc:  59.29 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6798989044053712  Time:  1.8257288932800293\n",
      "Test Loss:  0.6738759571192215\n",
      "Test Acc:  56.274 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6790145483330219  Time:  1.78883695602417\n",
      "Test Loss:  0.6696900275288796\n",
      "Test Acc:  57.69800000000001 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6788814937981376  Time:  1.8179636001586914\n",
      "Test Loss:  0.6660366918967695\n",
      "Test Acc:  59.07599999999999 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6777423435319079  Time:  1.8402812480926514\n",
      "Test Loss:  0.6659127178849006\n",
      "Test Acc:  59.062000000000005 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6784307750037116  Time:  1.790565013885498\n",
      "Test Loss:  0.6659010250349434\n",
      "Test Acc:  58.852000000000004 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6770145190458228  Time:  1.8127398490905762\n",
      "Test Loss:  0.6692345656302511\n",
      "Test Acc:  58.534 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6773372264239039  Time:  1.8124029636383057\n",
      "Test Loss:  0.6673521439031679\n",
      "Test Acc:  57.023999999999994 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6766546233727114  Time:  1.8938870429992676\n",
      "Test Loss:  0.6643677822181157\n",
      "Test Acc:  61.056 %\n",
      "================================================== 12\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=58, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=58, out_features=131, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=131, out_features=102, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=102, out_features=26, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=26, out_features=37, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=37, out_features=88, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=88, out_features=25, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=25, out_features=21, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=21, out_features=23, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=23, out_features=7, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=7, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6909835332066473  Time:  1.593742847442627\n",
      "Test Loss:  0.6706294082865423\n",
      "Test Acc:  56.82000000000001 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6829240709325693  Time:  1.5958130359649658\n",
      "Test Loss:  0.6695787538679279\n",
      "Test Acc:  55.954 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6804933891679249  Time:  1.5710818767547607\n",
      "Test Loss:  0.6680607400378402\n",
      "Test Acc:  57.018 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6795157774956557  Time:  1.6137170791625977\n",
      "Test Loss:  0.6683428971743097\n",
      "Test Acc:  58.452000000000005 %\n",
      "================================================== 12\n",
      "Train Loss:  0.678487389192094  Time:  1.5776100158691406\n",
      "Test Loss:  0.6700167218033148\n",
      "Test Acc:  56.184 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6782731820632072  Time:  1.6450309753417969\n",
      "Test Loss:  0.6692441200115242\n",
      "Test Acc:  57.628 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6782843949150865  Time:  1.6081616878509521\n",
      "Test Loss:  0.6678138326625435\n",
      "Test Acc:  58.446 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6775929249116104  Time:  1.5838558673858643\n",
      "Test Loss:  0.6688801339086221\n",
      "Test Acc:  57.682 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6773872464677714  Time:  1.600647211074829\n",
      "Test Loss:  0.6682538964918682\n",
      "Test Acc:  59.756 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6761231161382076  Time:  1.6333229541778564\n",
      "Test Loss:  0.6684017126657524\n",
      "Test Acc:  59.826 %\n",
      "================================================== 12\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=102, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=102, out_features=26, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=26, out_features=37, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=37, out_features=88, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=88, out_features=25, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=25, out_features=21, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=21, out_features=23, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=23, out_features=7, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=7, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6915103606063954  Time:  1.5067219734191895\n",
      "Test Loss:  0.6769047546751645\n",
      "Test Acc:  58.394 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6828110827146655  Time:  1.509286880493164\n",
      "Test Loss:  0.6713277682357904\n",
      "Test Acc:  56.604 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6798566951803917  Time:  1.5383961200714111\n",
      "Test Loss:  0.6668120169517945\n",
      "Test Acc:  59.034 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6793173029474968  Time:  1.5275020599365234\n",
      "Test Loss:  0.6706311730097752\n",
      "Test Acc:  58.089999999999996 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6791450814174039  Time:  1.5664639472961426\n",
      "Test Loss:  0.6715596309121774\n",
      "Test Acc:  58.540000000000006 %\n",
      "================================================== 12\n",
      "Train Loss:  0.677523215539264  Time:  1.5515899658203125\n",
      "Test Loss:  0.6663184482224134\n",
      "Test Acc:  57.37 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6782054263744911  Time:  1.5193731784820557\n",
      "Test Loss:  0.6699422372847187\n",
      "Test Acc:  57.879999999999995 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6785321316144762  Time:  1.5446038246154785\n",
      "Test Loss:  0.6685609315731087\n",
      "Test Acc:  57.426 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6780458734418354  Time:  1.5269381999969482\n",
      "Test Loss:  0.6659746617078781\n",
      "Test Acc:  59.754 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6771262272866103  Time:  1.545964002609253\n",
      "Test Loss:  0.6664134574179746\n",
      "Test Acc:  57.586000000000006 %\n",
      "================================================== 12\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=127, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=127, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=53, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=53, out_features=93, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=93, out_features=132, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=132, out_features=34, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=34, out_features=82, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=82, out_features=36, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=36, out_features=52, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=52, out_features=29, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=29, out_features=106, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=106, out_features=52, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=52, out_features=73, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=73, out_features=23, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=23, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6921194054349495  Time:  2.0181150436401367\n",
      "Test Loss:  0.68541233180737\n",
      "Test Acc:  55.42 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6823610141329521  Time:  2.0833358764648438\n",
      "Test Loss:  0.6742998069646408\n",
      "Test Acc:  58.644 %\n",
      "================================================== 12\n",
      "Train Loss:  0.680825383993831  Time:  2.0543601512908936\n",
      "Test Loss:  0.6719276813828216\n",
      "Test Acc:  58.176 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6789765725605679  Time:  2.215418815612793\n",
      "Test Loss:  0.6696184900950413\n",
      "Test Acc:  58.414 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6789814175045403  Time:  2.1296629905700684\n",
      "Test Loss:  0.6686717876974417\n",
      "Test Acc:  57.221999999999994 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6787221675371602  Time:  2.1227266788482666\n",
      "Test Loss:  0.6682008799849725\n",
      "Test Acc:  61.292 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6790326411706687  Time:  2.178361654281616\n",
      "Test Loss:  0.6688595252985857\n",
      "Test Acc:  59.034 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6758640247539882  Time:  2.118216037750244\n",
      "Test Loss:  0.6670836398796159\n",
      "Test Acc:  59.906000000000006 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6780345683550313  Time:  2.160752296447754\n",
      "Test Loss:  0.6676336277504357\n",
      "Test Acc:  57.278 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6766508824198786  Time:  2.1410560607910156\n",
      "Test Loss:  0.6646559232351731\n",
      "Test Acc:  59.546 %\n",
      "================================================== 12\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=30, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=30, out_features=133, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=133, out_features=111, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=111, out_features=111, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=111, out_features=34, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=34, out_features=25, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=25, out_features=112, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=112, out_features=47, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=47, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6885048712257051  Time:  1.636674165725708\n",
      "Test Loss:  0.6767142059243455\n",
      "Test Acc:  57.654 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6804832128712731  Time:  1.6565539836883545\n",
      "Test Loss:  0.6710325278797928\n",
      "Test Acc:  60.394000000000005 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6788895700099694  Time:  1.757073163986206\n",
      "Test Loss:  0.6687381662884537\n",
      "Test Acc:  58.742000000000004 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6794539968462756  Time:  1.712982177734375\n",
      "Test Loss:  0.6695906057649729\n",
      "Test Acc:  60.660000000000004 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6788814974962359  Time:  1.6955759525299072\n",
      "Test Loss:  0.669493597380969\n",
      "Test Acc:  58.202 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6771333970292641  Time:  1.7214372158050537\n",
      "Test Loss:  0.6684007218905857\n",
      "Test Acc:  57.99999999999999 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6773198797319927  Time:  1.7241301536560059\n",
      "Test Loss:  0.6732412716563867\n",
      "Test Acc:  58.506 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6782740356713316  Time:  1.728041172027588\n",
      "Test Loss:  0.6658541420284583\n",
      "Test Acc:  59.558 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6782222553326266  Time:  1.743459939956665\n",
      "Test Loss:  0.6692501911703421\n",
      "Test Acc:  56.233999999999995 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6774335185976794  Time:  1.7124130725860596\n",
      "Test Loss:  0.6658426024475876\n",
      "Test Acc:  58.658 %\n",
      "================================================== 13\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=29, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=29, out_features=77, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=77, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6920659877087948  Time:  1.7804028987884521\n",
      "Test Loss:  0.6882265462559096\n",
      "Test Acc:  53.604 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6863319741113343  Time:  1.7702360153198242\n",
      "Test Loss:  0.6798146066008782\n",
      "Test Acc:  55.732000000000006 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6821285529728354  Time:  1.740725040435791\n",
      "Test Loss:  0.6749853777642153\n",
      "Test Acc:  58.584 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6799827657476829  Time:  1.7683253288269043\n",
      "Test Loss:  0.6746002377904191\n",
      "Test Acc:  56.492 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6805888155516047  Time:  1.8236322402954102\n",
      "Test Loss:  0.6735365591487106\n",
      "Test Acc:  56.986000000000004 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6796082175561111  Time:  1.8073220252990723\n",
      "Test Loss:  0.6723002487299393\n",
      "Test Acc:  58.03 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6784605027115258  Time:  1.769413948059082\n",
      "Test Loss:  0.6670788958364603\n",
      "Test Acc:  58.367999999999995 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6785896316061925  Time:  1.8130018711090088\n",
      "Test Loss:  0.671273445292395\n",
      "Test Acc:  56.076 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6794371613620842  Time:  1.8059017658233643\n",
      "Test Loss:  0.6713717753181652\n",
      "Test Acc:  56.53 %\n",
      "================================================== 13\n",
      "Train Loss:  0.677720286550313  Time:  1.851511001586914\n",
      "Test Loss:  0.6691585870421662\n",
      "Test Acc:  58.245999999999995 %\n",
      "================================================== 13\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6912067634780912  Time:  1.7591371536254883\n",
      "Test Loss:  0.6853761837190512\n",
      "Test Acc:  54.87 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6828603309436436  Time:  1.7300159931182861\n",
      "Test Loss:  0.6736105744327817\n",
      "Test Acc:  55.054 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6806378884472116  Time:  1.8096270561218262\n",
      "Test Loss:  0.6703648986865063\n",
      "Test Acc:  56.396 %\n",
      "================================================== 13\n",
      "Train Loss:  0.680374805509609  Time:  1.803581953048706\n",
      "Test Loss:  0.6711139894869863\n",
      "Test Acc:  56.147999999999996 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6797302369653744  Time:  1.7987661361694336\n",
      "Test Loss:  0.6701512467496249\n",
      "Test Acc:  56.848 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6785828044814785  Time:  1.8399829864501953\n",
      "Test Loss:  0.6677731175203713\n",
      "Test Acc:  60.99400000000001 %\n",
      "================================================== 13\n",
      "Train Loss:  0.678686514823106  Time:  1.801948070526123\n",
      "Test Loss:  0.6701944807962495\n",
      "Test Acc:  58.160000000000004 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6783755367254689  Time:  1.8430719375610352\n",
      "Test Loss:  0.6698326784737256\n",
      "Test Acc:  57.472 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6771782691026256  Time:  1.8052406311035156\n",
      "Test Loss:  0.6690944670414438\n",
      "Test Acc:  58.684000000000005 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6779377956459992  Time:  1.8678340911865234\n",
      "Test Loss:  0.6672709003394964\n",
      "Test Acc:  59.416000000000004 %\n",
      "================================================== 13\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=7, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=7, out_features=136, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=136, out_features=80, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=80, out_features=133, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=133, out_features=32, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=32, out_features=107, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=107, out_features=31, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=31, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=129, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=129, out_features=31, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=31, out_features=113, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=113, out_features=71, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=71, out_features=16, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=16, out_features=107, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=107, out_features=98, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=98, out_features=84, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=84, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932760946942071  Time:  2.3995490074157715\n",
      "Test Loss:  0.6933580451473897\n",
      "Test Acc:  49.944 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6931875261077045  Time:  2.415501117706299\n",
      "Test Loss:  0.6931664204719116\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6932086885845574  Time:  2.44944167137146\n",
      "Test Loss:  0.6931680325342684\n",
      "Test Acc:  49.944 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6932194426547001  Time:  2.412220001220703\n",
      "Test Loss:  0.6932198602934273\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6931627525465331  Time:  2.466660261154175\n",
      "Test Loss:  0.6931789593428982\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "Train Loss:  0.693226484051586  Time:  2.4127681255340576\n",
      "Test Loss:  0.6931479910806734\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6932195592535674  Time:  2.4191060066223145\n",
      "Test Loss:  0.6931730399326402\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6932050413893958  Time:  2.4731431007385254\n",
      "Test Loss:  0.6932396855281324\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6932342226487876  Time:  2.469310998916626\n",
      "Test Loss:  0.6931652940657674\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6931967791849679  Time:  2.5318150520324707\n",
      "Test Loss:  0.6931480099352039\n",
      "Test Acc:  50.056 %\n",
      "================================================== 13\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=57, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=57, out_features=91, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=91, out_features=129, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=129, out_features=116, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=116, out_features=47, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=47, out_features=58, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=58, out_features=86, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=86, out_features=29, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=29, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6921671570217522  Time:  1.734853744506836\n",
      "Test Loss:  0.6837776890214609\n",
      "Test Acc:  54.442 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6829682146980814  Time:  1.6513957977294922\n",
      "Test Loss:  0.6721468674285072\n",
      "Test Acc:  56.157999999999994 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6796999647669548  Time:  1.681074857711792\n",
      "Test Loss:  0.6699169564003847\n",
      "Test Acc:  59.47 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6789841021064424  Time:  1.7434790134429932\n",
      "Test Loss:  0.6703812568163385\n",
      "Test Acc:  57.568 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6789252575296555  Time:  1.743652105331421\n",
      "Test Loss:  0.669900043886535\n",
      "Test Acc:  57.562000000000005 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6791330354057089  Time:  1.8209428787231445\n",
      "Test Loss:  0.6672110511940352\n",
      "Test Acc:  58.47599999999999 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6779528538676074  Time:  1.8218629360198975\n",
      "Test Loss:  0.6651995838904867\n",
      "Test Acc:  60.248000000000005 %\n",
      "================================================== 14\n",
      "Train Loss:  0.678509016541669  Time:  1.823577880859375\n",
      "Test Loss:  0.6666376900916197\n",
      "Test Acc:  57.30800000000001 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6769325100157383  Time:  1.8302721977233887\n",
      "Test Loss:  0.6684052345095849\n",
      "Test Acc:  57.918000000000006 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6774746510234192  Time:  1.7634398937225342\n",
      "Test Loss:  0.6678066770641171\n",
      "Test Acc:  58.742000000000004 %\n",
      "================================================== 14\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=61, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=61, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6904440324236877  Time:  1.7329168319702148\n",
      "Test Loss:  0.6813247617410154\n",
      "Test Acc:  56.794 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6817686860143704  Time:  1.743379831314087\n",
      "Test Loss:  0.6746867615349439\n",
      "Test Acc:  55.730000000000004 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6794116199016571  Time:  1.8128631114959717\n",
      "Test Loss:  0.6756590878476902\n",
      "Test Acc:  58.52 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6787193149980837  Time:  1.9057998657226562\n",
      "Test Loss:  0.6731467584566194\n",
      "Test Acc:  58.653999999999996 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6788651995415235  Time:  1.9272289276123047\n",
      "Test Loss:  0.670437148639134\n",
      "Test Acc:  56.472 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6787230102685247  Time:  1.9383811950683594\n",
      "Test Loss:  0.6698024269877648\n",
      "Test Acc:  56.733999999999995 %\n",
      "================================================== 14\n",
      "Train Loss:  0.679263033135964  Time:  1.947627067565918\n",
      "Test Loss:  0.6688932551413166\n",
      "Test Acc:  60.821999999999996 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6787771787086543  Time:  1.8938889503479004\n",
      "Test Loss:  0.6689519876120041\n",
      "Test Acc:  56.614 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6773616695491067  Time:  1.9172089099884033\n",
      "Test Loss:  0.6672372605119433\n",
      "Test Acc:  59.412 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6780800714980076  Time:  1.85862398147583\n",
      "Test Loss:  0.6683113730075408\n",
      "Test Acc:  58.216 %\n",
      "================================================== 14\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6918717224667542  Time:  1.7420368194580078\n",
      "Test Loss:  0.6842571141160264\n",
      "Test Acc:  54.44 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6831465128564487  Time:  1.7522132396697998\n",
      "Test Loss:  0.6723986879295233\n",
      "Test Acc:  56.226 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6805898969190834  Time:  1.804671049118042\n",
      "Test Loss:  0.6685257888570124\n",
      "Test Acc:  61.05199999999999 %\n",
      "================================================== 14\n",
      "Train Loss:  0.679123983113435  Time:  1.8472859859466553\n",
      "Test Loss:  0.6703769366960136\n",
      "Test Acc:  56.422000000000004 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6793995828089052  Time:  1.8561348915100098\n",
      "Test Loss:  0.6670611722736942\n",
      "Test Acc:  59.036 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6780856086389861  Time:  1.8043179512023926\n",
      "Test Loss:  0.665490843203603\n",
      "Test Acc:  57.79600000000001 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6782352265215268  Time:  1.7870590686798096\n",
      "Test Loss:  0.6709967340741839\n",
      "Test Acc:  57.65 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6774325588323774  Time:  1.7659778594970703\n",
      "Test Loss:  0.6684105733827669\n",
      "Test Acc:  59.436 %\n",
      "================================================== 14\n",
      "Train Loss:  0.678418792512295  Time:  1.793874979019165\n",
      "Test Loss:  0.6665024489772563\n",
      "Test Acc:  57.879999999999995 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6777814037173334  Time:  1.8161420822143555\n",
      "Test Loss:  0.6691893062421254\n",
      "Test Acc:  60.224 %\n",
      "================================================== 14\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=130, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=130, out_features=23, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=23, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6919732433165947  Time:  0.9997382164001465\n",
      "Test Loss:  0.6769787714797624\n",
      "Test Acc:  59.372 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6862277451657901  Time:  1.0694198608398438\n",
      "Test Loss:  0.6726820541887867\n",
      "Test Acc:  59.089999999999996 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6845679800875866  Time:  1.0126338005065918\n",
      "Test Loss:  0.6710310481032546\n",
      "Test Acc:  60.682 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6836557314343696  Time:  1.0247199535369873\n",
      "Test Loss:  0.672020465743785\n",
      "Test Acc:  59.746 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6834841986207196  Time:  1.0302457809448242\n",
      "Test Loss:  0.67099237806943\n",
      "Test Acc:  59.668 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6826864313470186  Time:  0.9859068393707275\n",
      "Test Loss:  0.6701777431429649\n",
      "Test Acc:  59.07 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6802890239840876  Time:  0.9832332134246826\n",
      "Test Loss:  0.6711447801516981\n",
      "Test Acc:  59.455999999999996 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6802874897083226  Time:  1.0036468505859375\n",
      "Test Loss:  0.6686193468619366\n",
      "Test Acc:  59.709999999999994 %\n",
      "================================================== 14\n",
      "Train Loss:  0.6804480815890932  Time:  1.0151121616363525\n",
      "Test Loss:  0.66777495674941\n",
      "Test Acc:  59.592 %\n",
      "================================================== 14\n",
      "Train Loss:  0.678131951250299  Time:  1.012293815612793\n",
      "Test Loss:  0.6708553132353997\n",
      "Test Acc:  58.93000000000001 %\n",
      "================================================== 14\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=82, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=82, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=79, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=79, out_features=135, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=135, out_features=54, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=54, out_features=5, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=5, out_features=23, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=23, out_features=57, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=57, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6929718336484728  Time:  1.7684547901153564\n",
      "Test Loss:  0.6856505572795868\n",
      "Test Acc:  54.922000000000004 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6856616846401326  Time:  1.6438312530517578\n",
      "Test Loss:  0.6748467838885833\n",
      "Test Acc:  59.316 %\n",
      "================================================== 15\n",
      "Train Loss:  0.679922275952179  Time:  1.7006618976593018\n",
      "Test Loss:  0.6712228488557193\n",
      "Test Acc:  58.05200000000001 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6792133085048981  Time:  1.6862342357635498\n",
      "Test Loss:  0.6673767283862951\n",
      "Test Acc:  58.152 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6778522695068026  Time:  1.6405267715454102\n",
      "Test Loss:  0.6694187211747072\n",
      "Test Acc:  57.056 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6781097217632907  Time:  1.6928482055664062\n",
      "Test Loss:  0.6680470890536601\n",
      "Test Acc:  60.324 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6780338050240148  Time:  1.6831810474395752\n",
      "Test Loss:  0.6669500123481361\n",
      "Test Acc:  61.284000000000006 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6762724956021692  Time:  1.689483880996704\n",
      "Test Loss:  0.6678844349724906\n",
      "Test Acc:  59.855999999999995 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6779647112762841  Time:  1.726370096206665\n",
      "Test Loss:  0.6669187892456444\n",
      "Test Acc:  56.39999999999999 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6771832730648292  Time:  1927.2689971923828\n",
      "Test Loss:  0.6661989570272212\n",
      "Test Acc:  58.86 %\n",
      "================================================== 15\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=117, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=117, out_features=13, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=13, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.689940738199401  Time:  2.3244640827178955\n",
      "Test Loss:  0.6789801634696065\n",
      "Test Acc:  55.879999999999995 %\n",
      "================================================== 15\n",
      "Train Loss:  0.682789586538816  Time:  2.032588005065918\n",
      "Test Loss:  0.6741886485596092\n",
      "Test Acc:  57.684000000000005 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6807673743171413  Time:  3.2203409671783447\n",
      "Test Loss:  0.6723263406631897\n",
      "Test Acc:  57.292 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6787873356446733  Time:  2.0304808616638184\n",
      "Test Loss:  0.6694692285085211\n",
      "Test Acc:  57.778 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6794536929496013  Time:  3.612488269805908\n",
      "Test Loss:  0.6665731735375463\n",
      "Test Acc:  58.343999999999994 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6783757481696832  Time:  2.0113611221313477\n",
      "Test Loss:  0.6699291437256093\n",
      "Test Acc:  60.696000000000005 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6783787527658643  Time:  2.034018039703369\n",
      "Test Loss:  0.6685216657969416\n",
      "Test Acc:  59.006 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6770878366745301  Time:  1.936479091644287\n",
      "Test Loss:  0.6671302613555169\n",
      "Test Acc:  57.438 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6769866888975575  Time:  1.8162598609924316\n",
      "Test Loss:  0.6654173376000657\n",
      "Test Acc:  59.943999999999996 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6772077821031974  Time:  1.7958910465240479\n",
      "Test Loss:  0.6646080099198283\n",
      "Test Acc:  60.068 %\n",
      "================================================== 15\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6907370203602923  Time:  1.686269998550415\n",
      "Test Loss:  0.6745267358361459\n",
      "Test Acc:  58.934 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6813095555688343  Time:  1.6810920238494873\n",
      "Test Loss:  0.670617722735113\n",
      "Test Acc:  56.68 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6793979269309636  Time:  1.7027289867401123\n",
      "Test Loss:  0.6701453835988531\n",
      "Test Acc:  58.562000000000005 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6788439622325618  Time:  1.7552549839019775\n",
      "Test Loss:  0.6688750960997173\n",
      "Test Acc:  58.472 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6787254172955116  Time:  1.7714850902557373\n",
      "Test Loss:  0.6673720171865152\n",
      "Test Acc:  60.618 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6775762047210749  Time:  1.7540109157562256\n",
      "Test Loss:  0.6692856778295673\n",
      "Test Acc:  58.926 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6778967306561714  Time:  1.8268442153930664\n",
      "Test Loss:  0.6661626708750822\n",
      "Test Acc:  58.728 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6767229853320296  Time:  1.79954195022583\n",
      "Test Loss:  0.6668522327530141\n",
      "Test Acc:  57.114 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6771978574947719  Time:  1.8164021968841553\n",
      "Test Loss:  0.6680509423723027\n",
      "Test Acc:  59.040000000000006 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6772156864622213  Time:  2.36146879196167\n",
      "Test Loss:  0.6678193467003959\n",
      "Test Acc:  60.952 %\n",
      "================================================== 15\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=63, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=63, out_features=129, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=129, out_features=107, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=107, out_features=73, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=73, out_features=12, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=12, out_features=107, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=107, out_features=21, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=21, out_features=14, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=14, out_features=98, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=98, out_features=122, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=122, out_features=124, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=124, out_features=103, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=103, out_features=109, bias=True)\n",
      "    (25): LeakyReLU(negative_slope=0.01)\n",
      "    (26): Linear(in_features=109, out_features=27, bias=True)\n",
      "    (27): LeakyReLU(negative_slope=0.01)\n",
      "    (28): Linear(in_features=27, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6921283239430754  Time:  2.3711929321289062\n",
      "Test Loss:  0.6851992153999756\n",
      "Test Acc:  52.278000000000006 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6864216105781332  Time:  2.9545419216156006\n",
      "Test Loss:  0.6812862577487011\n",
      "Test Acc:  54.645999999999994 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6814237086877336  Time:  2.2767391204833984\n",
      "Test Loss:  0.6721057444810867\n",
      "Test Acc:  57.672000000000004 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6802141407545466  Time:  2.7737178802490234\n",
      "Test Loss:  0.669732288742552\n",
      "Test Acc:  58.716 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6785837833898781  Time:  2.6192617416381836\n",
      "Test Loss:  0.6712120169279526\n",
      "Test Acc:  59.132 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6782750548237432  Time:  2.8219399452209473\n",
      "Test Loss:  0.6700485768366833\n",
      "Test Acc:  60.260000000000005 %\n",
      "================================================== 15\n",
      "Train Loss:  0.678914975945967  Time:  2.2295219898223877\n",
      "Test Loss:  0.668705650434202\n",
      "Test Acc:  58.312 %\n",
      "================================================== 15\n",
      "Train Loss:  0.679018486590281  Time:  2.2676939964294434\n",
      "Test Loss:  0.6664051559506631\n",
      "Test Acc:  58.164 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6783971257888488  Time:  2.2920751571655273\n",
      "Test Loss:  0.6677702330813116\n",
      "Test Acc:  60.19799999999999 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6775489125373589  Time:  2.2285876274108887\n",
      "Test Loss:  0.6676732706172126\n",
      "Test Acc:  60.062000000000005 %\n",
      "================================================== 15\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=135, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=135, out_features=67, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=67, out_features=75, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=75, out_features=40, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=40, out_features=91, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=91, out_features=138, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=138, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=92, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=92, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6903532056042748  Time:  1.7865169048309326\n",
      "Test Loss:  0.6758390771491187\n",
      "Test Acc:  56.989999999999995 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6816578816323384  Time:  1.758944034576416\n",
      "Test Loss:  0.6719455904498393\n",
      "Test Acc:  59.974000000000004 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6790488144777117  Time:  1.7465178966522217\n",
      "Test Loss:  0.6703483501867372\n",
      "Test Acc:  59.870000000000005 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6781230273037931  Time:  2.267320156097412\n",
      "Test Loss:  0.6700941467163514\n",
      "Test Acc:  56.172 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6785170414151936  Time:  2.0782997608184814\n",
      "Test Loss:  0.6683448358457915\n",
      "Test Acc:  57.635999999999996 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6780967938638952  Time:  1.8880655765533447\n",
      "Test Loss:  0.6664618010423622\n",
      "Test Acc:  58.558 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6773790258125667  Time:  2.005624771118164\n",
      "Test Loss:  0.6711146062125966\n",
      "Test Acc:  57.879999999999995 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6777095057233407  Time:  1.8570947647094727\n",
      "Test Loss:  0.66598450924669\n",
      "Test Acc:  59.048 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6764559669651254  Time:  2.0492560863494873\n",
      "Test Loss:  0.664480481828962\n",
      "Test Acc:  61.12800000000001 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6771605726141129  Time:  1.8696839809417725\n",
      "Test Loss:  0.6689579781829095\n",
      "Test Acc:  57.730000000000004 %\n",
      "================================================== 16\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=21, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=21, out_features=52, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=52, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6928298129652538  Time:  1.886702299118042\n",
      "Test Loss:  0.6885240436816702\n",
      "Test Acc:  53.757999999999996 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6867012048724794  Time:  1.8225781917572021\n",
      "Test Loss:  0.6820338602576937\n",
      "Test Acc:  55.588 %\n",
      "================================================== 16\n",
      "Train Loss:  0.68263352635133  Time:  1.9558789730072021\n",
      "Test Loss:  0.6738660007107015\n",
      "Test Acc:  57.79600000000001 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6813670725282961  Time:  2.0298960208892822\n",
      "Test Loss:  0.6716979422739574\n",
      "Test Acc:  57.936 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6812674853488476  Time:  1.8969972133636475\n",
      "Test Loss:  0.6755526360808587\n",
      "Test Acc:  56.25 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6787184298473553  Time:  1.8892431259155273\n",
      "Test Loss:  0.6701412550648864\n",
      "Test Acc:  59.001999999999995 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6800306485082112  Time:  1.9513640403747559\n",
      "Test Loss:  0.6715499439409801\n",
      "Test Acc:  58.268 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6797544292724915  Time:  1.9560089111328125\n",
      "Test Loss:  0.6732174884908053\n",
      "Test Acc:  59.18600000000001 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6783716352316584  Time:  1.938767671585083\n",
      "Test Loss:  0.6719797855737258\n",
      "Test Acc:  57.614 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6785707636906283  Time:  2.0177578926086426\n",
      "Test Loss:  0.6703193871950617\n",
      "Test Acc:  57.138 %\n",
      "================================================== 16\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=87, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=87, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=110, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=110, out_features=43, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=43, out_features=84, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=84, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=101, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=101, out_features=67, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=67, out_features=108, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6913968666191519  Time:  1.8661739826202393\n",
      "Test Loss:  0.6755440344615858\n",
      "Test Acc:  55.772 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6810938111186897  Time:  1.861098051071167\n",
      "Test Loss:  0.6716351813199569\n",
      "Test Acc:  58.292 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6798761471779677  Time:  1.8916559219360352\n",
      "Test Loss:  0.6718182223183768\n",
      "Test Acc:  55.944 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6786572254487198  Time:  1.9692459106445312\n",
      "Test Loss:  0.6693973602080832\n",
      "Test Acc:  56.85 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6786311447185321  Time:  1.8852510452270508\n",
      "Test Loss:  0.670162703917951\n",
      "Test Acc:  57.276 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6782094668732942  Time:  1.835486888885498\n",
      "Test Loss:  0.6663281114733949\n",
      "Test Acc:  57.774 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6787726285248777  Time:  1.800482988357544\n",
      "Test Loss:  0.666582351132315\n",
      "Test Acc:  59.40800000000001 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6770355586152877  Time:  1.987718105316162\n",
      "Test Loss:  0.6674339117444291\n",
      "Test Acc:  57.208000000000006 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6777693840273975  Time:  1.8001048564910889\n",
      "Test Loss:  0.6674508452415466\n",
      "Test Acc:  58.926 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6769415442525906  Time:  1.8531830310821533\n",
      "Test Loss:  0.6677150410048815\n",
      "Test Acc:  57.804 %\n",
      "================================================== 16\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=60, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=60, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=91, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=91, out_features=31, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=31, out_features=90, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=90, out_features=38, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=38, out_features=6, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=6, out_features=95, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=95, out_features=47, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=47, out_features=130, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=4, out_features=76, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=76, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6910296145581851  Time:  1.7958381175994873\n",
      "Test Loss:  0.6803157068028742\n",
      "Test Acc:  56.047999999999995 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6819171611845058  Time:  1.9257631301879883\n",
      "Test Loss:  0.6761304191788848\n",
      "Test Acc:  57.386 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6809825346852741  Time:  1.9377212524414062\n",
      "Test Loss:  0.6747596355117097\n",
      "Test Acc:  59.534 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6804145117745782  Time:  1.8669848442077637\n",
      "Test Loss:  0.6733346937262282\n",
      "Test Acc:  56.202 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6788827520217339  Time:  1.9248580932617188\n",
      "Test Loss:  0.6682993362144548\n",
      "Test Acc:  58.565999999999995 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6783337321159614  Time:  1.8802330493927002\n",
      "Test Loss:  0.6671754349859393\n",
      "Test Acc:  60.660000000000004 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6785248470132368  Time:  2.367396116256714\n",
      "Test Loss:  0.6707947704256797\n",
      "Test Acc:  56.74400000000001 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6785068651185419  Time:  2.3236751556396484\n",
      "Test Loss:  0.6668456792831421\n",
      "Test Acc:  60.73 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6785352428029053  Time:  2.114617109298706\n",
      "Test Loss:  0.6695725753599283\n",
      "Test Acc:  57.248 %\n",
      "================================================== 16\n",
      "Train Loss:  0.676538882464388  Time:  1.9840960502624512\n",
      "Test Loss:  0.6681074384523897\n",
      "Test Acc:  60.419999999999995 %\n",
      "================================================== 16\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=60, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=60, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=91, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=91, out_features=22, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=22, out_features=14, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=14, out_features=39, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=39, out_features=12, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=12, out_features=122, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=122, out_features=35, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=35, out_features=132, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=132, out_features=40, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=40, out_features=53, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=53, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932675666617651  Time:  1.976161003112793\n",
      "Test Loss:  0.6925630855317019\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6849397699328235  Time:  2.0924110412597656\n",
      "Test Loss:  0.6747215222947451\n",
      "Test Acc:  55.51199999999999 %\n",
      "================================================== 17\n",
      "Train Loss:  0.678753257232861  Time:  1.9818768501281738\n",
      "Test Loss:  0.6684289088054579\n",
      "Test Acc:  58.702 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6796305797395915  Time:  1.9613690376281738\n",
      "Test Loss:  0.6693116490330014\n",
      "Test Acc:  60.548 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6783159061940047  Time:  1.9174509048461914\n",
      "Test Loss:  0.6697837406275223\n",
      "Test Acc:  58.720000000000006 %\n",
      "================================================== 17\n",
      "Train Loss:  0.678069084665201  Time:  1.8712620735168457\n",
      "Test Loss:  0.6668776860650705\n",
      "Test Acc:  60.343999999999994 %\n",
      "================================================== 17\n",
      "Train Loss:  0.677488967667531  Time:  1.914618968963623\n",
      "Test Loss:  0.6675949768752468\n",
      "Test Acc:  57.48799999999999 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6774885922017759  Time:  2.026287078857422\n",
      "Test Loss:  0.6708272683377169\n",
      "Test Acc:  56.948 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6769425594893685  Time:  1.8898351192474365\n",
      "Test Loss:  0.6663015080349786\n",
      "Test Acc:  59.28 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6770723729673094  Time:  1.9079651832580566\n",
      "Test Loss:  0.6666352986073008\n",
      "Test Acc:  59.498 %\n",
      "================================================== 17\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=31, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=31, out_features=90, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=90, out_features=38, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=38, out_features=6, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=6, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=47, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=47, out_features=130, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=4, out_features=76, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=76, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933560195195414  Time:  2.138862133026123\n",
      "Test Loss:  0.6916277004139764\n",
      "Test Acc:  49.944 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6858165812318342  Time:  2.1903581619262695\n",
      "Test Loss:  0.6756720783150926\n",
      "Test Acc:  56.338 %\n",
      "================================================== 17\n",
      "Train Loss:  0.68051582183281  Time:  2.1088850498199463\n",
      "Test Loss:  0.6741423749802063\n",
      "Test Acc:  59.57599999999999 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6804558830539675  Time:  2.091101884841919\n",
      "Test Loss:  0.6715724115469017\n",
      "Test Acc:  58.602 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6796959423235733  Time:  2.0298449993133545\n",
      "Test Loss:  0.6743814251860794\n",
      "Test Acc:  57.664 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6798281782735003  Time:  2.1589901447296143\n",
      "Test Loss:  0.6693930026828027\n",
      "Test Acc:  58.147999999999996 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6786797895048656  Time:  2.0636298656463623\n",
      "Test Loss:  0.6679952026021724\n",
      "Test Acc:  58.118 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6788664662489926  Time:  2.1558659076690674\n",
      "Test Loss:  0.6668737649309392\n",
      "Test Acc:  59.29 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6778250710807577  Time:  2.001392126083374\n",
      "Test Loss:  0.6662755237550152\n",
      "Test Acc:  60.024 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6763056592784659  Time:  2.189594030380249\n",
      "Test Loss:  0.6677175410548035\n",
      "Test Acc:  60.24 %\n",
      "================================================== 17\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=60, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=60, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=91, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=91, out_features=31, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=31, out_features=90, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=90, out_features=38, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=38, out_features=6, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=6, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=47, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=47, out_features=130, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=4, out_features=76, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=76, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.69323429595815  Time:  2.159883975982666\n",
      "Test Loss:  0.6932690237857857\n",
      "Test Acc:  49.944 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6933011982562768  Time:  2.068457841873169\n",
      "Test Loss:  0.6931472098340794\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6932502121385866  Time:  2.0174999237060547\n",
      "Test Loss:  0.6931612865657223\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6932580249152914  Time:  2.058004856109619\n",
      "Test Loss:  0.6932008318146881\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6932065615253727  Time:  2.0296311378479004\n",
      "Test Loss:  0.6931684643638377\n",
      "Test Acc:  49.944 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6932796137611361  Time:  2.057692289352417\n",
      "Test Loss:  0.6941152430918752\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6933541719930886  Time:  2.134842872619629\n",
      "Test Loss:  0.6931480844410098\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6932922223188581  Time:  2.059314012527466\n",
      "Test Loss:  0.693164869534726\n",
      "Test Acc:  49.944 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6932641565799713  Time:  2.050238847732544\n",
      "Test Loss:  0.6933165524078875\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6932133103374147  Time:  2.0500142574310303\n",
      "Test Loss:  0.6944062983503148\n",
      "Test Acc:  50.056 %\n",
      "================================================== 17\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=113, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=113, out_features=73, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=73, out_features=40, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=40, out_features=61, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=61, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6891891464699794  Time:  1.3226752281188965\n",
      "Test Loss:  0.673094986348736\n",
      "Test Acc:  58.97 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6819043179063031  Time:  1.296917200088501\n",
      "Test Loss:  0.6681938366014131\n",
      "Test Acc:  58.443999999999996 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6787731390799919  Time:  1.3337101936340332\n",
      "Test Loss:  0.6684126142336397\n",
      "Test Acc:  58.592 %\n",
      "================================================== 17\n",
      "Train Loss:  0.677711489861899  Time:  1.3202428817749023\n",
      "Test Loss:  0.6704318702829127\n",
      "Test Acc:  58.07 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6783859040180262  Time:  1.3815629482269287\n",
      "Test Loss:  0.6664612968357242\n",
      "Test Acc:  58.298 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6773942785106436  Time:  1.3816709518432617\n",
      "Test Loss:  0.6686005437252472\n",
      "Test Acc:  57.786 %\n",
      "================================================== 17\n",
      "Train Loss:  0.677222381543069  Time:  1.3394660949707031\n",
      "Test Loss:  0.6689197360252848\n",
      "Test Acc:  58.47 %\n",
      "================================================== 17\n",
      "Train Loss:  0.675248432333452  Time:  1.3126442432403564\n",
      "Test Loss:  0.6672040850532298\n",
      "Test Acc:  59.028000000000006 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6759074671860159  Time:  1.3479187488555908\n",
      "Test Loss:  0.668253201611188\n",
      "Test Acc:  57.204 %\n",
      "================================================== 17\n",
      "Train Loss:  0.676745361655298  Time:  1.2437491416931152\n",
      "Test Loss:  0.6652459137293757\n",
      "Test Acc:  60.050000000000004 %\n",
      "================================================== 17\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=125, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=125, out_features=111, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=111, out_features=131, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=131, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=8, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=8, out_features=108, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=108, out_features=7, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=7, out_features=88, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=88, out_features=96, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=96, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933917823064066  Time:  2.154536247253418\n",
      "Test Loss:  0.6931949741378123\n",
      "Test Acc:  49.944 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6913287874555936  Time:  2.792757034301758\n",
      "Test Loss:  0.681153059613948\n",
      "Test Acc:  55.242000000000004 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6818112907183431  Time:  2.3986051082611084\n",
      "Test Loss:  0.6750604327844114\n",
      "Test Acc:  57.550000000000004 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6796282882237956  Time:  2.357138156890869\n",
      "Test Loss:  0.670664266055944\n",
      "Test Acc:  56.769999999999996 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6794014520888781  Time:  2.4872140884399414\n",
      "Test Loss:  0.6710961789500957\n",
      "Test Acc:  57.854000000000006 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6786528891020448  Time:  2.455793857574463\n",
      "Test Loss:  0.6716417357021448\n",
      "Test Acc:  57.69 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6788549477601573  Time:  2.3380582332611084\n",
      "Test Loss:  0.6691959792253922\n",
      "Test Acc:  58.106 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6787616806308718  Time:  2.543318033218384\n",
      "Test Loss:  0.669605729835374\n",
      "Test Acc:  56.943999999999996 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6790236047584645  Time:  2.510693073272705\n",
      "Test Loss:  0.6730908368315015\n",
      "Test Acc:  57.18600000000001 %\n",
      "================================================== 18\n",
      "Train Loss:  0.677789197786011  Time:  2.478855848312378\n",
      "Test Loss:  0.6681442646955957\n",
      "Test Acc:  57.702 %\n",
      "================================================== 18\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=43, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=43, out_features=110, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=110, out_features=21, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=21, out_features=31, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=31, out_features=90, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=90, out_features=38, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=38, out_features=6, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=6, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=47, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=47, out_features=130, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=4, out_features=76, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=76, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6934280515152172  Time:  2.025627851486206\n",
      "Test Loss:  0.6934577117160875\n",
      "Test Acc:  49.944 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6932724474990455  Time:  1.9497151374816895\n",
      "Test Loss:  0.6934362516111258\n",
      "Test Acc:  50.056 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6933140332681419  Time:  1.9653711318969727\n",
      "Test Loss:  0.6931616429771695\n",
      "Test Acc:  50.056 %\n",
      "================================================== 18\n",
      "Train Loss:  0.693355163736065  Time:  1.9651758670806885\n",
      "Test Loss:  0.6931532776477386\n",
      "Test Acc:  49.944 %\n",
      "================================================== 18\n",
      "Train Loss:  0.69320227673454  Time:  1.8962218761444092\n",
      "Test Loss:  0.6938589607574501\n",
      "Test Acc:  50.056 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6932192277299226  Time:  1.893312931060791\n",
      "Test Loss:  0.693932871733393\n",
      "Test Acc:  50.056 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6933341498357536  Time:  1.891329050064087\n",
      "Test Loss:  0.6933848952152291\n",
      "Test Acc:  49.944 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6932697698582698  Time:  1.9104456901550293\n",
      "Test Loss:  0.6932048174191494\n",
      "Test Acc:  50.056 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6932850299525435  Time:  1.9007010459899902\n",
      "Test Loss:  0.6931462446037604\n",
      "Test Acc:  50.056 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6932135270024739  Time:  1.960296392440796\n",
      "Test Loss:  0.6931533293456448\n",
      "Test Acc:  49.944 %\n",
      "================================================== 18\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=31, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=31, out_features=90, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=90, out_features=38, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=38, out_features=6, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=6, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=47, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=47, out_features=130, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=4, out_features=76, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=76, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6931631713017931  Time:  1.9314019680023193\n",
      "Test Loss:  0.6866456388818974\n",
      "Test Acc:  55.013999999999996 %\n",
      "================================================== 18\n",
      "Train Loss:  0.686133838700552  Time:  2.0135390758514404\n",
      "Test Loss:  0.6768030247517994\n",
      "Test Acc:  58.37 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6833944966758254  Time:  1.999506950378418\n",
      "Test Loss:  0.6797461774276228\n",
      "Test Acc:  55.592 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6808066228880499  Time:  1.9383490085601807\n",
      "Test Loss:  0.6692694656702937\n",
      "Test Acc:  58.162000000000006 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6799593522165813  Time:  2.1266558170318604\n",
      "Test Loss:  0.6730418345149682\n",
      "Test Acc:  60.214 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6797197706943011  Time:  2.1481690406799316\n",
      "Test Loss:  0.6691541632219237\n",
      "Test Acc:  58.830000000000005 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6784855666821891  Time:  2.0294852256774902\n",
      "Test Loss:  0.6681972933667046\n",
      "Test Acc:  59.472 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6787716976917573  Time:  2.0094690322875977\n",
      "Test Loss:  0.6678966761243587\n",
      "Test Acc:  59.556 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6796194077843297  Time:  2.133603096008301\n",
      "Test Loss:  0.6679686356563957\n",
      "Test Acc:  59.714 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6788287667462426  Time:  2.026978015899658\n",
      "Test Loss:  0.6680563338557068\n",
      "Test Acc:  58.922 %\n",
      "================================================== 18\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=47, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=47, out_features=16, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=16, out_features=4, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4, out_features=132, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=132, out_features=75, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=75, out_features=140, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=140, out_features=34, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=34, out_features=73, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=73, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932102002366616  Time:  1.6204769611358643\n",
      "Test Loss:  0.6925934845087479\n",
      "Test Acc:  51.656 %\n",
      "================================================== 18\n",
      "Train Loss:  0.687818749542654  Time:  1.5919160842895508\n",
      "Test Loss:  0.6766310531874092\n",
      "Test Acc:  58.108000000000004 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6793743591674053  Time:  1.614103078842163\n",
      "Test Loss:  0.6709270781400253\n",
      "Test Acc:  57.872 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6793296595994573  Time:  1.6253798007965088\n",
      "Test Loss:  0.6725405697919884\n",
      "Test Acc:  56.291999999999994 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6788352695259735  Time:  1.6481451988220215\n",
      "Test Loss:  0.6688587200276706\n",
      "Test Acc:  60.354 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6777741761103163  Time:  1.6336970329284668\n",
      "Test Loss:  0.6699206935507911\n",
      "Test Acc:  57.86599999999999 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6783344428034594  Time:  1.7105119228363037\n",
      "Test Loss:  0.6707344949245453\n",
      "Test Acc:  57.8 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6770251793582944  Time:  1.6831700801849365\n",
      "Test Loss:  0.669185437414111\n",
      "Test Acc:  57.384 %\n",
      "================================================== 18\n",
      "Train Loss:  0.678091259333339  Time:  1.7092571258544922\n",
      "Test Loss:  0.6700830286254689\n",
      "Test Acc:  57.264 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6774438199770711  Time:  1.690627098083496\n",
      "Test Loss:  0.6664818300276386\n",
      "Test Acc:  58.684000000000005 %\n",
      "================================================== 18\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=109, out_features=111, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6926560691238319  Time:  2.2904210090637207\n",
      "Test Loss:  0.6825060336565485\n",
      "Test Acc:  57.06400000000001 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6837738084532049  Time:  2.2618441581726074\n",
      "Test Loss:  0.6739140469200757\n",
      "Test Acc:  56.554 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6805458229823704  Time:  2.3036670684814453\n",
      "Test Loss:  0.6713898644155386\n",
      "Test Acc:  60.924 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6800278890742003  Time:  2.379925012588501\n",
      "Test Loss:  0.6691371360603644\n",
      "Test Acc:  57.054 %\n",
      "================================================== 19\n",
      "Train Loss:  0.679420511217883  Time:  2.555811882019043\n",
      "Test Loss:  0.6708452467407499\n",
      "Test Acc:  56.611999999999995 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6776128526586686  Time:  2.546081781387329\n",
      "Test Loss:  0.669798916699935\n",
      "Test Acc:  57.742000000000004 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6783434996639726  Time:  2.5283961296081543\n",
      "Test Loss:  0.668501079082489\n",
      "Test Acc:  59.852000000000004 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6781786475738469  Time:  2.4917309284210205\n",
      "Test Loss:  0.6665694114505029\n",
      "Test Acc:  57.821999999999996 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6772263163197649  Time:  2.855557918548584\n",
      "Test Loss:  0.6667122035002222\n",
      "Test Acc:  60.007999999999996 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6769047148906402  Time:  2.543086051940918\n",
      "Test Loss:  0.6697377440880756\n",
      "Test Acc:  58.452000000000005 %\n",
      "================================================== 19\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=70, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=70, out_features=101, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=101, out_features=31, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=31, out_features=90, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=90, out_features=38, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=38, out_features=6, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=6, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=47, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=47, out_features=130, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=4, out_features=76, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=76, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6934018485302472  Time:  2.0580430030822754\n",
      "Test Loss:  0.6931535933090716\n",
      "Test Acc:  50.056 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6932314749616776  Time:  2.0735840797424316\n",
      "Test Loss:  0.6932384748848117\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6933705328154738  Time:  2.1510751247406006\n",
      "Test Loss:  0.6933685863504604\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6932820236160807  Time:  2.1594910621643066\n",
      "Test Loss:  0.6931558932576861\n",
      "Test Acc:  50.056 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6933043131428044  Time:  2.0228209495544434\n",
      "Test Loss:  0.6932478449782546\n",
      "Test Acc:  50.056 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6932940472216502  Time:  2.0821518898010254\n",
      "Test Loss:  0.6933524845814218\n",
      "Test Acc:  50.056 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6933046459716602  Time:  2.0246469974517822\n",
      "Test Loss:  0.6932148313035771\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Train Loss:  0.693368621116137  Time:  2.034512996673584\n",
      "Test Loss:  0.6933863232938611\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Train Loss:  0.693353684931776  Time:  2.095583915710449\n",
      "Test Loss:  0.6931594841334284\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6933277869311563  Time:  2.23209285736084\n",
      "Test Loss:  0.693813225140377\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=31, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=31, out_features=90, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=90, out_features=38, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=38, out_features=6, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=6, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=47, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=47, out_features=130, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=4, out_features=76, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=76, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6925045430225177  Time:  2.2649338245391846\n",
      "Test Loss:  0.6856919171250596\n",
      "Test Acc:  53.452 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6842585913021199  Time:  2.0310888290405273\n",
      "Test Loss:  0.6762777515211884\n",
      "Test Acc:  59.160000000000004 %\n",
      "================================================== 19\n",
      "Train Loss:  0.680861278171957  Time:  2.0254712104797363\n",
      "Test Loss:  0.6749373130044158\n",
      "Test Acc:  57.242000000000004 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6786972412662785  Time:  2.1244571208953857\n",
      "Test Loss:  0.668622590753497\n",
      "Test Acc:  60.28 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6798679143407919  Time:  2.040727138519287\n",
      "Test Loss:  0.669957482084936\n",
      "Test Acc:  58.745999999999995 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6788725176431837  Time:  2.0422210693359375\n",
      "Test Loss:  0.6686855940794458\n",
      "Test Acc:  57.082 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6782525066476669  Time:  1.9731037616729736\n",
      "Test Loss:  0.6705738400318184\n",
      "Test Acc:  58.496 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6790504657874142  Time:  1.914517879486084\n",
      "Test Loss:  0.6647554167679378\n",
      "Test Acc:  61.988 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6788322993003539  Time:  1.915616750717163\n",
      "Test Loss:  0.6653274148702621\n",
      "Test Acc:  60.96 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6772972584640893  Time:  1.8611557483673096\n",
      "Test Loss:  0.6685907810318227\n",
      "Test Acc:  57.424 %\n",
      "================================================== 19\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=124, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=124, out_features=11, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=11, out_features=104, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=104, out_features=118, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=118, out_features=23, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=23, out_features=134, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=134, out_features=104, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=104, out_features=119, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=119, out_features=26, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=26, out_features=25, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=25, out_features=81, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=81, out_features=95, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=95, out_features=128, bias=True)\n",
      "    (25): LeakyReLU(negative_slope=0.01)\n",
      "    (26): Linear(in_features=128, out_features=12, bias=True)\n",
      "    (27): LeakyReLU(negative_slope=0.01)\n",
      "    (28): Linear(in_features=12, out_features=137, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=137, out_features=44, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=44, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933194773040549  Time:  2.3331170082092285\n",
      "Test Loss:  0.6931911521420187\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6933225961062159  Time:  2.356501579284668\n",
      "Test Loss:  0.6931469641169723\n",
      "Test Acc:  49.944 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6910735677628621  Time:  2.373694896697998\n",
      "Test Loss:  0.6762659376373097\n",
      "Test Acc:  58.812 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6850711244301204  Time:  2.368741989135742\n",
      "Test Loss:  0.6768830743979435\n",
      "Test Acc:  56.922 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6811365524347681  Time:  2.3929247856140137\n",
      "Test Loss:  0.6743544677690584\n",
      "Test Acc:  58.996 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6793307829512297  Time:  2.408787250518799\n",
      "Test Loss:  0.6713999108392366\n",
      "Test Acc:  60.49 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6784061954839387  Time:  2.4094200134277344\n",
      "Test Loss:  0.6678547500347605\n",
      "Test Acc:  58.568 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6776995702381552  Time:  2.417609214782715\n",
      "Test Loss:  0.6684816540503988\n",
      "Test Acc:  57.220000000000006 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6791698119066057  Time:  2.427428960800171\n",
      "Test Loss:  0.6680462488106319\n",
      "Test Acc:  58.56400000000001 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6783650310805244  Time:  2.540034055709839\n",
      "Test Loss:  0.6655264551542244\n",
      "Test Acc:  57.586000000000006 %\n",
      "================================================== 19\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=81, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=81, out_features=95, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=95, out_features=31, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=31, out_features=26, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=26, out_features=33, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=33, out_features=33, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=33, out_features=5, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=5, out_features=133, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=133, out_features=89, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6914417099778669  Time:  1.971513032913208\n",
      "Test Loss:  0.6836855995411776\n",
      "Test Acc:  54.442 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6828076667594214  Time:  1.9645438194274902\n",
      "Test Loss:  0.6774551302802806\n",
      "Test Acc:  57.328 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6809863591716238  Time:  1.9991726875305176\n",
      "Test Loss:  0.6720481134799062\n",
      "Test Acc:  56.324 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6801286368039403  Time:  1.9821341037750244\n",
      "Test Loss:  0.66859954778029\n",
      "Test Acc:  57.852000000000004 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6800072991499936  Time:  1.9955830574035645\n",
      "Test Loss:  0.6704607174104574\n",
      "Test Acc:  59.364 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6791393898264335  Time:  1.9536280632019043\n",
      "Test Loss:  0.6700013599225453\n",
      "Test Acc:  58.398 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6792464869735885  Time:  1.9579601287841797\n",
      "Test Loss:  0.6714285058634621\n",
      "Test Acc:  58.089999999999996 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6784099549272634  Time:  1.9687418937683105\n",
      "Test Loss:  0.6696515937848967\n",
      "Test Acc:  58.797999999999995 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6779764871092608  Time:  2.003779172897339\n",
      "Test Loss:  0.6717336615737604\n",
      "Test Acc:  58.948 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6780106571033924  Time:  2.0016109943389893\n",
      "Test Loss:  0.6712742797574218\n",
      "Test Acc:  57.098000000000006 %\n",
      "================================================== 20\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=67, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=67, out_features=89, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=89, out_features=73, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=73, out_features=111, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.693358050645703  Time:  2.1298837661743164\n",
      "Test Loss:  0.6931555976673048\n",
      "Test Acc:  49.944 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6931521257344824  Time:  2.1464061737060547\n",
      "Test Loss:  0.6936174634159827\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932430791593817  Time:  2.1191487312316895\n",
      "Test Loss:  0.6931838928436747\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6931516006045098  Time:  2.1621992588043213\n",
      "Test Loss:  0.6933111484561648\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932271331766225  Time:  2.261077880859375\n",
      "Test Loss:  0.6931766399315425\n",
      "Test Acc:  49.944 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932155481220161  Time:  2.235225200653076\n",
      "Test Loss:  0.6931354789709558\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6931300902888723  Time:  2.2714009284973145\n",
      "Test Loss:  0.6932473359059315\n",
      "Test Acc:  49.944 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6913068372837818  Time:  2.266104221343994\n",
      "Test Loss:  0.6793213492753555\n",
      "Test Acc:  56.696000000000005 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6812989372406563  Time:  2.3115389347076416\n",
      "Test Loss:  0.6733349181559621\n",
      "Test Acc:  56.748 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6804508147013448  Time:  2.3575479984283447\n",
      "Test Loss:  0.6701622374203741\n",
      "Test Acc:  56.176 %\n",
      "================================================== 20\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=111, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932336374790999  Time:  2.1380841732025146\n",
      "Test Loss:  0.6934377602776702\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6894534928520231  Time:  2.1146552562713623\n",
      "Test Loss:  0.6811617320897628\n",
      "Test Acc:  59.06400000000001 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6825735312308708  Time:  2.1556808948516846\n",
      "Test Loss:  0.6738282870881411\n",
      "Test Acc:  56.754000000000005 %\n",
      "================================================== 20\n",
      "Train Loss:  0.680581944267245  Time:  2.2044150829315186\n",
      "Test Loss:  0.6713251152208873\n",
      "Test Acc:  56.120000000000005 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6789586663681225  Time:  2.2295072078704834\n",
      "Test Loss:  0.670792390193258\n",
      "Test Acc:  57.08 %\n",
      "================================================== 20\n",
      "Train Loss:  0.679476804106775  Time:  2.2649829387664795\n",
      "Test Loss:  0.6717099285855586\n",
      "Test Acc:  58.096000000000004 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6785889072139768  Time:  2.2819321155548096\n",
      "Test Loss:  0.6699373433176352\n",
      "Test Acc:  57.568 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6776323881897613  Time:  2.2672410011291504\n",
      "Test Loss:  0.6687749085985885\n",
      "Test Acc:  60.278 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6786212048826427  Time:  2.2466158866882324\n",
      "Test Loss:  0.6701552651974619\n",
      "Test Acc:  57.47 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6771028437753663  Time:  2.1875293254852295\n",
      "Test Loss:  0.6691279958705513\n",
      "Test Acc:  59.330000000000005 %\n",
      "================================================== 20\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=15, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=15, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=5, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=5, out_features=123, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=123, out_features=115, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=115, out_features=14, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=14, out_features=46, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=46, out_features=75, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=75, out_features=48, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=48, out_features=62, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=62, out_features=33, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=33, out_features=59, bias=True)\n",
      "    (25): LeakyReLU(negative_slope=0.01)\n",
      "    (26): Linear(in_features=59, out_features=7, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=7, out_features=60, bias=True)\n",
      "    (29): LeakyReLU(negative_slope=0.01)\n",
      "    (30): Linear(in_features=60, out_features=55, bias=True)\n",
      "    (31): LeakyReLU(negative_slope=0.01)\n",
      "    (32): Linear(in_features=55, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932947966304138  Time:  2.0804121494293213\n",
      "Test Loss:  0.6931477572236743\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932085291312559  Time:  2.098262071609497\n",
      "Test Loss:  0.6937644858749545\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932916167008616  Time:  2.121795892715454\n",
      "Test Loss:  0.6933277884916383\n",
      "Test Acc:  49.944 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932022016848961  Time:  2.1112558841705322\n",
      "Test Loss:  0.6931532289908857\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932188052765644  Time:  2.1579840183258057\n",
      "Test Loss:  0.6932596007780153\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932362561678365  Time:  2.1585261821746826\n",
      "Test Loss:  0.6931877276118921\n",
      "Test Acc:  49.944 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932342246066044  Time:  2.1434550285339355\n",
      "Test Loss:  0.6932136887798503\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932104673698871  Time:  2.164320945739746\n",
      "Test Loss:  0.6931855617737284\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6931899572810988  Time:  2.125314235687256\n",
      "Test Loss:  0.6932539772622439\n",
      "Test Acc:  50.056 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6932135013333203  Time:  2.1380388736724854\n",
      "Test Loss:  0.6931489055254021\n",
      "Test Acc:  49.944 %\n",
      "================================================== 20\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=81, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=81, out_features=41, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=41, out_features=40, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=40, out_features=21, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=21, out_features=108, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=108, out_features=123, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=123, out_features=81, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=81, out_features=23, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=23, out_features=94, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=94, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6916681856569582  Time:  1.8816440105438232\n",
      "Test Loss:  0.6798923994813647\n",
      "Test Acc:  56.120000000000005 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6829042995933199  Time:  1.904412031173706\n",
      "Test Loss:  0.6778189786234681\n",
      "Test Acc:  55.336 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6807742216726289  Time:  1.9647910594940186\n",
      "Test Loss:  0.6709421648054706\n",
      "Test Acc:  58.168 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6799888330219436  Time:  1.9908239841461182\n",
      "Test Loss:  0.6704971693000015\n",
      "Test Acc:  58.668 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6788536897540962  Time:  1.976822853088379\n",
      "Test Loss:  0.6701407879590988\n",
      "Test Acc:  57.986000000000004 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6784866125914302  Time:  1.994070053100586\n",
      "Test Loss:  0.670572065577215\n",
      "Test Acc:  58.69799999999999 %\n",
      "================================================== 21\n",
      "Train Loss:  0.677746517814859  Time:  2.0131349563598633\n",
      "Test Loss:  0.6721204412834985\n",
      "Test Acc:  61.046 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6777181564456355  Time:  1.9543960094451904\n",
      "Test Loss:  0.6675233907845556\n",
      "Test Acc:  57.638 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6771462209468341  Time:  1.958461046218872\n",
      "Test Loss:  0.6667551179321445\n",
      "Test Acc:  58.568 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6779120673228355  Time:  1.9601380825042725\n",
      "Test Loss:  0.6699121637003762\n",
      "Test Acc:  56.699999999999996 %\n",
      "================================================== 21\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=88, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=88, out_features=138, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=138, out_features=84, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=84, out_features=111, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932579629177594  Time:  2.0998289585113525\n",
      "Test Loss:  0.6931457242795399\n",
      "Test Acc:  50.056 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6896386042128514  Time:  2.102442979812622\n",
      "Test Loss:  0.6754704917571983\n",
      "Test Acc:  58.656 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6819865146692652  Time:  2.1301701068878174\n",
      "Test Loss:  0.6677061708606019\n",
      "Test Acc:  56.901999999999994 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6796393592427247  Time:  2.3103220462799072\n",
      "Test Loss:  0.6698611518558191\n",
      "Test Acc:  58.59799999999999 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6793068771379708  Time:  2.3396108150482178\n",
      "Test Loss:  0.6691019474243631\n",
      "Test Acc:  57.52 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6783286050288346  Time:  2.290461778640747\n",
      "Test Loss:  0.6690491690319411\n",
      "Test Acc:  58.804 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6780121624469757  Time:  2.284759998321533\n",
      "Test Loss:  0.6666496861345914\n",
      "Test Acc:  58.864000000000004 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6766234573221555  Time:  2.2410359382629395\n",
      "Test Loss:  0.6679332295850832\n",
      "Test Acc:  57.176 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6772420904497161  Time:  2.241652011871338\n",
      "Test Loss:  0.6681495889717218\n",
      "Test Acc:  59.91 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6773713004850123  Time:  2.3174569606781006\n",
      "Test Loss:  0.6679680307908934\n",
      "Test Acc:  57.908 %\n",
      "================================================== 21\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=54, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=54, out_features=87, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=87, out_features=109, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=109, out_features=111, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6919676626685762  Time:  2.105764865875244\n",
      "Test Loss:  0.681575097599808\n",
      "Test Acc:  55.19199999999999 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6826068239055411  Time:  2.112097978591919\n",
      "Test Loss:  0.6738345437512105\n",
      "Test Acc:  55.989999999999995 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6809941638995262  Time:  2.1478981971740723\n",
      "Test Loss:  0.6726137037788119\n",
      "Test Acc:  56.501999999999995 %\n",
      "================================================== 21\n",
      "Train Loss:  0.679949084990216  Time:  2.236785888671875\n",
      "Test Loss:  0.670840536453286\n",
      "Test Acc:  59.343999999999994 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6792318960175897  Time:  2.1689722537994385\n",
      "Test Loss:  0.6717361379034665\n",
      "Test Acc:  57.24399999999999 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6793848360977034  Time:  2.2149739265441895\n",
      "Test Loss:  0.6721380876035107\n",
      "Test Acc:  59.550000000000004 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6788603095677648  Time:  2.2506208419799805\n",
      "Test Loss:  0.676033651038092\n",
      "Test Acc:  57.379999999999995 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6785277811280133  Time:  2.2574241161346436\n",
      "Test Loss:  0.6734301371842014\n",
      "Test Acc:  57.358 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6778048507488557  Time:  2.3005170822143555\n",
      "Test Loss:  0.67074959466652\n",
      "Test Acc:  57.867999999999995 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6760943636406948  Time:  2.2467498779296875\n",
      "Test Loss:  0.670095367395148\n",
      "Test Acc:  57.656 %\n",
      "================================================== 21\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=24, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=24, out_features=76, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=76, out_features=130, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=130, out_features=32, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=32, out_features=78, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=78, out_features=75, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=75, out_features=61, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=61, out_features=62, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=62, out_features=73, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=73, out_features=66, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=66, out_features=68, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=68, out_features=131, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=131, out_features=30, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=30, out_features=63, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=63, out_features=85, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=85, out_features=24, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=24, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6936678477447399  Time:  2.1981592178344727\n",
      "Test Loss:  0.6931590912293415\n",
      "Test Acc:  49.944 %\n",
      "================================================== 21\n",
      "Train Loss:  0.693204048123673  Time:  2.1763999462127686\n",
      "Test Loss:  0.6931206012258724\n",
      "Test Acc:  50.056 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6913541994825767  Time:  2.2740981578826904\n",
      "Test Loss:  0.6853450561056331\n",
      "Test Acc:  54.022000000000006 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6886389409973673  Time:  2.2262141704559326\n",
      "Test Loss:  0.6823466030918822\n",
      "Test Acc:  55.422000000000004 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6866485700555092  Time:  2.330789089202881\n",
      "Test Loss:  0.6819060858415098\n",
      "Test Acc:  54.62200000000001 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6833666919791785  Time:  2.2864019870758057\n",
      "Test Loss:  0.6745169360418709\n",
      "Test Acc:  55.784 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6809903570335277  Time:  2.3025588989257812\n",
      "Test Loss:  0.6760425768336471\n",
      "Test Acc:  55.794 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6815220585269649  Time:  2.345284938812256\n",
      "Test Loss:  0.6725049614906311\n",
      "Test Acc:  56.854000000000006 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6800699253586957  Time:  2.384324073791504\n",
      "Test Loss:  0.6707782182766466\n",
      "Test Acc:  59.094 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6786145161103158  Time:  2.3855416774749756\n",
      "Test Loss:  0.6719095326808034\n",
      "Test Acc:  57.172 %\n",
      "================================================== 21\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=88, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=88, out_features=138, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=138, out_features=84, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=84, out_features=119, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=119, out_features=97, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=97, out_features=85, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=85, out_features=71, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=71, out_features=131, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=131, out_features=76, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=76, out_features=136, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=136, out_features=32, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=32, out_features=73, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=73, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6929802859786653  Time:  2.1394827365875244\n",
      "Test Loss:  0.6857207977041906\n",
      "Test Acc:  53.944 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6849517798336753  Time:  2.0633950233459473\n",
      "Test Loss:  0.6746785756276579\n",
      "Test Acc:  56.044000000000004 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6807129106817454  Time:  2.098337173461914\n",
      "Test Loss:  0.6726602601761721\n",
      "Test Acc:  57.074000000000005 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6795905528277376  Time:  2.2181291580200195\n",
      "Test Loss:  0.6686656818706163\n",
      "Test Acc:  58.426 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6791826134180501  Time:  2.239604949951172\n",
      "Test Loss:  0.6709794946471039\n",
      "Test Acc:  58.348 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6788774959362336  Time:  2.2690303325653076\n",
      "Test Loss:  0.667562513327112\n",
      "Test Acc:  57.03 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6794254392603017  Time:  2.307943344116211\n",
      "Test Loss:  0.6701102147296983\n",
      "Test Acc:  59.01 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6790678470674223  Time:  2.262821674346924\n",
      "Test Loss:  0.670568620367926\n",
      "Test Acc:  58.752 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6789969162349283  Time:  2.260725975036621\n",
      "Test Loss:  0.670044121997697\n",
      "Test Acc:  59.61 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6771885702644822  Time:  2.2591569423675537\n",
      "Test Loss:  0.6675575320817986\n",
      "Test Acc:  57.46 %\n",
      "================================================== 22\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=112, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=112, out_features=21, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=21, out_features=122, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=122, out_features=111, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932010626705893  Time:  2.0331380367279053\n",
      "Test Loss:  0.6885568426579846\n",
      "Test Acc:  49.944 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6904186227895918  Time:  2.0536811351776123\n",
      "Test Loss:  0.6717676079394866\n",
      "Test Acc:  59.050000000000004 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6820530212708633  Time:  2.08107590675354\n",
      "Test Loss:  0.6685992369846422\n",
      "Test Acc:  59.922 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6801781032207238  Time:  2.1534337997436523\n",
      "Test Loss:  0.6702877210111035\n",
      "Test Acc:  57.38799999999999 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6788866778794866  Time:  2.2001070976257324\n",
      "Test Loss:  0.6722326819994011\n",
      "Test Acc:  57.424 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6772340638359098  Time:  2.1913979053497314\n",
      "Test Loss:  0.6708464756303903\n",
      "Test Acc:  60.672000000000004 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6779348882880524  Time:  2.210211992263794\n",
      "Test Loss:  0.6707243651759868\n",
      "Test Acc:  58.47 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6769878035479219  Time:  2.2864980697631836\n",
      "Test Loss:  0.6691668441100996\n",
      "Test Acc:  59.232 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6782253356310572  Time:  2.268526077270508\n",
      "Test Loss:  0.6679442737783704\n",
      "Test Acc:  58.414 %\n",
      "================================================== 22\n",
      "Train Loss:  0.676877295231297  Time:  2.168501138687134\n",
      "Test Loss:  0.6665271037087148\n",
      "Test Acc:  60.092 %\n",
      "================================================== 22\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=88, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=88, out_features=138, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=138, out_features=84, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=84, out_features=111, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6922402244849797  Time:  2.0490241050720215\n",
      "Test Loss:  0.6836095047848565\n",
      "Test Acc:  55.547999999999995 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6837718915330232  Time:  2.0699429512023926\n",
      "Test Loss:  0.6742981146184766\n",
      "Test Acc:  56.568 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6806891595360136  Time:  2.1159069538116455\n",
      "Test Loss:  0.6703377417763885\n",
      "Test Acc:  58.47 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6788645839169077  Time:  2.4086902141571045\n",
      "Test Loss:  0.6702178874794318\n",
      "Test Acc:  56.906 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6783130160213386  Time:  2.3505699634552\n",
      "Test Loss:  0.6697320229544932\n",
      "Test Acc:  59.144 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6794170667655277  Time:  2.351879835128784\n",
      "Test Loss:  0.6675256329531573\n",
      "Test Acc:  57.736 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6789844658252967  Time:  2.2980589866638184\n",
      "Test Loss:  0.671063514996548\n",
      "Test Acc:  58.06400000000001 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6783204916184836  Time:  2.306067943572998\n",
      "Test Loss:  0.6680433713659948\n",
      "Test Acc:  59.721999999999994 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6783210948435929  Time:  2.2553670406341553\n",
      "Test Loss:  0.6705581567117146\n",
      "Test Acc:  60.648 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6771133639951692  Time:  2.307889938354492\n",
      "Test Loss:  0.667052728485088\n",
      "Test Acc:  57.95 %\n",
      "================================================== 22\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=105, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=105, out_features=56, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=56, out_features=118, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=118, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6914147288694868  Time:  1.1390070915222168\n",
      "Test Loss:  0.6818545454618882\n",
      "Test Acc:  56.145999999999994 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6857402007075122  Time:  1.1398646831512451\n",
      "Test Loss:  0.672645841021927\n",
      "Test Acc:  58.465999999999994 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6804871902848683  Time:  1.1967191696166992\n",
      "Test Loss:  0.6715939197005057\n",
      "Test Acc:  58.47599999999999 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6808660696022701  Time:  1.1808619499206543\n",
      "Test Loss:  0.6725445441445526\n",
      "Test Acc:  56.711999999999996 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6813744587184739  Time:  1.179133415222168\n",
      "Test Loss:  0.6701797128331904\n",
      "Test Acc:  58.302 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6791713644553276  Time:  1.186513900756836\n",
      "Test Loss:  0.6695481006588254\n",
      "Test Acc:  58.408 %\n",
      "================================================== 22\n",
      "Train Loss:  0.679487940386264  Time:  1.1391611099243164\n",
      "Test Loss:  0.6718494405551833\n",
      "Test Acc:  58.092 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6786881955000605  Time:  1.142477035522461\n",
      "Test Loss:  0.6697787955707434\n",
      "Test Acc:  58.89 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6792842023564081  Time:  1.138828992843628\n",
      "Test Loss:  0.6694028052140255\n",
      "Test Acc:  58.465999999999994 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6786249088544916  Time:  1.154430866241455\n",
      "Test Loss:  0.6703098848157999\n",
      "Test Acc:  58.15 %\n",
      "================================================== 22\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=112, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=112, out_features=21, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=21, out_features=122, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=122, out_features=78, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=78, out_features=136, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=136, out_features=48, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=48, out_features=120, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=120, out_features=47, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=47, out_features=134, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=134, out_features=19, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=19, out_features=21, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=21, out_features=41, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=41, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6916444251572129  Time:  1.8880078792572021\n",
      "Test Loss:  0.6895676893847329\n",
      "Test Acc:  51.124 %\n",
      "================================================== 23\n",
      "Train Loss:  0.686179104077555  Time:  1.9049551486968994\n",
      "Test Loss:  0.6701722291051125\n",
      "Test Acc:  60.104 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6801732924297779  Time:  1.8918890953063965\n",
      "Test Loss:  0.6691382354011342\n",
      "Test Acc:  60.01 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6781341411771565  Time:  1.9403810501098633\n",
      "Test Loss:  0.6659277139269576\n",
      "Test Acc:  59.248 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6787576001055919  Time:  1.910799264907837\n",
      "Test Loss:  0.6678559244895468\n",
      "Test Acc:  58.06400000000001 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6782971622735045  Time:  1.8963518142700195\n",
      "Test Loss:  0.6701402104630763\n",
      "Test Acc:  57.208000000000006 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6792198041059675  Time:  1.9150891304016113\n",
      "Test Loss:  0.6669660198445223\n",
      "Test Acc:  59.611999999999995 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6776506430911322  Time:  1.9342718124389648\n",
      "Test Loss:  0.6684837362595967\n",
      "Test Acc:  60.370000000000005 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6772671609464354  Time:  1.9489259719848633\n",
      "Test Loss:  0.6755037176973966\n",
      "Test Acc:  58.553999999999995 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6765514114042268  Time:  1.9820139408111572\n",
      "Test Loss:  0.6656595727010649\n",
      "Test Acc:  57.962 %\n",
      "================================================== 23\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=12, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=12, out_features=113, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=113, out_features=88, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=88, out_features=111, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6925525121445203  Time:  2.028759002685547\n",
      "Test Loss:  0.6881197757866918\n",
      "Test Acc:  52.734 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6883354006457503  Time:  2.0143320560455322\n",
      "Test Loss:  0.6869911676158711\n",
      "Test Acc:  53.474 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6861711077011414  Time:  2.1113321781158447\n",
      "Test Loss:  0.6841562977250741\n",
      "Test Acc:  53.961999999999996 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6848103673788752  Time:  2.1602349281311035\n",
      "Test Loss:  0.6824661198319221\n",
      "Test Acc:  54.466 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6847460807239922  Time:  2.1336429119110107\n",
      "Test Loss:  0.6801815197175863\n",
      "Test Acc:  55.013999999999996 %\n",
      "================================================== 23\n",
      "Train Loss:  0.683010495927212  Time:  2.17777419090271\n",
      "Test Loss:  0.6803415989389225\n",
      "Test Acc:  55.174 %\n",
      "================================================== 23\n",
      "Train Loss:  0.683044988526045  Time:  2.201998233795166\n",
      "Test Loss:  0.6803360082665268\n",
      "Test Acc:  54.43599999999999 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6820122473431329  Time:  2.152704954147339\n",
      "Test Loss:  0.6780883593826877\n",
      "Test Acc:  55.442 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6815674163564278  Time:  2.181105852127075\n",
      "Test Loss:  0.676331033816143\n",
      "Test Acc:  56.072 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6818631371877489  Time:  2.188171863555908\n",
      "Test Loss:  0.677513097013746\n",
      "Test Acc:  56.202 %\n",
      "================================================== 23\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=112, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=112, out_features=21, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=21, out_features=122, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=122, out_features=111, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=111, out_features=17, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=17, out_features=132, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=132, out_features=78, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=78, out_features=112, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=112, out_features=41, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=41, out_features=137, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=137, out_features=126, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=126, out_features=115, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=115, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6914123392453159  Time:  2.0312068462371826\n",
      "Test Loss:  0.6800882329746168\n",
      "Test Acc:  55.035999999999994 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6803340944495514  Time:  2.0462517738342285\n",
      "Test Loss:  0.6676401906475729\n",
      "Test Acc:  58.550000000000004 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6793956421587589  Time:  2.085162878036499\n",
      "Test Loss:  0.6698994980174668\n",
      "Test Acc:  60.106 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6784487701680538  Time:  2.2180771827697754\n",
      "Test Loss:  0.6668025693114923\n",
      "Test Acc:  60.982 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6788928373886721  Time:  2.1699938774108887\n",
      "Test Loss:  0.6682647089568936\n",
      "Test Acc:  57.728 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6777254704141269  Time:  2.140565872192383\n",
      "Test Loss:  0.6676478860329609\n",
      "Test Acc:  58.550000000000004 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6771833824850347  Time:  2.1760857105255127\n",
      "Test Loss:  0.6679600078840645\n",
      "Test Acc:  59.687999999999995 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6775471696453373  Time:  2.1727449893951416\n",
      "Test Loss:  0.6680347140954466\n",
      "Test Acc:  57.528 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6770117783198392  Time:  2.183995246887207\n",
      "Test Loss:  0.6684740678388246\n",
      "Test Acc:  57.402 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6776342254920598  Time:  2.2072980403900146\n",
      "Test Loss:  0.668298993183642\n",
      "Test Acc:  58.34 %\n",
      "================================================== 23\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=15, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=15, out_features=137, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=137, out_features=48, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=48, out_features=68, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=68, out_features=11, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=11, out_features=46, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=46, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6909151183862756  Time:  1.4649488925933838\n",
      "Test Loss:  0.676835083231634\n",
      "Test Acc:  58.986000000000004 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6823135211955021  Time:  1.4293839931488037\n",
      "Test Loss:  0.6713178759934951\n",
      "Test Acc:  60.354 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6795894873403284  Time:  1.3973772525787354\n",
      "Test Loss:  0.6694604228345715\n",
      "Test Acc:  56.645999999999994 %\n",
      "================================================== 23\n",
      "Train Loss:  0.678762652578145  Time:  1.3964660167694092\n",
      "Test Loss:  0.6712828181227859\n",
      "Test Acc:  57.065999999999995 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6783096011537705  Time:  1.416905164718628\n",
      "Test Loss:  0.6680709874751617\n",
      "Test Acc:  59.602 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6782417656296361  Time:  1.4534478187561035\n",
      "Test Loss:  0.6696797122760695\n",
      "Test Acc:  59.438 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6773892377849913  Time:  1.460402011871338\n",
      "Test Loss:  0.6666158103213018\n",
      "Test Acc:  57.276 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6767266260011353  Time:  1.4498670101165771\n",
      "Test Loss:  0.66646337722029\n",
      "Test Acc:  58.522 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6772206251638649  Time:  1.4611761569976807\n",
      "Test Loss:  0.6660262163804502\n",
      "Test Acc:  59.61 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6764653690540008  Time:  1.4785218238830566\n",
      "Test Loss:  0.6668693155658488\n",
      "Test Acc:  58.384 %\n",
      "================================================== 23\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=21, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=21, out_features=112, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=112, out_features=69, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=69, out_features=96, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=96, out_features=34, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=34, out_features=83, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=83, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6884937854143824  Time:  1.4567210674285889\n",
      "Test Loss:  0.6764546255676114\n",
      "Test Acc:  55.732000000000006 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6811565343480911  Time:  1.4445991516113281\n",
      "Test Loss:  0.6760141283881907\n",
      "Test Acc:  60.888 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6793684491710942  Time:  1.4434590339660645\n",
      "Test Loss:  0.6720375640659916\n",
      "Test Acc:  57.708000000000006 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6787316751306074  Time:  1.453221082687378\n",
      "Test Loss:  0.6702490445910668\n",
      "Test Acc:  58.111999999999995 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6786466053802601  Time:  1.5067219734191895\n",
      "Test Loss:  0.6695211122230608\n",
      "Test Acc:  55.718 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6777963953731704  Time:  1.4950881004333496\n",
      "Test Loss:  0.6675982642538694\n",
      "Test Acc:  57.402 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6774123974525146  Time:  1.4902591705322266\n",
      "Test Loss:  0.6687972515213246\n",
      "Test Acc:  57.391999999999996 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6783447537544  Time:  1.5061910152435303\n",
      "Test Loss:  0.6721039101177332\n",
      "Test Acc:  59.346 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6776708899623286  Time:  1.4577140808105469\n",
      "Test Loss:  0.670169436809968\n",
      "Test Acc:  57.858 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6763904977888957  Time:  1.4748961925506592\n",
      "Test Loss:  0.6702942422458104\n",
      "Test Acc:  60.982 %\n",
      "================================================== 24\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=26, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=26, out_features=7, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=7, out_features=15, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=15, out_features=137, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=137, out_features=48, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=48, out_features=68, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=68, out_features=11, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=11, out_features=46, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=46, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6927968444180315  Time:  1.3867061138153076\n",
      "Test Loss:  0.6823991950677366\n",
      "Test Acc:  56.382 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6861165104991328  Time:  1.383545160293579\n",
      "Test Loss:  0.672735891171864\n",
      "Test Acc:  59.488 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6811735127529088  Time:  1.36338210105896\n",
      "Test Loss:  0.6718183372701917\n",
      "Test Acc:  59.314 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6795471971922548  Time:  1.4067718982696533\n",
      "Test Loss:  0.6709601544604009\n",
      "Test Acc:  57.626 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6789204954665943  Time:  1.4094672203063965\n",
      "Test Loss:  0.6702164685239598\n",
      "Test Acc:  59.428000000000004 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6788347905134633  Time:  1.3918631076812744\n",
      "Test Loss:  0.6686210781335831\n",
      "Test Acc:  58.284000000000006 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6777741948183436  Time:  1.4067888259887695\n",
      "Test Loss:  0.670299652583745\n",
      "Test Acc:  60.636 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6783699419376624  Time:  1.3948187828063965\n",
      "Test Loss:  0.6694276810300593\n",
      "Test Acc:  58.306000000000004 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6767178249620173  Time:  1.4200799465179443\n",
      "Test Loss:  0.6693928707010892\n",
      "Test Acc:  60.211999999999996 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6779828667640686  Time:  1.3862509727478027\n",
      "Test Loss:  0.6668107509613037\n",
      "Test Acc:  59.412 %\n",
      "================================================== 24\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=15, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=15, out_features=137, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=137, out_features=48, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=48, out_features=68, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=68, out_features=11, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=11, out_features=46, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=46, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.690961384425198  Time:  1.4337959289550781\n",
      "Test Loss:  0.677013923623124\n",
      "Test Acc:  56.342000000000006 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6827229905302508  Time:  1.4708459377288818\n",
      "Test Loss:  0.6729060140799503\n",
      "Test Acc:  57.384 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6798178325169277  Time:  1.4883499145507812\n",
      "Test Loss:  0.6685483105936829\n",
      "Test Acc:  57.742000000000004 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6782480680594479  Time:  1.443660020828247\n",
      "Test Loss:  0.6668152873005185\n",
      "Test Acc:  57.550000000000004 %\n",
      "================================================== 24\n",
      "Train Loss:  0.678250853815218  Time:  1.4667341709136963\n",
      "Test Loss:  0.669277737335283\n",
      "Test Acc:  57.07600000000001 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6778061944637855  Time:  1.452821969985962\n",
      "Test Loss:  0.6727122454618921\n",
      "Test Acc:  59.422 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6775692183605946  Time:  1.42458176612854\n",
      "Test Loss:  0.6685632391243564\n",
      "Test Acc:  59.794000000000004 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6771575203777229  Time:  1.4196031093597412\n",
      "Test Loss:  0.6696981854584753\n",
      "Test Acc:  59.666 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6762909784804295  Time:  1.4273242950439453\n",
      "Test Loss:  0.6667360712070854\n",
      "Test Acc:  59.546 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6771379480831814  Time:  1.4126520156860352\n",
      "Test Loss:  0.6658953641142163\n",
      "Test Acc:  59.156 %\n",
      "================================================== 24\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=113, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=113, out_features=58, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=58, out_features=104, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=104, out_features=55, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=55, out_features=105, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=105, out_features=136, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=136, out_features=91, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=91, out_features=117, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=117, out_features=132, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=132, out_features=25, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=25, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6892692788238943  Time:  1.8199329376220703\n",
      "Test Loss:  0.6712639742359823\n",
      "Test Acc:  58.548 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6821360270472339  Time:  1.8582172393798828\n",
      "Test Loss:  0.675572686961719\n",
      "Test Acc:  58.804 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6791161598950407  Time:  1.9657657146453857\n",
      "Test Loss:  0.6749605308381879\n",
      "Test Acc:  60.882000000000005 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6795458921985905  Time:  2.0327117443084717\n",
      "Test Loss:  0.6717686051008652\n",
      "Test Acc:  59.709999999999994 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6791876980858128  Time:  1.9360857009887695\n",
      "Test Loss:  0.6720205624492801\n",
      "Test Acc:  58.80800000000001 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6785852776391663  Time:  1.953348159790039\n",
      "Test Loss:  0.671681498386422\n",
      "Test Acc:  56.906 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6782964159102336  Time:  1.966559886932373\n",
      "Test Loss:  0.6683502820681553\n",
      "Test Acc:  58.486000000000004 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6788185593420571  Time:  1.9560041427612305\n",
      "Test Loss:  0.6686921435959485\n",
      "Test Acc:  60.57599999999999 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6779106520388248  Time:  2.0164101123809814\n",
      "Test Loss:  0.6660728113991874\n",
      "Test Acc:  59.838 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6783815548367744  Time:  1.9966168403625488\n",
      "Test Loss:  0.6697846775760457\n",
      "Test Acc:  57.916000000000004 %\n",
      "================================================== 24\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=126, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=126, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=20, out_features=45, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=45, out_features=37, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=37, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6898214710454871  Time:  1.5335681438446045\n",
      "Test Loss:  0.6702191403933934\n",
      "Test Acc:  58.80800000000001 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6812675584406749  Time:  1.535757064819336\n",
      "Test Loss:  0.672246665674813\n",
      "Test Acc:  56.316 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6793938455355428  Time:  1.5324480533599854\n",
      "Test Loss:  0.6698587174926486\n",
      "Test Acc:  57.443999999999996 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6787303890625056  Time:  1.475235939025879\n",
      "Test Loss:  0.6672896609014395\n",
      "Test Acc:  57.748 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6772870873882822  Time:  1.4749770164489746\n",
      "Test Loss:  0.6699313636945219\n",
      "Test Acc:  59.541999999999994 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6774769814780158  Time:  1.4791209697723389\n",
      "Test Loss:  0.6696393590192405\n",
      "Test Acc:  57.152 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6776136425289795  Time:  1.4911518096923828\n",
      "Test Loss:  0.6698255551104643\n",
      "Test Acc:  58.138 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6766280349153672  Time:  1.5189220905303955\n",
      "Test Loss:  0.6685352906280634\n",
      "Test Acc:  59.44199999999999 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6765845307033428  Time:  1.534224033355713\n",
      "Test Loss:  0.6667263596033564\n",
      "Test Acc:  58.513999999999996 %\n",
      "================================================== 25\n",
      "Train Loss:  0.678138871915149  Time:  1.5185720920562744\n",
      "Test Loss:  0.6703286204411059\n",
      "Test Acc:  58.918000000000006 %\n",
      "================================================== 25\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=108, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=108, out_features=108, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=108, out_features=21, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=21, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=69, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=69, out_features=96, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=96, out_features=34, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=34, out_features=83, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=83, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6886012741248974  Time:  1.4927809238433838\n",
      "Test Loss:  0.675868204661778\n",
      "Test Acc:  56.916 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6803966466962856  Time:  1.4961748123168945\n",
      "Test Loss:  0.6729023094688144\n",
      "Test Acc:  58.106 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6798073951780361  Time:  1.4769110679626465\n",
      "Test Loss:  0.6731776315338758\n",
      "Test Acc:  60.67399999999999 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6786275956317456  Time:  1.5214619636535645\n",
      "Test Loss:  0.666395687327093\n",
      "Test Acc:  59.374 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6778550815843317  Time:  1.5212020874023438\n",
      "Test Loss:  0.6637253247353495\n",
      "Test Acc:  60.082 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6764811581938807  Time:  1.5208792686462402\n",
      "Test Loss:  0.6690200174949608\n",
      "Test Acc:  57.184000000000005 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6777678628037446  Time:  1.5424063205718994\n",
      "Test Loss:  0.6669234940592124\n",
      "Test Acc:  56.745999999999995 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6768950025965698  Time:  1.5624959468841553\n",
      "Test Loss:  0.6661520734125254\n",
      "Test Acc:  59.95 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6748956624608841  Time:  1.5423753261566162\n",
      "Test Loss:  0.6678836625449511\n",
      "Test Acc:  59.608000000000004 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6764137662240188  Time:  1.544813871383667\n",
      "Test Loss:  0.6694925786281118\n",
      "Test Acc:  56.896 %\n",
      "================================================== 25\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=21, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=21, out_features=112, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=112, out_features=69, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=69, out_features=96, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=96, out_features=34, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=34, out_features=83, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=83, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.688874512040702  Time:  1.4313740730285645\n",
      "Test Loss:  0.6705221016796268\n",
      "Test Acc:  57.348 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6795466433476357  Time:  1.416546106338501\n",
      "Test Loss:  0.6678583746661946\n",
      "Test Acc:  58.314 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6794618716205123  Time:  1.451742172241211\n",
      "Test Loss:  0.6718122773632711\n",
      "Test Acc:  57.135999999999996 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6786942877908693  Time:  1.4744620323181152\n",
      "Test Loss:  0.6672265152541959\n",
      "Test Acc:  57.562000000000005 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6780264131779218  Time:  1.5194933414459229\n",
      "Test Loss:  0.6685893413971882\n",
      "Test Acc:  59.31999999999999 %\n",
      "================================================== 25\n",
      "Train Loss:  0.678065266269837  Time:  1.5669660568237305\n",
      "Test Loss:  0.6666602994106254\n",
      "Test Acc:  60.69 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6776199123285113  Time:  1.5564019680023193\n",
      "Test Loss:  0.6679189083527546\n",
      "Test Acc:  58.328 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6771373877125065  Time:  1.5197129249572754\n",
      "Test Loss:  0.6681879311800003\n",
      "Test Acc:  58.446 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6769621139460237  Time:  1.5080058574676514\n",
      "Test Loss:  0.6679977738127416\n",
      "Test Acc:  57.940000000000005 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6766297325600673  Time:  1.495492696762085\n",
      "Test Loss:  0.6685856857470104\n",
      "Test Acc:  58.146 %\n",
      "================================================== 25\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=119, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=119, out_features=24, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=24, out_features=41, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=41, out_features=62, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=62, out_features=44, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=44, out_features=123, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=123, out_features=16, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=16, out_features=102, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=102, out_features=9, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=9, out_features=65, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=65, out_features=21, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=21, out_features=123, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=123, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6920943092690767  Time:  1.6870191097259521\n",
      "Test Loss:  0.6843138741595405\n",
      "Test Acc:  58.884 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6827948885242434  Time:  1.6735811233520508\n",
      "Test Loss:  0.6692188290917144\n",
      "Test Acc:  59.572 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6795475395926593  Time:  1.669631004333496\n",
      "Test Loss:  0.6677185966044056\n",
      "Test Acc:  57.599999999999994 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6782663075158196  Time:  1.7296030521392822\n",
      "Test Loss:  0.669028387690077\n",
      "Test Acc:  61.318 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6785220273219756  Time:  1.7365257740020752\n",
      "Test Loss:  0.6689003499186769\n",
      "Test Acc:  56.401999999999994 %\n",
      "================================================== 25\n",
      "Train Loss:  0.678117709220761  Time:  1.7426161766052246\n",
      "Test Loss:  0.6701782911407704\n",
      "Test Acc:  59.18 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6781821244389471  Time:  1.7689471244812012\n",
      "Test Loss:  0.6670575287877297\n",
      "Test Acc:  59.646 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6767616596100104  Time:  1.7787079811096191\n",
      "Test Loss:  0.6660811116500777\n",
      "Test Acc:  60.824 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6776989757144538  Time:  1.7004129886627197\n",
      "Test Loss:  0.6669936268305292\n",
      "Test Acc:  58.338 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6771717497902195  Time:  1.6879842281341553\n",
      "Test Loss:  0.6677244448540162\n",
      "Test Acc:  58.268 %\n",
      "================================================== 25\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=71, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=71, out_features=123, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=123, out_features=93, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=93, out_features=77, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=46, out_features=43, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=43, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6901713418264459  Time:  1.5153648853302002\n",
      "Test Loss:  0.6755726416500247\n",
      "Test Acc:  58.330000000000005 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6818293285195844  Time:  1.520263910293579\n",
      "Test Loss:  0.6704064677564465\n",
      "Test Acc:  59.116 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6800290189520286  Time:  1.579801082611084\n",
      "Test Loss:  0.6728077582558807\n",
      "Test Acc:  55.989999999999995 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6792368425505004  Time:  1.5946760177612305\n",
      "Test Loss:  0.6697051856590777\n",
      "Test Acc:  57.848 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6780971969566206  Time:  1.6365830898284912\n",
      "Test Loss:  0.6696209460496902\n",
      "Test Acc:  58.19800000000001 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6774501920181469  Time:  1.638166904449463\n",
      "Test Loss:  0.6705781309580316\n",
      "Test Acc:  59.278 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6769405651266558  Time:  1.615011215209961\n",
      "Test Loss:  0.6699939105583697\n",
      "Test Acc:  58.604 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6767075168390344  Time:  1.568674087524414\n",
      "Test Loss:  0.6687130091749892\n",
      "Test Acc:  57.062000000000005 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6770736725225935  Time:  1.571972131729126\n",
      "Test Loss:  0.6665390480537804\n",
      "Test Acc:  58.806000000000004 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6760091420507779  Time:  1.5717546939849854\n",
      "Test Loss:  0.6692530552343446\n",
      "Test Acc:  60.352 %\n",
      "================================================== 26\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=129, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=129, out_features=56, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=56, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=126, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=126, out_features=20, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=20, out_features=45, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=45, out_features=37, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=37, out_features=95, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=95, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6873249781392786  Time:  1.4743461608886719\n",
      "Test Loss:  0.6682760146807651\n",
      "Test Acc:  59.367999999999995 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6798333917221013  Time:  1.4918551445007324\n",
      "Test Loss:  0.6701425691040195\n",
      "Test Acc:  58.572 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6785836060986902  Time:  1.5285148620605469\n",
      "Test Loss:  0.6716098882714097\n",
      "Test Acc:  58.728 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6787333773435468  Time:  1.556779146194458\n",
      "Test Loss:  0.6696000911143362\n",
      "Test Acc:  57.902 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6773707871019405  Time:  1.648575782775879\n",
      "Test Loss:  0.669010575936765\n",
      "Test Acc:  57.094 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6779710234951799  Time:  1.622992753982544\n",
      "Test Loss:  0.6693066839052706\n",
      "Test Acc:  56.58 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6772999334944426  Time:  1.5873050689697266\n",
      "Test Loss:  0.6683835445009932\n",
      "Test Acc:  58.57599999999999 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6775189318361073  Time:  1.519603967666626\n",
      "Test Loss:  0.6670859963918219\n",
      "Test Acc:  58.84 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6758302213066686  Time:  1.6564993858337402\n",
      "Test Loss:  0.6687642515313869\n",
      "Test Acc:  58.47 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6774854810133467  Time:  1.565814733505249\n",
      "Test Loss:  0.6673474682837116\n",
      "Test Acc:  59.102 %\n",
      "================================================== 26\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=103, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=103, out_features=126, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=126, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=20, out_features=45, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=45, out_features=37, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=37, out_features=95, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=95, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6881571734038583  Time:  1.62434720993042\n",
      "Test Loss:  0.6744876081237987\n",
      "Test Acc:  56.757999999999996 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6804927165490867  Time:  1.5530478954315186\n",
      "Test Loss:  0.6685057565265772\n",
      "Test Acc:  58.9 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6793850031647369  Time:  1.5828242301940918\n",
      "Test Loss:  0.6719930743684575\n",
      "Test Acc:  56.004 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6781772540433564  Time:  1.5719058513641357\n",
      "Test Loss:  0.6685659900611761\n",
      "Test Acc:  59.364 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6785669709644179  Time:  1.537592887878418\n",
      "Test Loss:  0.6696238487350697\n",
      "Test Acc:  58.568 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6781450299015881  Time:  1.5098450183868408\n",
      "Test Loss:  0.6687557745952996\n",
      "Test Acc:  57.908 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6770259859788157  Time:  1.4891881942749023\n",
      "Test Loss:  0.6684865626145382\n",
      "Test Acc:  57.95400000000001 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6773155270701777  Time:  1.496683120727539\n",
      "Test Loss:  0.668578503387315\n",
      "Test Acc:  59.706 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6776368757234003  Time:  1.5391721725463867\n",
      "Test Loss:  0.6693786778012101\n",
      "Test Acc:  57.264 %\n",
      "================================================== 26\n",
      "Train Loss:  0.676721738855334  Time:  1.574077844619751\n",
      "Test Loss:  0.6687074303627014\n",
      "Test Acc:  58.544 %\n",
      "================================================== 26\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=80, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=80, out_features=31, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=31, out_features=30, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=30, out_features=26, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=26, out_features=58, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=58, out_features=42, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=42, out_features=110, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=110, out_features=92, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=92, out_features=18, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=18, out_features=17, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=17, out_features=45, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=45, out_features=74, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=74, out_features=34, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=34, out_features=91, bias=True)\n",
      "    (27): LeakyReLU(negative_slope=0.01)\n",
      "    (28): Linear(in_features=91, out_features=12, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=12, out_features=97, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=97, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933212806708622  Time:  2.0754120349884033\n",
      "Test Loss:  0.6931845293361314\n",
      "Test Acc:  50.056 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6933285735384391  Time:  2.0915980339050293\n",
      "Test Loss:  0.693160068927979\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6932069500432397  Time:  2.0866141319274902\n",
      "Test Loss:  0.6932204380935553\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6932123884232375  Time:  2.0157182216644287\n",
      "Test Loss:  0.693227940980269\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6932401406939013  Time:  2.0242910385131836\n",
      "Test Loss:  0.6931695895535606\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6932762563228607  Time:  2.0446488857269287\n",
      "Test Loss:  0.6932237063135419\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6932375152615735  Time:  2.097080945968628\n",
      "Test Loss:  0.693214764096299\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6932337168794479  Time:  2.1378798484802246\n",
      "Test Loss:  0.693336006329984\n",
      "Test Acc:  50.056 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6931984233160089  Time:  2.138517141342163\n",
      "Test Loss:  0.6934528846521767\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6932491159787143  Time:  2.129246950149536\n",
      "Test Loss:  0.6932678742676365\n",
      "Test Acc:  49.944 %\n",
      "================================================== 26\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=39, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=39, out_features=14, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=14, out_features=18, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=18, out_features=87, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=87, out_features=19, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=19, out_features=117, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=117, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6918421098350609  Time:  1.4312660694122314\n",
      "Test Loss:  0.6776789378146736\n",
      "Test Acc:  59.263999999999996 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6813980514985801  Time:  1.3833370208740234\n",
      "Test Loss:  0.6710444910793888\n",
      "Test Acc:  57.824 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6800651448051425  Time:  1.3855600357055664\n",
      "Test Loss:  0.6676747646867013\n",
      "Test Acc:  59.065999999999995 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6785632306641906  Time:  1.400285005569458\n",
      "Test Loss:  0.6690309303147453\n",
      "Test Acc:  56.598000000000006 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6775017158828512  Time:  1.4394221305847168\n",
      "Test Loss:  0.6660924119000532\n",
      "Test Acc:  58.08 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6783838000175727  Time:  1.4922099113464355\n",
      "Test Loss:  0.6675256928619073\n",
      "Test Acc:  59.95399999999999 %\n",
      "================================================== 27\n",
      "Train Loss:  0.677871915763312  Time:  1.4838483333587646\n",
      "Test Loss:  0.6663802694909426\n",
      "Test Acc:  59.596000000000004 %\n",
      "================================================== 27\n",
      "Train Loss:  0.678178576874907  Time:  1.4423768520355225\n",
      "Test Loss:  0.6665393171869979\n",
      "Test Acc:  59.67400000000001 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6771241907220688  Time:  1.44295072555542\n",
      "Test Loss:  0.6651064668382917\n",
      "Test Acc:  60.194 %\n",
      "================================================== 27\n",
      "Train Loss:  0.676243743974797  Time:  1.4425301551818848\n",
      "Test Loss:  0.6671890072068389\n",
      "Test Acc:  57.706 %\n",
      "================================================== 27\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=63, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=63, out_features=71, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=71, out_features=123, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=123, out_features=93, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=93, out_features=77, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=46, out_features=43, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=43, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6917421306571822  Time:  1.4861948490142822\n",
      "Test Loss:  0.6772530306966937\n",
      "Test Acc:  58.702 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6810920721423017  Time:  1.5240318775177002\n",
      "Test Loss:  0.6690238583452848\n",
      "Test Acc:  59.184000000000005 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6783330901695864  Time:  1.5561249256134033\n",
      "Test Loss:  0.669253661924479\n",
      "Test Acc:  57.56 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6770332290308319  Time:  1.59720778465271\n",
      "Test Loss:  0.670180650998135\n",
      "Test Acc:  58.982 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6764998172756529  Time:  1.630985975265503\n",
      "Test Loss:  0.6717332610670401\n",
      "Test Acc:  60.29600000000001 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6784867387618462  Time:  1.6192188262939453\n",
      "Test Loss:  0.6692186472367267\n",
      "Test Acc:  57.352000000000004 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6786521327321546  Time:  1.6348741054534912\n",
      "Test Loss:  0.6693945746032559\n",
      "Test Acc:  58.806000000000004 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6762253037334358  Time:  1.6017558574676514\n",
      "Test Loss:  0.6722187004527267\n",
      "Test Acc:  59.512 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6765084066530214  Time:  1.5756118297576904\n",
      "Test Loss:  0.6678812722770535\n",
      "Test Acc:  59.434 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6771262077084423  Time:  1.6040661334991455\n",
      "Test Loss:  0.6661320754459926\n",
      "Test Acc:  58.006 %\n",
      "================================================== 27\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=71, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=71, out_features=123, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=123, out_features=93, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=93, out_features=77, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=46, out_features=43, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=43, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6908268308552512  Time:  1.542435884475708\n",
      "Test Loss:  0.6785881318608109\n",
      "Test Acc:  56.992 %\n",
      "================================================== 27\n",
      "Train Loss:  0.68261024508163  Time:  1.536818027496338\n",
      "Test Loss:  0.6750570693794562\n",
      "Test Acc:  57.04 %\n",
      "================================================== 27\n",
      "Train Loss:  0.679341033862455  Time:  1.4874308109283447\n",
      "Test Loss:  0.6672342942685497\n",
      "Test Acc:  59.082 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6788201388651437  Time:  1.5439481735229492\n",
      "Test Loss:  0.669105126541488\n",
      "Test Acc:  59.312 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6784987554062892  Time:  1.5413219928741455\n",
      "Test Loss:  0.671749725633738\n",
      "Test Acc:  57.364000000000004 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6775151865325705  Time:  1.5553219318389893\n",
      "Test Loss:  0.6670452082643703\n",
      "Test Acc:  58.940000000000005 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6767703703720204  Time:  1.6036028861999512\n",
      "Test Loss:  0.6685278972192686\n",
      "Test Acc:  58.391999999999996 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6766662849997082  Time:  1.552657127380371\n",
      "Test Loss:  0.6719883105584553\n",
      "Test Acc:  58.714 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6773451125099711  Time:  1.5644068717956543\n",
      "Test Loss:  0.6686073252741171\n",
      "Test Acc:  59.486000000000004 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6772684698557332  Time:  1.5264308452606201\n",
      "Test Loss:  0.6673199172530856\n",
      "Test Acc:  58.642 %\n",
      "================================================== 27\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=20, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=20, out_features=125, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=125, out_features=101, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=101, out_features=50, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=50, out_features=4, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=4, out_features=111, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=111, out_features=5, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=5, out_features=136, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=136, out_features=128, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=128, out_features=97, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=97, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6934164253464581  Time:  1.6051242351531982\n",
      "Test Loss:  0.6931766800734461\n",
      "Test Acc:  49.944 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6931811092543776  Time:  1.6438040733337402\n",
      "Test Loss:  0.6932296105185334\n",
      "Test Acc:  50.056 %\n",
      "================================================== 27\n",
      "Train Loss:  0.692285444614661  Time:  1.6578450202941895\n",
      "Test Loss:  0.6844892459256309\n",
      "Test Acc:  56.010000000000005 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6847866377691283  Time:  1.6880550384521484\n",
      "Test Loss:  0.6731351744763705\n",
      "Test Acc:  56.672 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6819626114664287  Time:  1.7392971515655518\n",
      "Test Loss:  0.6732523109839887\n",
      "Test Acc:  56.10000000000001 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6805559042161399  Time:  1.7902061939239502\n",
      "Test Loss:  0.6717295713570653\n",
      "Test Acc:  59.034 %\n",
      "================================================== 27\n",
      "Train Loss:  0.679561666763612  Time:  1.7036559581756592\n",
      "Test Loss:  0.6727523919270963\n",
      "Test Acc:  58.632 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6779911250528627  Time:  1.6902549266815186\n",
      "Test Loss:  0.6706620187175517\n",
      "Test Acc:  56.95 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6791576197112563  Time:  1.677189826965332\n",
      "Test Loss:  0.6682795547709173\n",
      "Test Acc:  60.094 %\n",
      "================================================== 27\n",
      "Train Loss:  0.6791040788601784  Time:  1.6911900043487549\n",
      "Test Loss:  0.6697464019668345\n",
      "Test Acc:  57.426 %\n",
      "================================================== 27\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=80, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6890908710277863  Time:  1.5197272300720215\n",
      "Test Loss:  0.672181746485282\n",
      "Test Acc:  58.077999999999996 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6804064890764056  Time:  1.5694968700408936\n",
      "Test Loss:  0.6690308050233491\n",
      "Test Acc:  59.309999999999995 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6789334888440849  Time:  1.5768818855285645\n",
      "Test Loss:  0.6711021205600427\n",
      "Test Acc:  57.958 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6781774161070803  Time:  1.5791819095611572\n",
      "Test Loss:  0.671172631942496\n",
      "Test Acc:  56.106 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6789095506180812  Time:  1.5052669048309326\n",
      "Test Loss:  0.6689036762227818\n",
      "Test Acc:  58.18 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6775727150213979  Time:  1.4854872226715088\n",
      "Test Loss:  0.6693545303174427\n",
      "Test Acc:  60.748000000000005 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6775555199515211  Time:  1.5073611736297607\n",
      "Test Loss:  0.669318862715546\n",
      "Test Acc:  57.76 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6771241970305896  Time:  1.5167248249053955\n",
      "Test Loss:  0.6677204589454495\n",
      "Test Acc:  59.41 %\n",
      "================================================== 28\n",
      "Train Loss:  0.677276058571182  Time:  1.5299787521362305\n",
      "Test Loss:  0.6730497658860927\n",
      "Test Acc:  59.714 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6754569132832715  Time:  1.5946388244628906\n",
      "Test Loss:  0.6675519474915096\n",
      "Test Acc:  59.77 %\n",
      "================================================== 28\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=118, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=118, out_features=59, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=59, out_features=71, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=71, out_features=123, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=123, out_features=93, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=93, out_features=77, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=46, out_features=43, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=43, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6891421107915197  Time:  1.6044561862945557\n",
      "Test Loss:  0.6739707592190528\n",
      "Test Acc:  59.25 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6805629275576042  Time:  1.5893900394439697\n",
      "Test Loss:  0.6715760243182279\n",
      "Test Acc:  58.040000000000006 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6798507155728166  Time:  1.5503199100494385\n",
      "Test Loss:  0.6709864838999144\n",
      "Test Acc:  56.85 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6788362984674691  Time:  1.5139367580413818\n",
      "Test Loss:  0.6688609929109106\n",
      "Test Acc:  58.391999999999996 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6779862509591736  Time:  1.4933781623840332\n",
      "Test Loss:  0.6661759514589699\n",
      "Test Acc:  58.29 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6782920497612361  Time:  1.525373935699463\n",
      "Test Loss:  0.6672594532066461\n",
      "Test Acc:  59.321999999999996 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6768448803111584  Time:  1.533858060836792\n",
      "Test Loss:  0.6681318949071728\n",
      "Test Acc:  59.428000000000004 %\n",
      "================================================== 28\n",
      "Train Loss:  0.67673848123446  Time:  1.6210286617279053\n",
      "Test Loss:  0.667899566645525\n",
      "Test Acc:  57.477999999999994 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6768189370632172  Time:  1.6100449562072754\n",
      "Test Loss:  0.6700558635045071\n",
      "Test Acc:  58.512 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6772464283191375  Time:  1.622025966644287\n",
      "Test Loss:  0.6688282158301802\n",
      "Test Acc:  58.134 %\n",
      "================================================== 28\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=71, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=71, out_features=123, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=123, out_features=93, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=93, out_features=77, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=77, out_features=46, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=46, out_features=43, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=43, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6891127857413605  Time:  1.554440975189209\n",
      "Test Loss:  0.6709303779869663\n",
      "Test Acc:  58.682 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6813108616501745  Time:  1.538647174835205\n",
      "Test Loss:  0.6695329279315715\n",
      "Test Acc:  58.367999999999995 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6783371010836023  Time:  1.5220601558685303\n",
      "Test Loss:  0.6685695678603892\n",
      "Test Acc:  58.438 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6791867670351571  Time:  1.5681450366973877\n",
      "Test Loss:  0.6667853840151612\n",
      "Test Acc:  58.324 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6779813953559765  Time:  1.5781559944152832\n",
      "Test Loss:  0.6689003560007835\n",
      "Test Acc:  59.77400000000001 %\n",
      "================================================== 28\n",
      "Train Loss:  0.678154864667976  Time:  1.627579927444458\n",
      "Test Loss:  0.669880998986108\n",
      "Test Acc:  58.518 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6775340346524316  Time:  1.6482789516448975\n",
      "Test Loss:  0.6709933828334419\n",
      "Test Acc:  57.49999999999999 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6779627547646961  Time:  1.6462819576263428\n",
      "Test Loss:  0.6697191422691151\n",
      "Test Acc:  56.692 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6763202304387614  Time:  1.6015019416809082\n",
      "Test Loss:  0.6678109044323162\n",
      "Test Acc:  57.384 %\n",
      "================================================== 28\n",
      "Train Loss:  0.676850107246942  Time:  1.617142915725708\n",
      "Test Loss:  0.6676466866415374\n",
      "Test Acc:  58.718 %\n",
      "================================================== 28\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=132, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=132, out_features=6, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=6, out_features=5, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=5, out_features=68, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=68, out_features=51, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=51, out_features=85, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=85, out_features=29, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=29, out_features=122, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=122, out_features=32, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=32, out_features=47, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=47, out_features=97, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=97, out_features=118, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=118, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932644082681976  Time:  1.7056281566619873\n",
      "Test Loss:  0.6933120933114266\n",
      "Test Acc:  49.944 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6932450245766744  Time:  1.714540719985962\n",
      "Test Loss:  0.6931510138876584\n",
      "Test Acc:  49.944 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6927804198578327  Time:  1.755228042602539\n",
      "Test Loss:  0.690246197338007\n",
      "Test Acc:  53.535999999999994 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6859023286043292  Time:  1.7873718738555908\n",
      "Test Loss:  0.6799818459822207\n",
      "Test Acc:  56.07 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6815459006024103  Time:  1.7994885444641113\n",
      "Test Loss:  0.6721414233348808\n",
      "Test Acc:  58.013999999999996 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6799280143132175  Time:  1.8551719188690186\n",
      "Test Loss:  0.6681512502991424\n",
      "Test Acc:  61.22 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6790641953040214  Time:  1.8443803787231445\n",
      "Test Loss:  0.668393116824481\n",
      "Test Acc:  58.876 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6787850604440174  Time:  1.848417043685913\n",
      "Test Loss:  0.6677361802787197\n",
      "Test Acc:  58.652 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6784217451175634  Time:  1.8450727462768555\n",
      "Test Loss:  0.6685867470746137\n",
      "Test Acc:  58.458 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6782623581642652  Time:  1.8045830726623535\n",
      "Test Loss:  0.6668785950359033\n",
      "Test Acc:  57.25 %\n",
      "================================================== 28\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=15, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=15, out_features=48, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=48, out_features=9, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=9, out_features=86, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=86, out_features=72, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=72, out_features=93, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=93, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6900917785446139  Time:  1.4409539699554443\n",
      "Test Loss:  0.6725053407099782\n",
      "Test Acc:  58.58 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6798330458411335  Time:  1.441014051437378\n",
      "Test Loss:  0.6689787142130793\n",
      "Test Acc:  57.532000000000004 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6792822301822857  Time:  1.4301860332489014\n",
      "Test Loss:  0.6696922760836932\n",
      "Test Acc:  58.489999999999995 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6780350984883134  Time:  1.472681999206543\n",
      "Test Loss:  0.6675568372011185\n",
      "Test Acc:  58.852000000000004 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6780907814085049  Time:  1.468369960784912\n",
      "Test Loss:  0.6670680593471138\n",
      "Test Acc:  58.158 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6774578259809174  Time:  1.468451976776123\n",
      "Test Loss:  0.6661791986956889\n",
      "Test Acc:  60.196000000000005 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6786675694650107  Time:  1.4736640453338623\n",
      "Test Loss:  0.6674204088595449\n",
      "Test Acc:  58.18600000000001 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6773328942103978  Time:  1.474303960800171\n",
      "Test Loss:  0.6652209293477389\n",
      "Test Acc:  60.314 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6770474386476252  Time:  1.4671688079833984\n",
      "Test Loss:  0.6676840392910705\n",
      "Test Acc:  58.716 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6764646818603042  Time:  1.4695940017700195\n",
      "Test Loss:  0.6685686415555526\n",
      "Test Acc:  58.309999999999995 %\n",
      "================================================== 29\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=64, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=64, out_features=71, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=71, out_features=80, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6913512660204059  Time:  1.5020079612731934\n",
      "Test Loss:  0.6832965734053631\n",
      "Test Acc:  55.47 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6813395725984643  Time:  1.4457080364227295\n",
      "Test Loss:  0.6725908761121788\n",
      "Test Acc:  60.406000000000006 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6795167000624385  Time:  1.47340726852417\n",
      "Test Loss:  0.6709119045004552\n",
      "Test Acc:  55.879999999999995 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6787289365799758  Time:  1.4931669235229492\n",
      "Test Loss:  0.6678601640219591\n",
      "Test Acc:  59.062000000000005 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6778164553816302  Time:  1.4849419593811035\n",
      "Test Loss:  0.6706995848490267\n",
      "Test Acc:  59.372 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6772592998769161  Time:  1.5070111751556396\n",
      "Test Loss:  0.6704069972038269\n",
      "Test Acc:  58.646 %\n",
      "================================================== 29\n",
      "Train Loss:  0.67735088937474  Time:  1.4739768505096436\n",
      "Test Loss:  0.668939042456296\n",
      "Test Acc:  60.138000000000005 %\n",
      "================================================== 29\n",
      "Train Loss:  0.676675844366533  Time:  1.4829738140106201\n",
      "Test Loss:  0.6718229487234232\n",
      "Test Acc:  59.053999999999995 %\n",
      "================================================== 29\n",
      "Train Loss:  0.677045948749041  Time:  1.4950671195983887\n",
      "Test Loss:  0.66968970122386\n",
      "Test Acc:  59.406000000000006 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6780237344494702  Time:  1.4851667881011963\n",
      "Test Loss:  0.6717338735351757\n",
      "Test Acc:  57.828 %\n",
      "================================================== 29\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=80, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6904169599505237  Time:  1.4858078956604004\n",
      "Test Loss:  0.6742934602863935\n",
      "Test Acc:  59.053999999999995 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6807462938510589  Time:  1.4919111728668213\n",
      "Test Loss:  0.6723796837792104\n",
      "Test Acc:  57.886 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6803776650098119  Time:  1.5234968662261963\n",
      "Test Loss:  0.6702792857374463\n",
      "Test Acc:  58.278 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6783755317221593  Time:  1.5449321269989014\n",
      "Test Loss:  0.6682771885273407\n",
      "Test Acc:  60.348 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6771690773702886  Time:  1.5354480743408203\n",
      "Test Loss:  0.6667025502847166\n",
      "Test Acc:  58.376 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6781844670755149  Time:  1.5331499576568604\n",
      "Test Loss:  0.6713603981295411\n",
      "Test Acc:  60.192 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6778037230463794  Time:  1.5361380577087402\n",
      "Test Loss:  0.6682273046094545\n",
      "Test Acc:  59.18 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6757440484353225  Time:  1.5466508865356445\n",
      "Test Loss:  0.66803562367449\n",
      "Test Acc:  57.742000000000004 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6769429802024451  Time:  1.4984400272369385\n",
      "Test Loss:  0.6678592699522875\n",
      "Test Acc:  59.492 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6759512254791539  Time:  1.539376974105835\n",
      "Test Loss:  0.669892934512119\n",
      "Test Acc:  59.128 %\n",
      "================================================== 29\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=124, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=124, out_features=44, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=44, out_features=4, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4, out_features=78, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=78, out_features=97, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=97, out_features=45, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=45, out_features=11, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=11, out_features=102, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=102, out_features=91, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=91, out_features=110, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=110, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6908827506712754  Time:  1.6393039226531982\n",
      "Test Loss:  0.6701319077793433\n",
      "Test Acc:  59.343999999999994 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6808572624721666  Time:  1.6360430717468262\n",
      "Test Loss:  0.6671605274385336\n",
      "Test Acc:  60.0 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6796451548155207  Time:  1.6711602210998535\n",
      "Test Loss:  0.6679759211077982\n",
      "Test Acc:  57.278 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6779725381927769  Time:  1.7629709243774414\n",
      "Test Loss:  0.6676041565987528\n",
      "Test Acc:  58.955999999999996 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6779048986678576  Time:  1.7354450225830078\n",
      "Test Loss:  0.6684560447323079\n",
      "Test Acc:  59.67 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6782582047646933  Time:  1.7348790168762207\n",
      "Test Loss:  0.6687491134721406\n",
      "Test Acc:  59.106 %\n",
      "================================================== 29\n",
      "Train Loss:  0.678682345978535  Time:  1.6898720264434814\n",
      "Test Loss:  0.6679954908940257\n",
      "Test Acc:  60.01 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6770353428203694  Time:  1.676109790802002\n",
      "Test Loss:  0.6683321066048681\n",
      "Test Acc:  60.251999999999995 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6762918531894684  Time:  1.7368640899658203\n",
      "Test Loss:  0.669718267053974\n",
      "Test Acc:  58.443999999999996 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6774758385480756  Time:  1.7300879955291748\n",
      "Test Loss:  0.6692929243554875\n",
      "Test Acc:  57.184000000000005 %\n",
      "================================================== 29\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=59, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=59, out_features=118, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=118, out_features=70, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=70, out_features=82, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=82, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=20, out_features=105, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=105, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6893575683127354  Time:  1.5187067985534668\n",
      "Test Loss:  0.6774977795323547\n",
      "Test Acc:  56.19 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6817479960239716  Time:  1.524305820465088\n",
      "Test Loss:  0.6676153127028017\n",
      "Test Acc:  60.368 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6786593901414941  Time:  1.491326093673706\n",
      "Test Loss:  0.6674314196012459\n",
      "Test Acc:  58.896 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6790374550071075  Time:  1.503601312637329\n",
      "Test Loss:  0.6689712502518479\n",
      "Test Acc:  59.053999999999995 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6788492975008749  Time:  1.4939560890197754\n",
      "Test Loss:  0.6664246855949869\n",
      "Test Acc:  59.831999999999994 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6777110982985393  Time:  1.4815800189971924\n",
      "Test Loss:  0.6695035486196985\n",
      "Test Acc:  57.374 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6786896490702664  Time:  1.4771409034729004\n",
      "Test Loss:  0.671346473146458\n",
      "Test Acc:  59.772000000000006 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6767269375115416  Time:  1.5316858291625977\n",
      "Test Loss:  0.6695236758309968\n",
      "Test Acc:  57.138 %\n",
      "================================================== 30\n",
      "Train Loss:  0.677542929884291  Time:  1.5140221118927002\n",
      "Test Loss:  0.6706616613937884\n",
      "Test Acc:  58.694 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6772488401318989  Time:  1.5325219631195068\n",
      "Test Loss:  0.6702368885886912\n",
      "Test Acc:  55.962 %\n",
      "================================================== 30\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=61, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=61, out_features=66, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=66, out_features=80, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6910113907643478  Time:  1.5031218528747559\n",
      "Test Loss:  0.6812241019643083\n",
      "Test Acc:  54.523999999999994 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6809567167376079  Time:  1.4726941585540771\n",
      "Test Loss:  0.6698030047878927\n",
      "Test Acc:  59.238 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6798372379619709  Time:  1.4789068698883057\n",
      "Test Loss:  0.6706028288724472\n",
      "Test Acc:  56.867999999999995 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6790400441110569  Time:  1.4534120559692383\n",
      "Test Loss:  0.6708793445509307\n",
      "Test Acc:  57.19800000000001 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6783538776157546  Time:  1.446716070175171\n",
      "Test Loss:  0.6667274178899064\n",
      "Test Acc:  57.69 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6779345554591966  Time:  1.4897470474243164\n",
      "Test Loss:  0.6680173813080301\n",
      "Test Acc:  58.752 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6775960036872948  Time:  1.5001471042633057\n",
      "Test Loss:  0.6690029894210854\n",
      "Test Acc:  57.286 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6777966451035798  Time:  1.4994912147521973\n",
      "Test Loss:  0.667006180602677\n",
      "Test Acc:  59.594 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6761316376445937  Time:  1.4984521865844727\n",
      "Test Loss:  0.6697883247112741\n",
      "Test Acc:  59.766 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6766797201911898  Time:  1.5303559303283691\n",
      "Test Loss:  0.6681779288515752\n",
      "Test Acc:  58.146 %\n",
      "================================================== 30\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=80, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6895007715173012  Time:  1.4944450855255127\n",
      "Test Loss:  0.6723693174367048\n",
      "Test Acc:  59.187999999999995 %\n",
      "================================================== 30\n",
      "Train Loss:  0.680812891817441  Time:  1.5109422206878662\n",
      "Test Loss:  0.6694762773659765\n",
      "Test Acc:  58.192 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6794314784725217  Time:  1.4769947528839111\n",
      "Test Loss:  0.6680368261069668\n",
      "Test Acc:  57.726 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6786632442126309  Time:  1.4753170013427734\n",
      "Test Loss:  0.6684890541495109\n",
      "Test Acc:  58.84 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6788843304571444  Time:  1.5252909660339355\n",
      "Test Loss:  0.6676241509160217\n",
      "Test Acc:  58.376 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6771831840929323  Time:  1.5188779830932617\n",
      "Test Loss:  0.6663426051334459\n",
      "Test Acc:  60.995999999999995 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6775654175855818  Time:  1.530372142791748\n",
      "Test Loss:  0.672237674800717\n",
      "Test Acc:  56.458 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6765693713278667  Time:  1.537248134613037\n",
      "Test Loss:  0.6680913750006228\n",
      "Test Acc:  60.604 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6770013457667219  Time:  1.5212218761444092\n",
      "Test Loss:  0.6683294739650221\n",
      "Test Acc:  58.586000000000006 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6757345636830713  Time:  1.529500961303711\n",
      "Test Loss:  0.6673841579836242\n",
      "Test Acc:  58.17 %\n",
      "================================================== 30\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=27, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=27, out_features=93, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=93, out_features=25, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=25, out_features=26, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=26, out_features=78, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=78, out_features=11, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=11, out_features=40, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=40, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=137, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=137, out_features=85, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=85, out_features=110, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=110, out_features=7, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=7, out_features=125, bias=True)\n",
      "    (25): LeakyReLU(negative_slope=0.01)\n",
      "    (26): Linear(in_features=125, out_features=86, bias=True)\n",
      "    (27): LeakyReLU(negative_slope=0.01)\n",
      "    (28): Linear(in_features=86, out_features=35, bias=True)\n",
      "    (29): LeakyReLU(negative_slope=0.01)\n",
      "    (30): Linear(in_features=35, out_features=36, bias=True)\n",
      "    (31): LeakyReLU(negative_slope=0.01)\n",
      "    (32): Linear(in_features=36, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6931859261363092  Time:  2.1632959842681885\n",
      "Test Loss:  0.6932331874054305\n",
      "Test Acc:  49.944 %\n",
      "================================================== 30\n",
      "Train Loss:  0.693199784868825  Time:  2.171229124069214\n",
      "Test Loss:  0.6931475051203553\n",
      "Test Acc:  50.056 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6931702612090285  Time:  2.220215082168579\n",
      "Test Loss:  0.6931530556508473\n",
      "Test Acc:  49.944 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6931903140388266  Time:  2.303752899169922\n",
      "Test Loss:  0.6931946140771009\n",
      "Test Acc:  49.944 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6932428812023497  Time:  2.292941093444824\n",
      "Test Loss:  0.6931598727800408\n",
      "Test Acc:  50.056 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6931796698239598  Time:  2.2829840183258057\n",
      "Test Loss:  0.6931507657377087\n",
      "Test Acc:  49.944 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6931667419245643  Time:  2.3391659259796143\n",
      "Test Loss:  0.693271816689141\n",
      "Test Acc:  50.056 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6932097301430946  Time:  2.2925260066986084\n",
      "Test Loss:  0.6931527478962528\n",
      "Test Acc:  50.056 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6931970550196014  Time:  2.2241628170013428\n",
      "Test Loss:  0.6931515357324055\n",
      "Test Acc:  49.944 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6931926090351856  Time:  2.257307291030884\n",
      "Test Loss:  0.693203513111387\n",
      "Test Acc:  50.056 %\n",
      "================================================== 30\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=57, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=57, out_features=108, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=108, out_features=124, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=124, out_features=30, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=30, out_features=118, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=118, out_features=70, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=70, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.689823537412351  Time:  1.5077309608459473\n",
      "Test Loss:  0.6768431256012041\n",
      "Test Acc:  56.006 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6815639125169629  Time:  1.5364210605621338\n",
      "Test Loss:  0.6742195888441436\n",
      "Test Acc:  57.042 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6803449311830702  Time:  1.5511808395385742\n",
      "Test Loss:  0.6733460468905312\n",
      "Test Acc:  58.766 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6788356191050398  Time:  1.6070621013641357\n",
      "Test Loss:  0.6704189211738353\n",
      "Test Acc:  57.965999999999994 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6785068912227659  Time:  1.6396369934082031\n",
      "Test Loss:  0.6700766208220501\n",
      "Test Acc:  59.056 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6781741933230936  Time:  1.6211988925933838\n",
      "Test Loss:  0.6680194425339602\n",
      "Test Acc:  57.524 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6760759675589791  Time:  1.6042530536651611\n",
      "Test Loss:  0.6702803723057922\n",
      "Test Acc:  56.8 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6773075206871451  Time:  1.5592548847198486\n",
      "Test Loss:  0.6693798701982109\n",
      "Test Acc:  59.406000000000006 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6765802778901845  Time:  1.5685818195343018\n",
      "Test Loss:  0.6669100532118155\n",
      "Test Acc:  59.104 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6756583530537403  Time:  1.5623397827148438\n",
      "Test Loss:  0.6687161454132625\n",
      "Test Acc:  58.52 %\n",
      "================================================== 31\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=53, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=53, out_features=78, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=78, out_features=80, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6907622851159451  Time:  1.493549108505249\n",
      "Test Loss:  0.6849693084249691\n",
      "Test Acc:  56.632000000000005 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6830745654819655  Time:  1.4838171005249023\n",
      "Test Loss:  0.6701417507565751\n",
      "Test Acc:  57.692 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6800305651922296  Time:  1.4880249500274658\n",
      "Test Loss:  0.6676197751444213\n",
      "Test Acc:  60.153999999999996 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6799657649367395  Time:  1.5179619789123535\n",
      "Test Loss:  0.668777622738663\n",
      "Test Acc:  58.214 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6778093770037602  Time:  1.4701321125030518\n",
      "Test Loss:  0.6703602613843217\n",
      "Test Acc:  59.84 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6778796178146  Time:  1.4538650512695312\n",
      "Test Loss:  0.670092908703551\n",
      "Test Acc:  59.08 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6781545111732762  Time:  1.4398469924926758\n",
      "Test Loss:  0.6685164683327383\n",
      "Test Acc:  58.32000000000001 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6773466828965793  Time:  1.4615390300750732\n",
      "Test Loss:  0.6656510334233848\n",
      "Test Acc:  58.952000000000005 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6768968322851362  Time:  1.4454619884490967\n",
      "Test Loss:  0.6686024416466149\n",
      "Test Acc:  57.008 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6772419947342281  Time:  1.4952168464660645\n",
      "Test Loss:  0.6684762473617282\n",
      "Test Acc:  60.260000000000005 %\n",
      "================================================== 31\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=85, out_features=80, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6893967753344209  Time:  1.5219860076904297\n",
      "Test Loss:  0.6743476868283992\n",
      "Test Acc:  56.43 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6816475582818915  Time:  1.5051250457763672\n",
      "Test Loss:  0.6697876596329163\n",
      "Test Acc:  58.858 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6802262929234192  Time:  1.535226821899414\n",
      "Test Loss:  0.6709415903505014\n",
      "Test Acc:  57.992 %\n",
      "================================================== 31\n",
      "Train Loss:  0.67918840790317  Time:  1.5400300025939941\n",
      "Test Loss:  0.6696448478163505\n",
      "Test Acc:  61.466 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6781489796882129  Time:  1.5478768348693848\n",
      "Test Loss:  0.6690052230747379\n",
      "Test Acc:  57.394 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6773573334199668  Time:  1.5158326625823975\n",
      "Test Loss:  0.6689674075768919\n",
      "Test Acc:  60.85 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6771138649787346  Time:  1.510998010635376\n",
      "Test Loss:  0.6686403562827986\n",
      "Test Acc:  60.424 %\n",
      "================================================== 31\n",
      "Train Loss:  0.677540195249293  Time:  1.55421781539917\n",
      "Test Loss:  0.6673775817058525\n",
      "Test Acc:  57.918000000000006 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6778383337668259  Time:  1.5475172996520996\n",
      "Test Loss:  0.6671222101668922\n",
      "Test Acc:  58.56 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6760574398684676  Time:  1.5708222389221191\n",
      "Test Loss:  0.6668448840477028\n",
      "Test Acc:  60.524 %\n",
      "================================================== 31\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=21, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=21, out_features=83, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=83, out_features=126, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=126, out_features=92, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=92, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=105, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=105, out_features=126, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=126, out_features=75, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=75, out_features=9, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=9, out_features=44, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=44, out_features=135, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=135, out_features=68, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=68, out_features=110, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=110, out_features=108, bias=True)\n",
      "    (27): LeakyReLU(negative_slope=0.01)\n",
      "    (28): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933805779384  Time:  2.1909921169281006\n",
      "Test Loss:  0.693201368256491\n",
      "Test Acc:  50.056 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6932421487613316  Time:  2.21648907661438\n",
      "Test Loss:  0.6931688432790795\n",
      "Test Acc:  50.056 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6931956414758724  Time:  2.2461869716644287\n",
      "Test Loss:  0.6931745540730807\n",
      "Test Acc:  50.056 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6932390865183224  Time:  2.1816508769989014\n",
      "Test Loss:  0.693150883730577\n",
      "Test Acc:  50.056 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6932003502428097  Time:  2.166141986846924\n",
      "Test Loss:  0.693155341002406\n",
      "Test Acc:  50.056 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6931808845405161  Time:  2.204026937484741\n",
      "Test Loss:  0.6931637638077444\n",
      "Test Acc:  49.944 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6931781059634077  Time:  2.3057730197906494\n",
      "Test Loss:  0.6931963055109491\n",
      "Test Acc:  49.944 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6931674545698793  Time:  2.3344919681549072\n",
      "Test Loss:  0.6931665910750019\n",
      "Test Acc:  49.944 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6931711459246865  Time:  2.36130690574646\n",
      "Test Loss:  0.6932000453983035\n",
      "Test Acc:  49.944 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6931553750577635  Time:  2.373936891555786\n",
      "Test Loss:  0.6930940738137887\n",
      "Test Acc:  49.998 %\n",
      "================================================== 31\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=78, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=78, out_features=19, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=19, out_features=118, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=118, out_features=16, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=16, out_features=107, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=107, out_features=105, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=105, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6900884992014753  Time:  1.459183931350708\n",
      "Test Loss:  0.6716047716992242\n",
      "Test Acc:  58.45 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6811895642402398  Time:  1.4422760009765625\n",
      "Test Loss:  0.6723669156128046\n",
      "Test Acc:  56.064 %\n",
      "================================================== 32\n",
      "Train Loss:  0.67960817818224  Time:  1.440713882446289\n",
      "Test Loss:  0.6660842919836238\n",
      "Test Acc:  59.160000000000004 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6796998542590733  Time:  1.4354097843170166\n",
      "Test Loss:  0.6688266375235149\n",
      "Test Acc:  57.391999999999996 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6780344648082761  Time:  1.4357540607452393\n",
      "Test Loss:  0.6665718488547266\n",
      "Test Acc:  59.488 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6773606799814823  Time:  1.4946038722991943\n",
      "Test Loss:  0.6691368343878765\n",
      "Test Acc:  58.855999999999995 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6775046528255853  Time:  1.520097017288208\n",
      "Test Loss:  0.6667124616856478\n",
      "Test Acc:  57.546 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6768447963425713  Time:  1.5376331806182861\n",
      "Test Loss:  0.6672453472808916\n",
      "Test Acc:  58.47599999999999 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6769946511209446  Time:  1.537114143371582\n",
      "Test Loss:  0.6659401871111928\n",
      "Test Acc:  59.352000000000004 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6776893386875626  Time:  1.4552850723266602\n",
      "Test Loss:  0.6680354664520342\n",
      "Test Acc:  59.19799999999999 %\n",
      "================================================== 32\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=57, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=57, out_features=129, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=129, out_features=80, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6921203330050419  Time:  1.4812750816345215\n",
      "Test Loss:  0.6765216002050711\n",
      "Test Acc:  56.788000000000004 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6818124349534946  Time:  1.4956989288330078\n",
      "Test Loss:  0.6710564393778237\n",
      "Test Acc:  60.548 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6804734429303747  Time:  1.490614891052246\n",
      "Test Loss:  0.6686574886648022\n",
      "Test Acc:  59.72 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6798311165214455  Time:  1.5459301471710205\n",
      "Test Loss:  0.6705477629997292\n",
      "Test Acc:  56.566 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6794967864551683  Time:  1.5660927295684814\n",
      "Test Loss:  0.6679966270315404\n",
      "Test Acc:  58.111999999999995 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6780319355265068  Time:  1.5693511962890625\n",
      "Test Loss:  0.6693144662647831\n",
      "Test Acc:  59.69 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6778347820696169  Time:  1.5837440490722656\n",
      "Test Loss:  0.6672978708330466\n",
      "Test Acc:  60.79599999999999 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6773799853603335  Time:  1.5891139507293701\n",
      "Test Loss:  0.6685164090321989\n",
      "Test Acc:  57.372 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6784647154982073  Time:  1.523845911026001\n",
      "Test Loss:  0.6685694370950971\n",
      "Test Acc:  58.15 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6769744610264353  Time:  1.513075828552246\n",
      "Test Loss:  0.6699497252702713\n",
      "Test Acc:  60.268 %\n",
      "================================================== 32\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=91, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=91, out_features=85, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=85, out_features=80, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=80, out_features=76, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=76, out_features=28, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=28, out_features=91, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=91, out_features=68, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=68, out_features=125, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=125, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6878954409247767  Time:  1.4704127311706543\n",
      "Test Loss:  0.6733536495237934\n",
      "Test Acc:  59.062000000000005 %\n",
      "================================================== 32\n",
      "Train Loss:  0.680558113286095  Time:  1.447591781616211\n",
      "Test Loss:  0.6741480030575577\n",
      "Test Acc:  56.462 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6793757753215567  Time:  1.5250020027160645\n",
      "Test Loss:  0.6720123655942022\n",
      "Test Acc:  56.830000000000005 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6798050553694258  Time:  1.5547220706939697\n",
      "Test Loss:  0.6706802434459025\n",
      "Test Acc:  57.245999999999995 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6767526769290005  Time:  1.5701439380645752\n",
      "Test Loss:  0.6651531871484251\n",
      "Test Acc:  59.11 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6765701022461383  Time:  1.6121230125427246\n",
      "Test Loss:  0.6676466796471148\n",
      "Test Acc:  57.668 %\n",
      "================================================== 32\n",
      "Train Loss:  0.678060119169472  Time:  1.5534768104553223\n",
      "Test Loss:  0.6695321180990764\n",
      "Test Acc:  59.30800000000001 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6775728999263179  Time:  1.5262420177459717\n",
      "Test Loss:  0.6740633662866087\n",
      "Test Acc:  55.972 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6777348172490614  Time:  1.5001559257507324\n",
      "Test Loss:  0.6705066555616807\n",
      "Test Acc:  60.036 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6767099965227782  Time:  1.5184931755065918\n",
      "Test Loss:  0.669460116904609\n",
      "Test Acc:  57.706 %\n",
      "================================================== 32\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=89, out_features=66, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=66, out_features=67, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=67, out_features=15, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=15, out_features=76, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=76, out_features=40, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=40, out_features=42, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=42, out_features=91, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.689167622232089  Time:  1.3979129791259766\n",
      "Test Loss:  0.6732659732200661\n",
      "Test Acc:  58.068 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6808376092545307  Time:  1.4394748210906982\n",
      "Test Loss:  0.6706959054786332\n",
      "Test Acc:  57.582 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6800191206653623  Time:  1.4457190036773682\n",
      "Test Loss:  0.6737018638119405\n",
      "Test Acc:  56.792 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6778629241633589  Time:  1.4539358615875244\n",
      "Test Loss:  0.6695668396292901\n",
      "Test Acc:  57.230000000000004 %\n",
      "================================================== 32\n",
      "Train Loss:  0.677951869738363  Time:  1.4812119007110596\n",
      "Test Loss:  0.6710269162241294\n",
      "Test Acc:  55.826 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6776465281952907  Time:  1.4444832801818848\n",
      "Test Loss:  0.6695775444410286\n",
      "Test Acc:  60.587999999999994 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6775726414945004  Time:  1.4279427528381348\n",
      "Test Loss:  0.6680827362804996\n",
      "Test Acc:  59.282000000000004 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6783281149220293  Time:  1.419590950012207\n",
      "Test Loss:  0.6680925439815132\n",
      "Test Acc:  58.998 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6766533973443247  Time:  1.4111952781677246\n",
      "Test Loss:  0.6656137254773354\n",
      "Test Acc:  59.955999999999996 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6763565607749633  Time:  1.4657351970672607\n",
      "Test Loss:  0.668531251501064\n",
      "Test Acc:  60.788 %\n",
      "================================================== 32\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=66, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=66, out_features=133, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=133, out_features=88, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=88, out_features=24, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=24, out_features=7, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=7, out_features=114, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=114, out_features=75, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=75, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6891206161384165  Time:  1.4960010051727295\n",
      "Test Loss:  0.671848285563138\n",
      "Test Acc:  56.308 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6819022330489471  Time:  1.4900758266448975\n",
      "Test Loss:  0.6670607553452862\n",
      "Test Acc:  60.675999999999995 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6789009692895152  Time:  1.5104670524597168\n",
      "Test Loss:  0.6705635950273398\n",
      "Test Acc:  59.45399999999999 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6788670024732604  Time:  1.524271011352539\n",
      "Test Loss:  0.6687531726700919\n",
      "Test Acc:  58.288 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6776440010888733  Time:  1.5282490253448486\n",
      "Test Loss:  0.6670307437984311\n",
      "Test Acc:  60.187999999999995 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6779556000319711  Time:  1.5086488723754883\n",
      "Test Loss:  0.6693651116624171\n",
      "Test Acc:  58.328 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6770169745831593  Time:  1.4934358596801758\n",
      "Test Loss:  0.6690890700841436\n",
      "Test Acc:  58.221999999999994 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6795366228061871  Time:  1.4837908744812012\n",
      "Test Loss:  0.6699574486333497\n",
      "Test Acc:  57.738 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6763558228955652  Time:  1.529120922088623\n",
      "Test Loss:  0.6676019074357286\n",
      "Test Acc:  57.51199999999999 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6772870706380719  Time:  1.5181241035461426\n",
      "Test Loss:  0.6674891786307705\n",
      "Test Acc:  59.294000000000004 %\n",
      "================================================== 33\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=107, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=107, out_features=136, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=136, out_features=67, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=67, out_features=15, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=15, out_features=76, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=76, out_features=40, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=40, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=91, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6882429681990269  Time:  1.475362777709961\n",
      "Test Loss:  0.6699188455026976\n",
      "Test Acc:  58.68 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6812245014810214  Time:  1.4809448719024658\n",
      "Test Loss:  0.6692642155958681\n",
      "Test Acc:  59.336 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6790132925023128  Time:  1.458545207977295\n",
      "Test Loss:  0.6687609830072948\n",
      "Test Acc:  57.728 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6777998561406657  Time:  1.5017330646514893\n",
      "Test Loss:  0.6673509241366873\n",
      "Test Acc:  59.148 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6782126596374233  Time:  1.4991559982299805\n",
      "Test Loss:  0.6679380977032136\n",
      "Test Acc:  58.694 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6776847343375213  Time:  1.4808299541473389\n",
      "Test Loss:  0.6692922607976564\n",
      "Test Acc:  57.172 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6769856575631747  Time:  1.4965801239013672\n",
      "Test Loss:  0.6684570370280013\n",
      "Test Acc:  59.504000000000005 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6763143543779415  Time:  1.5172369480133057\n",
      "Test Loss:  0.6682578413462152\n",
      "Test Acc:  58.211999999999996 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6755469608915984  Time:  1.6104531288146973\n",
      "Test Loss:  0.6675431959483088\n",
      "Test Acc:  58.248 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6762755336987711  Time:  1.5449020862579346\n",
      "Test Loss:  0.6680934721109818\n",
      "Test Acc:  56.838 %\n",
      "================================================== 33\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=66, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=66, out_features=67, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=67, out_features=15, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=15, out_features=76, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=76, out_features=40, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=40, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=91, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6896060046053281  Time:  1.4636690616607666\n",
      "Test Loss:  0.6730744528527163\n",
      "Test Acc:  57.26 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6802188321186678  Time:  1.4039182662963867\n",
      "Test Loss:  0.6712503588321258\n",
      "Test Acc:  57.716 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6792388971704636  Time:  1.4204602241516113\n",
      "Test Loss:  0.672408251129851\n",
      "Test Acc:  58.418000000000006 %\n",
      "================================================== 33\n",
      "Train Loss:  0.679238862799902  Time:  1.441960096359253\n",
      "Test Loss:  0.6709888735596015\n",
      "Test Acc:  59.656 %\n",
      "================================================== 33\n",
      "Train Loss:  0.679690355367034  Time:  1.433962106704712\n",
      "Test Loss:  0.6692717951171252\n",
      "Test Acc:  57.38799999999999 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6786106909713606  Time:  1.4526419639587402\n",
      "Test Loss:  0.6705874739861002\n",
      "Test Acc:  56.330000000000005 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6778516745480307  Time:  1.4443480968475342\n",
      "Test Loss:  0.6705571507312813\n",
      "Test Acc:  56.452000000000005 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6771883649112534  Time:  1.5175302028656006\n",
      "Test Loss:  0.669797340522007\n",
      "Test Acc:  58.018 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6755766864240604  Time:  232.37847304344177\n",
      "Test Loss:  0.6656333773720021\n",
      "Test Acc:  60.272000000000006 %\n",
      "================================================== 33\n",
      "Train Loss:  0.675940586702667  Time:  1.524543285369873\n",
      "Test Loss:  0.6673080206525569\n",
      "Test Acc:  59.84 %\n",
      "================================================== 33\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=117, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=117, out_features=10, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=10, out_features=47, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=47, out_features=83, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=83, out_features=25, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=25, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=99, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=99, out_features=129, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=129, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.686735574128854  Time:  2.1040701866149902\n",
      "Test Loss:  0.6721521339246205\n",
      "Test Acc:  56.364000000000004 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6807974502988106  Time:  2.0526461601257324\n",
      "Test Loss:  0.6710797727716212\n",
      "Test Acc:  59.272000000000006 %\n",
      "================================================== 33\n",
      "Train Loss:  0.68009434303228  Time:  2.069220781326294\n",
      "Test Loss:  0.668279378693931\n",
      "Test Acc:  59.12 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6795804581067858  Time:  1.9630000591278076\n",
      "Test Loss:  0.6670141049793789\n",
      "Test Acc:  60.995999999999995 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6782621306224461  Time:  2.130398988723755\n",
      "Test Loss:  0.6696231477722829\n",
      "Test Acc:  57.862 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6770275187318342  Time:  1.7705252170562744\n",
      "Test Loss:  0.6700805395233388\n",
      "Test Acc:  56.798 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6768317046391703  Time:  1.5404329299926758\n",
      "Test Loss:  0.6696479600303027\n",
      "Test Acc:  58.312 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6769179932392426  Time:  1.5661020278930664\n",
      "Test Loss:  0.6683881662938059\n",
      "Test Acc:  59.0 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6760586045519279  Time:  1.5056540966033936\n",
      "Test Loss:  0.6663811526128224\n",
      "Test Acc:  60.866 %\n",
      "================================================== 33\n",
      "Train Loss:  0.677056278625544  Time:  1.5074498653411865\n",
      "Test Loss:  0.6692586811829586\n",
      "Test Acc:  56.989999999999995 %\n",
      "================================================== 33\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=66, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=66, out_features=120, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=68, out_features=36, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=36, out_features=11, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=11, out_features=120, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=120, out_features=89, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6887603067133549  Time:  1.4394490718841553\n",
      "Test Loss:  0.6783599433850269\n",
      "Test Acc:  56.035999999999994 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6812760943043841  Time:  1.449688196182251\n",
      "Test Loss:  0.6707808460508075\n",
      "Test Acc:  61.012 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6787837371773964  Time:  1.452500820159912\n",
      "Test Loss:  0.6683562036071505\n",
      "Test Acc:  59.036 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6791901723311765  Time:  1.510303020477295\n",
      "Test Loss:  0.6693907705496769\n",
      "Test Acc:  59.258 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6784580441286964  Time:  1.5234458446502686\n",
      "Test Loss:  0.6711142537545185\n",
      "Test Acc:  57.501999999999995 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6780915316874093  Time:  1.5031092166900635\n",
      "Test Loss:  0.6680452507369372\n",
      "Test Acc:  60.316 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6776596999516452  Time:  1.5180251598358154\n",
      "Test Loss:  0.6666823056887607\n",
      "Test Acc:  57.49999999999999 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6769516537659359  Time:  1.466609001159668\n",
      "Test Loss:  0.6688950216891815\n",
      "Test Acc:  58.714 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6777571280942346  Time:  1.4640476703643799\n",
      "Test Loss:  0.6690569225014472\n",
      "Test Acc:  61.224000000000004 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6776189166699013  Time:  1.4596869945526123\n",
      "Test Loss:  0.6679526713429665\n",
      "Test Acc:  60.221999999999994 %\n",
      "================================================== 34\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=101, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=101, out_features=28, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=28, out_features=67, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=67, out_features=15, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=15, out_features=76, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=76, out_features=40, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=40, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=91, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.688638199637406  Time:  1.4004700183868408\n",
      "Test Loss:  0.6747302945171084\n",
      "Test Acc:  56.068 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6812703592063737  Time:  1.4459829330444336\n",
      "Test Loss:  0.6713838352232563\n",
      "Test Acc:  56.65599999999999 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6786617699765811  Time:  1.4592390060424805\n",
      "Test Loss:  0.6694383636421087\n",
      "Test Acc:  57.754000000000005 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6792749020740063  Time:  1.454507827758789\n",
      "Test Loss:  0.6679607900429745\n",
      "Test Acc:  59.96 %\n",
      "================================================== 34\n",
      "Train Loss:  0.677368712468739  Time:  1.4642589092254639\n",
      "Test Loss:  0.666045253678244\n",
      "Test Acc:  58.962 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6768163159815934  Time:  1.4705028533935547\n",
      "Test Loss:  0.6661248322652311\n",
      "Test Acc:  59.467999999999996 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6775614162431146  Time:  1.525010347366333\n",
      "Test Loss:  0.6721001002861529\n",
      "Test Acc:  56.54 %\n",
      "================================================== 34\n",
      "Train Loss:  0.67671428588185  Time:  1.9933907985687256\n",
      "Test Loss:  0.6705446714649395\n",
      "Test Acc:  60.906000000000006 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6764773136942926  Time:  1.7609431743621826\n",
      "Test Loss:  0.6670628904688115\n",
      "Test Acc:  58.574000000000005 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6756413857432177  Time:  1.7084920406341553\n",
      "Test Loss:  0.6673715874856833\n",
      "Test Acc:  58.187999999999995 %\n",
      "================================================== 34\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=66, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=66, out_features=67, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=67, out_features=15, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=15, out_features=76, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=76, out_features=40, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=40, out_features=42, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=42, out_features=91, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=91, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6884795057512548  Time:  1.7011220455169678\n",
      "Test Loss:  0.6747566504137856\n",
      "Test Acc:  56.455999999999996 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6803703527816021  Time:  1.460960865020752\n",
      "Test Loss:  0.6718327722379139\n",
      "Test Acc:  56.804 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6793091334130642  Time:  1.4742751121520996\n",
      "Test Loss:  0.6683692947334173\n",
      "Test Acc:  57.833999999999996 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6786495255727838  Time:  1.4829168319702148\n",
      "Test Loss:  0.6673389396497181\n",
      "Test Acc:  58.594 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6790835426671662  Time:  1.43831205368042\n",
      "Test Loss:  0.6698049124406309\n",
      "Test Acc:  58.59799999999999 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6772913689160869  Time:  1.445702314376831\n",
      "Test Loss:  0.6704800901364307\n",
      "Test Acc:  57.472 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6764717802514125  Time:  1.4456260204315186\n",
      "Test Loss:  0.6694683277485322\n",
      "Test Acc:  57.11000000000001 %\n",
      "================================================== 34\n",
      "Train Loss:  0.676522011521959  Time:  1.4489717483520508\n",
      "Test Loss:  0.6696568116241571\n",
      "Test Acc:  58.123999999999995 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6770072446687379  Time:  1.4462740421295166\n",
      "Test Loss:  0.6666574213577776\n",
      "Test Acc:  59.992000000000004 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6764697476025046  Time:  1.5080370903015137\n",
      "Test Loss:  0.6677236745552141\n",
      "Test Acc:  58.087999999999994 %\n",
      "================================================== 34\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=16, out_features=77, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=77, out_features=5, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=5, out_features=54, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=54, out_features=85, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=85, out_features=52, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=52, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6922664875096648  Time:  1.3134539127349854\n",
      "Test Loss:  0.6848146410620942\n",
      "Test Acc:  53.87 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6847674246252018  Time:  1.2984890937805176\n",
      "Test Loss:  0.6761387659578907\n",
      "Test Acc:  57.489999999999995 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6820307763388557  Time:  1.2891600131988525\n",
      "Test Loss:  0.6741781231700158\n",
      "Test Acc:  59.05200000000001 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6814236042708376  Time:  1.2530088424682617\n",
      "Test Loss:  0.6743317258601286\n",
      "Test Acc:  58.284000000000006 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6804199858303488  Time:  1.270707130432129\n",
      "Test Loss:  0.672943757504833\n",
      "Test Acc:  58.9 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6798557130089642  Time:  1.2793340682983398\n",
      "Test Loss:  0.6713733368990372\n",
      "Test Acc:  57.694 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6794017622940731  Time:  1.278592824935913\n",
      "Test Loss:  0.6698010633794629\n",
      "Test Acc:  58.550000000000004 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6794322587712838  Time:  1.3292558193206787\n",
      "Test Loss:  0.6730699733811982\n",
      "Test Acc:  55.85 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6801867828752003  Time:  1.314521074295044\n",
      "Test Loss:  0.6713284056405632\n",
      "Test Acc:  57.74 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6794708012229335  Time:  1.3203151226043701\n",
      "Test Loss:  0.6703510120206949\n",
      "Test Acc:  58.426 %\n",
      "================================================== 34\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=89, out_features=66, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=66, out_features=27, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=27, out_features=124, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=124, out_features=94, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=94, out_features=50, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=50, out_features=127, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=127, out_features=114, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=114, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6907583468586859  Time:  1.5354671478271484\n",
      "Test Loss:  0.6832914829862361\n",
      "Test Acc:  54.006 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6817227767331757  Time:  1.5492260456085205\n",
      "Test Loss:  0.6703921672032804\n",
      "Test Acc:  57.274 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6798135586028552  Time:  1.5020971298217773\n",
      "Test Loss:  0.6714053601026535\n",
      "Test Acc:  58.233999999999995 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6791660937949688  Time:  1.5264461040496826\n",
      "Test Loss:  0.6686653595189659\n",
      "Test Acc:  56.910000000000004 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6773065078432543  Time:  1.5356578826904297\n",
      "Test Loss:  0.6654859255163037\n",
      "Test Acc:  58.158 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6782321923405584  Time:  1.5484528541564941\n",
      "Test Loss:  0.6703581074062659\n",
      "Test Acc:  57.166 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6775969177702047  Time:  1.5764260292053223\n",
      "Test Loss:  0.6690896904590179\n",
      "Test Acc:  59.099999999999994 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6783931555539152  Time:  1.600508213043213\n",
      "Test Loss:  0.6697089918413941\n",
      "Test Acc:  57.708000000000006 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6769975158419922  Time:  1.6018280982971191\n",
      "Test Loss:  0.6692852134607277\n",
      "Test Acc:  57.582 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6769518961001487  Time:  1.6112887859344482\n",
      "Test Loss:  0.6668213648455483\n",
      "Test Acc:  58.248 %\n",
      "================================================== 35\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=109, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=109, out_features=30, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=30, out_features=120, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=68, out_features=36, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=36, out_features=11, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=11, out_features=120, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=120, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6879575387404783  Time:  1.4801819324493408\n",
      "Test Loss:  0.6732847073248455\n",
      "Test Acc:  57.424 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6789722873346649  Time:  1.4351780414581299\n",
      "Test Loss:  0.6716832184061712\n",
      "Test Acc:  56.035999999999994 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6797920210953177  Time:  1.4469168186187744\n",
      "Test Loss:  0.6703389405596013\n",
      "Test Acc:  56.974000000000004 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6764478207069592  Time:  1.4477152824401855\n",
      "Test Loss:  0.6651799018893924\n",
      "Test Acc:  59.728 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6775867636621433  Time:  1.4586122035980225\n",
      "Test Loss:  0.6677187006084286\n",
      "Test Acc:  59.618 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6783943050099115  Time:  1.4908642768859863\n",
      "Test Loss:  0.6698726707575272\n",
      "Test Acc:  58.214 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6781898530295295  Time:  1.4947319030761719\n",
      "Test Loss:  0.6687587341483758\n",
      "Test Acc:  57.494 %\n",
      "================================================== 35\n",
      "Train Loss:  0.676333091989921  Time:  1.4962937831878662\n",
      "Test Loss:  0.668432784627895\n",
      "Test Acc:  59.297999999999995 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6761108184382864  Time:  1.4880177974700928\n",
      "Test Loss:  0.6675601130237385\n",
      "Test Acc:  59.288 %\n",
      "================================================== 35\n",
      "Train Loss:  0.677646712447605  Time:  1.4637999534606934\n",
      "Test Loss:  0.6660611991371427\n",
      "Test Acc:  58.906000000000006 %\n",
      "================================================== 35\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=89, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=89, out_features=66, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=66, out_features=120, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=68, out_features=36, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=36, out_features=11, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=11, out_features=120, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=120, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6895285701229624  Time:  1.44679594039917\n",
      "Test Loss:  0.6772391355159332\n",
      "Test Acc:  60.092 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6804133279480203  Time:  1.4451758861541748\n",
      "Test Loss:  0.6678372210994059\n",
      "Test Acc:  58.908 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6795376071529667  Time:  1.4684181213378906\n",
      "Test Loss:  0.6656283042868789\n",
      "Test Acc:  59.426 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6795578372739527  Time:  1.5380401611328125\n",
      "Test Loss:  0.6689684689044952\n",
      "Test Acc:  57.077999999999996 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6776644654952697  Time:  1.5301852226257324\n",
      "Test Loss:  0.6666168427588989\n",
      "Test Acc:  60.5 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6777079349016621  Time:  1.542707920074463\n",
      "Test Loss:  0.6700448810446019\n",
      "Test Acc:  57.864000000000004 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6784584241626906  Time:  1.5413308143615723\n",
      "Test Loss:  0.6681510258694084\n",
      "Test Acc:  57.809999999999995 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6773770240536572  Time:  1.5352778434753418\n",
      "Test Loss:  0.6674053982204321\n",
      "Test Acc:  58.757999999999996 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6769104715246353  Time:  1.4964559078216553\n",
      "Test Loss:  0.6657245828180897\n",
      "Test Acc:  59.062000000000005 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6773416615315597  Time:  1.492506980895996\n",
      "Test Loss:  0.6654808068153809\n",
      "Test Acc:  57.682 %\n",
      "================================================== 35\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=105, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=105, out_features=65, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=65, out_features=113, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=113, out_features=98, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=98, out_features=47, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=47, out_features=132, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=132, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6907052397727966  Time:  1.3339459896087646\n",
      "Test Loss:  0.6775322480469333\n",
      "Test Acc:  56.257999999999996 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6801321680528404  Time:  1.337470293045044\n",
      "Test Loss:  0.6708180074180875\n",
      "Test Acc:  57.87 %\n",
      "================================================== 35\n",
      "Train Loss:  0.67926593418539  Time:  1.4157428741455078\n",
      "Test Loss:  0.6711376190793757\n",
      "Test Acc:  57.3 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6776860565164663  Time:  1.4210872650146484\n",
      "Test Loss:  0.6661949823705517\n",
      "Test Acc:  58.736 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6773407872575913  Time:  1.410897970199585\n",
      "Test Loss:  0.665943977480032\n",
      "Test Acc:  59.78 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6764111101192279  Time:  1.3839507102966309\n",
      "Test Loss:  0.6666808590597036\n",
      "Test Acc:  59.65200000000001 %\n",
      "================================================== 35\n",
      "Train Loss:  0.678003541309468  Time:  1.3955531120300293\n",
      "Test Loss:  0.6657086473946668\n",
      "Test Acc:  60.376 %\n",
      "================================================== 35\n",
      "Train Loss:  0.676997031391102  Time:  1.34706711769104\n",
      "Test Loss:  0.6639248184403594\n",
      "Test Acc:  59.902 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6772150077523976  Time:  1.3552348613739014\n",
      "Test Loss:  0.66847284442308\n",
      "Test Acc:  59.242 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6765062491389087  Time:  1.352334976196289\n",
      "Test Loss:  0.6689715029633775\n",
      "Test Acc:  56.525999999999996 %\n",
      "================================================== 35\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=109, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=109, out_features=30, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=30, out_features=48, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=48, out_features=73, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=73, out_features=139, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=139, out_features=41, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=41, out_features=130, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=130, out_features=116, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=116, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6894075083036493  Time:  1.5107829570770264\n",
      "Test Loss:  0.6730194429353792\n",
      "Test Acc:  57.032000000000004 %\n",
      "================================================== 36\n",
      "Train Loss:  0.679580583407061  Time:  1.5709059238433838\n",
      "Test Loss:  0.6666590607896143\n",
      "Test Acc:  60.004000000000005 %\n",
      "================================================== 36\n",
      "Train Loss:  0.678843550003358  Time:  1.616675853729248\n",
      "Test Loss:  0.6694772800012511\n",
      "Test Acc:  57.75599999999999 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6797071680100295  Time:  1.6708228588104248\n",
      "Test Loss:  0.6682549605564195\n",
      "Test Acc:  59.754 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6774362662847895  Time:  1.6731760501861572\n",
      "Test Loss:  0.6704274850840471\n",
      "Test Acc:  56.826 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6776218338169321  Time:  1.6015889644622803\n",
      "Test Loss:  0.667189068027905\n",
      "Test Acc:  58.168 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6768698553099249  Time:  1.593925952911377\n",
      "Test Loss:  0.6682422434797093\n",
      "Test Acc:  58.45 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6769837754486251  Time:  1.6372301578521729\n",
      "Test Loss:  0.6670292618931556\n",
      "Test Acc:  58.114 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6761838434821498  Time:  1.5998449325561523\n",
      "Test Loss:  0.6697106923984022\n",
      "Test Acc:  59.998 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6760903992792116  Time:  1.6482887268066406\n",
      "Test Loss:  0.6706843774537651\n",
      "Test Acc:  57.13 %\n",
      "================================================== 36\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=84, out_features=122, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=122, out_features=120, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=68, out_features=36, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=36, out_features=11, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=11, out_features=120, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=120, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6930109867214287  Time:  1.5360751152038574\n",
      "Test Loss:  0.6845932675867664\n",
      "Test Acc:  55.738 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6822638883642906  Time:  1.597203016281128\n",
      "Test Loss:  0.6723232241917629\n",
      "Test Acc:  58.362 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6800083339649395  Time:  1.593925952911377\n",
      "Test Loss:  0.6685707645148647\n",
      "Test Acc:  58.355999999999995 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6792537600454622  Time:  1.6250088214874268\n",
      "Test Loss:  0.6671239228881135\n",
      "Test Acc:  59.738 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6781436093967327  Time:  1.5548732280731201\n",
      "Test Loss:  0.6699264615165944\n",
      "Test Acc:  56.254000000000005 %\n",
      "================================================== 36\n",
      "Train Loss:  0.677954190838946  Time:  1.5514287948608398\n",
      "Test Loss:  0.6674860433048132\n",
      "Test Acc:  57.446 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6777110384763593  Time:  1.597486972808838\n",
      "Test Loss:  0.6655680020244754\n",
      "Test Acc:  59.568 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6773994175622063  Time:  1.570755958557129\n",
      "Test Loss:  0.6669163938079562\n",
      "Test Acc:  59.696000000000005 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6767548568492389  Time:  1.628302812576294\n",
      "Test Loss:  0.6650955698319844\n",
      "Test Acc:  58.406000000000006 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6762326448938273  Time:  1.650949239730835\n",
      "Test Loss:  0.6690494874302222\n",
      "Test Acc:  59.907999999999994 %\n",
      "================================================== 36\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=109, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=109, out_features=30, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=30, out_features=120, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=68, out_features=36, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=36, out_features=11, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=11, out_features=120, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=120, out_features=89, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6880566710538237  Time:  1.4973158836364746\n",
      "Test Loss:  0.6690308302640915\n",
      "Test Acc:  59.216 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6812923398331134  Time:  1.5142531394958496\n",
      "Test Loss:  0.6691027709415981\n",
      "Test Acc:  60.128 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6797596731760206  Time:  1.4780731201171875\n",
      "Test Loss:  0.6700087521149187\n",
      "Test Acc:  61.122 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6790675675346903  Time:  1.4451980590820312\n",
      "Test Loss:  0.6692854610024667\n",
      "Test Acc:  57.84 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6777579451564455  Time:  1.4413807392120361\n",
      "Test Loss:  0.670345991545794\n",
      "Test Acc:  59.58 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6777256340005972  Time:  1.4344689846038818\n",
      "Test Loss:  0.6644241800721811\n",
      "Test Acc:  59.594 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6769128829023264  Time:  1.4260540008544922\n",
      "Test Loss:  0.668081958987275\n",
      "Test Acc:  59.784000000000006 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6772923758865272  Time:  1.4970362186431885\n",
      "Test Loss:  0.6651188457498745\n",
      "Test Acc:  60.882000000000005 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6781189935485812  Time:  1.5164949893951416\n",
      "Test Loss:  0.6686717697552272\n",
      "Test Acc:  57.728 %\n",
      "================================================== 36\n",
      "Train Loss:  0.677578311549486  Time:  1.501405954360962\n",
      "Test Loss:  0.6690048423348641\n",
      "Test Acc:  57.965999999999994 %\n",
      "================================================== 36\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=125, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=125, out_features=119, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=119, out_features=80, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=80, out_features=58, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=58, out_features=127, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=127, out_features=39, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=39, out_features=94, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=94, out_features=64, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6876185836583159  Time:  1.5841960906982422\n",
      "Test Loss:  0.6708823764810756\n",
      "Test Acc:  56.828 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6812375270537216  Time:  1.5196678638458252\n",
      "Test Loss:  0.6709813025532937\n",
      "Test Acc:  59.792 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6801781595623406  Time:  1.5460050106048584\n",
      "Test Loss:  0.6728837258961736\n",
      "Test Acc:  55.501999999999995 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6796723027733991  Time:  1.5810821056365967\n",
      "Test Loss:  0.6702922996209593\n",
      "Test Acc:  56.812 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6779574786659575  Time:  1.5737969875335693\n",
      "Test Loss:  0.6681553335214148\n",
      "Test Acc:  58.013999999999996 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6774017802120125  Time:  1.647707223892212\n",
      "Test Loss:  0.6701975197208171\n",
      "Test Acc:  55.876000000000005 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6774163842201233  Time:  1.6606512069702148\n",
      "Test Loss:  0.6685928501644913\n",
      "Test Acc:  57.18 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6764361199236264  Time:  1.6586229801177979\n",
      "Test Loss:  0.6681415976918473\n",
      "Test Acc:  58.974000000000004 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6774165599885649  Time:  1.6631379127502441\n",
      "Test Loss:  0.667015803711755\n",
      "Test Acc:  59.998 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6768998558068797  Time:  1.6482021808624268\n",
      "Test Loss:  0.6660015257645626\n",
      "Test Acc:  57.424 %\n",
      "================================================== 36\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=84, out_features=122, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=122, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=130, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=12, out_features=66, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=66, out_features=17, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=17, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6895687860729051  Time:  1.4273710250854492\n",
      "Test Loss:  0.6744543563346473\n",
      "Test Acc:  59.653999999999996 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6799537389382829  Time:  1.4336059093475342\n",
      "Test Loss:  0.6717148490098058\n",
      "Test Acc:  57.728 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6785037234751847  Time:  1.4288592338562012\n",
      "Test Loss:  0.6696103422009215\n",
      "Test Acc:  60.087999999999994 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6781477656242622  Time:  1.4552199840545654\n",
      "Test Loss:  0.6701431128443504\n",
      "Test Acc:  58.550000000000004 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6787982915874815  Time:  1.5076360702514648\n",
      "Test Loss:  0.6681804255563386\n",
      "Test Acc:  59.34 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6772959362851442  Time:  1.535005807876587\n",
      "Test Loss:  0.6693697182499633\n",
      "Test Acc:  58.936 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6781925017381236  Time:  1.523205041885376\n",
      "Test Loss:  0.6688678653872743\n",
      "Test Acc:  57.02 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6771908896247836  Time:  1.5347099304199219\n",
      "Test Loss:  0.6693321098478473\n",
      "Test Acc:  57.894 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6773676195718946  Time:  1.487562894821167\n",
      "Test Loss:  0.666927518589156\n",
      "Test Acc:  57.964000000000006 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6767347304925432  Time:  1.4503629207611084\n",
      "Test Loss:  0.669446769721654\n",
      "Test Acc:  59.855999999999995 %\n",
      "================================================== 37\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=139, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=139, out_features=68, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=68, out_features=120, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=68, out_features=36, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=36, out_features=11, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=11, out_features=120, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=120, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6876306738296565  Time:  1.4635212421417236\n",
      "Test Loss:  0.6741415268304397\n",
      "Test Acc:  56.914 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6798457768711731  Time:  1.4730420112609863\n",
      "Test Loss:  0.6744962556629764\n",
      "Test Acc:  60.019999999999996 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6784429012858955  Time:  1.5047388076782227\n",
      "Test Loss:  0.6705924445877269\n",
      "Test Acc:  58.29 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6798404442132825  Time:  1.5629680156707764\n",
      "Test Loss:  0.6700719144879556\n",
      "Test Acc:  58.06 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6782714413465375  Time:  1.5582592487335205\n",
      "Test Loss:  0.6684241127602908\n",
      "Test Acc:  57.594 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6780848905552913  Time:  1.5897891521453857\n",
      "Test Loss:  0.6690477570708917\n",
      "Test Acc:  57.306000000000004 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6774598531914453  Time:  1.532121181488037\n",
      "Test Loss:  0.6683004683986002\n",
      "Test Acc:  57.233999999999995 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6770403787602474  Time:  1.4968619346618652\n",
      "Test Loss:  0.6692840910079528\n",
      "Test Acc:  59.746 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6758410247573017  Time:  1.4936587810516357\n",
      "Test Loss:  0.6688265989021379\n",
      "Test Acc:  58.904 %\n",
      "================================================== 37\n",
      "Train Loss:  0.676852282598941  Time:  1.5038988590240479\n",
      "Test Loss:  0.6649428773291257\n",
      "Test Acc:  58.988 %\n",
      "================================================== 37\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=84, out_features=122, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=122, out_features=120, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=120, out_features=68, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=68, out_features=36, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=36, out_features=11, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=11, out_features=120, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=120, out_features=89, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=89, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6899275655729057  Time:  1.520009994506836\n",
      "Test Loss:  0.6722997992622609\n",
      "Test Acc:  58.843999999999994 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6800255388239004  Time:  1.605506181716919\n",
      "Test Loss:  0.6706607925648592\n",
      "Test Acc:  57.908 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6801595924979579  Time:  1.6132688522338867\n",
      "Test Loss:  0.6706971915400758\n",
      "Test Acc:  56.586000000000006 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6775906718995449  Time:  1.6444778442382812\n",
      "Test Loss:  0.6719664660643558\n",
      "Test Acc:  59.148 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6777251110459767  Time:  1.608821153640747\n",
      "Test Loss:  0.6660155310314528\n",
      "Test Acc:  59.324 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6783212325433745  Time:  1.594844102859497\n",
      "Test Loss:  0.6673969398347699\n",
      "Test Acc:  59.45399999999999 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6764626757506906  Time:  1.5570778846740723\n",
      "Test Loss:  0.6722936818794328\n",
      "Test Acc:  58.742000000000004 %\n",
      "================================================== 37\n",
      "Train Loss:  0.676536847640128  Time:  1.5691609382629395\n",
      "Test Loss:  0.6697989924221622\n",
      "Test Acc:  58.42 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6756546425558355  Time:  1.593968152999878\n",
      "Test Loss:  0.6683912964499726\n",
      "Test Acc:  58.984 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6760087498348125  Time:  1.5975229740142822\n",
      "Test Loss:  0.6695474003042493\n",
      "Test Acc:  57.726 %\n",
      "================================================== 37\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=100, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=100, out_features=80, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=80, out_features=93, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=93, out_features=72, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=72, out_features=11, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=11, out_features=42, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=42, out_features=112, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=112, out_features=63, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=63, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6912959513873079  Time:  1.4977989196777344\n",
      "Test Loss:  0.6760004430401082\n",
      "Test Acc:  55.42399999999999 %\n",
      "================================================== 37\n",
      "Train Loss:  0.681382453572141  Time:  1.4914212226867676\n",
      "Test Loss:  0.6682693261881264\n",
      "Test Acc:  57.046 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6800667328121018  Time:  1.5082166194915771\n",
      "Test Loss:  0.6672785747416166\n",
      "Test Acc:  57.422 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6784633417634198  Time:  1.4731662273406982\n",
      "Test Loss:  0.6684255304993415\n",
      "Test Acc:  57.236 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6768572537133294  Time:  1.4991240501403809\n",
      "Test Loss:  0.6668821588462713\n",
      "Test Acc:  59.868 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6776838520147505  Time:  1.441884994506836\n",
      "Test Loss:  0.6644642599383179\n",
      "Test Acc:  61.065999999999995 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6782762166792459  Time:  1.4631130695343018\n",
      "Test Loss:  0.6678053529895082\n",
      "Test Acc:  60.784000000000006 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6775862052492851  Time:  1.4604177474975586\n",
      "Test Loss:  0.670052100809253\n",
      "Test Acc:  59.98 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6759561813660782  Time:  1.463838815689087\n",
      "Test Loss:  0.6669733302325619\n",
      "Test Acc:  60.728 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6766990388396883  Time:  1.5277581214904785\n",
      "Test Loss:  0.6673033456413113\n",
      "Test Acc:  57.699999999999996 %\n",
      "================================================== 37\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=84, out_features=122, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=122, out_features=94, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=94, out_features=4, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=4, out_features=65, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=65, out_features=101, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=101, out_features=140, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=140, out_features=45, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=45, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6884045452967177  Time:  1.6050291061401367\n",
      "Test Loss:  0.6725739334919014\n",
      "Test Acc:  56.862 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6800178920265532  Time:  1.5721111297607422\n",
      "Test Loss:  0.670470765354682\n",
      "Test Acc:  56.894 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6789848948047109  Time:  1.5559420585632324\n",
      "Test Loss:  0.6688634668077741\n",
      "Test Acc:  60.926 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6794521499724284  Time:  1.5033478736877441\n",
      "Test Loss:  0.6716419053929192\n",
      "Test Acc:  57.111999999999995 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6785419143899514  Time:  1.5189242362976074\n",
      "Test Loss:  0.6671007004927616\n",
      "Test Acc:  57.599999999999994 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6780971623685238  Time:  1.5208828449249268\n",
      "Test Loss:  0.6694213863538236\n",
      "Test Acc:  57.446 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6782880738703874  Time:  1.5513741970062256\n",
      "Test Loss:  0.667410783925835\n",
      "Test Acc:  57.48 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6760158693268351  Time:  1.5868568420410156\n",
      "Test Loss:  0.6709101248760613\n",
      "Test Acc:  59.056 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6765431846580366  Time:  1.581679105758667\n",
      "Test Loss:  0.6702721070270149\n",
      "Test Acc:  55.992 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6767785731893387  Time:  1.5777509212493896\n",
      "Test Loss:  0.6699040191514152\n",
      "Test Acc:  58.55200000000001 %\n",
      "================================================== 38\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=83, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=83, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=130, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=12, out_features=66, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=66, out_features=17, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=17, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6896613353360308  Time:  1.432647943496704\n",
      "Test Loss:  0.6752789650036364\n",
      "Test Acc:  58.474000000000004 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6816216868205662  Time:  1.400273084640503\n",
      "Test Loss:  0.671223163300631\n",
      "Test Acc:  56.784 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6799997680813726  Time:  1.3656229972839355\n",
      "Test Loss:  0.666377882872309\n",
      "Test Acc:  60.014 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6786101862896968  Time:  1.3863461017608643\n",
      "Test Loss:  0.6666198439744054\n",
      "Test Acc:  59.696000000000005 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6776266646211164  Time:  1.3924939632415771\n",
      "Test Loss:  0.6688148090425803\n",
      "Test Acc:  57.304 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6770543815010656  Time:  1.418921947479248\n",
      "Test Loss:  0.6676504979936444\n",
      "Test Acc:  59.754 %\n",
      "================================================== 38\n",
      "Train Loss:  0.677669222119951  Time:  1.456341028213501\n",
      "Test Loss:  0.6683188254127697\n",
      "Test Acc:  58.386 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6768540398482858  Time:  1.4631130695343018\n",
      "Test Loss:  0.6685301241826038\n",
      "Test Acc:  58.733999999999995 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6770718202103664  Time:  1.4589300155639648\n",
      "Test Loss:  0.6663911452098769\n",
      "Test Acc:  57.434 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6769836866942636  Time:  1.4399280548095703\n",
      "Test Loss:  0.6652984351527934\n",
      "Test Acc:  59.924 %\n",
      "================================================== 38\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=84, out_features=122, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=122, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=130, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=12, out_features=66, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=66, out_features=17, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=17, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6908887270158225  Time:  1.4538497924804688\n",
      "Test Loss:  0.675927284420753\n",
      "Test Acc:  58.472 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6832980300388197  Time:  1.4070799350738525\n",
      "Test Loss:  0.6702530377981614\n",
      "Test Acc:  58.79 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6805777858643636  Time:  1.4359018802642822\n",
      "Test Loss:  0.6701896975235063\n",
      "Test Acc:  58.14 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6792449152817691  Time:  1.4919390678405762\n",
      "Test Loss:  0.6706738882527059\n",
      "Test Acc:  57.644 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6775114679858633  Time:  1.4840540885925293\n",
      "Test Loss:  0.6706990255993239\n",
      "Test Acc:  57.638 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6772006106202619  Time:  1.5286211967468262\n",
      "Test Loss:  0.6694557149799503\n",
      "Test Acc:  57.156 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6778405578467097  Time:  1.5401947498321533\n",
      "Test Loss:  0.6675291663529922\n",
      "Test Acc:  58.878 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6773459504555611  Time:  1.536142110824585\n",
      "Test Loss:  0.6686569631707912\n",
      "Test Acc:  56.728 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6766485206402131  Time:  1.4898841381072998\n",
      "Test Loss:  0.6683817289921702\n",
      "Test Acc:  57.524 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6781981404680405  Time:  1.4583368301391602\n",
      "Test Loss:  0.6658825284364273\n",
      "Test Acc:  59.141999999999996 %\n",
      "================================================== 38\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=126, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=126, out_features=52, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=52, out_features=15, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=15, out_features=72, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=72, out_features=140, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=140, out_features=133, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=133, out_features=98, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=98, out_features=40, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=40, out_features=71, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=71, out_features=24, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=24, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6907013545941262  Time:  1.6372358798980713\n",
      "Test Loss:  0.6735041466902714\n",
      "Test Acc:  59.012 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6814946416955795  Time:  1.6863889694213867\n",
      "Test Loss:  0.6721443342311042\n",
      "Test Acc:  59.577999999999996 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6796716597393482  Time:  1.7051682472229004\n",
      "Test Loss:  0.6659332559425004\n",
      "Test Acc:  59.940000000000005 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6791926130761196  Time:  1.7947981357574463\n",
      "Test Loss:  0.6725533744510339\n",
      "Test Acc:  56.574000000000005 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6783648242045494  Time:  1.7733078002929688\n",
      "Test Loss:  0.6654259176278601\n",
      "Test Acc:  60.082 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6780233293989278  Time:  1.7999589443206787\n",
      "Test Loss:  0.666573339883162\n",
      "Test Acc:  58.792 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6783180019281206  Time:  1.785674810409546\n",
      "Test Loss:  0.66830960554736\n",
      "Test Acc:  58.611999999999995 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6776139285877674  Time:  1.7409279346466064\n",
      "Test Loss:  0.6634839332225372\n",
      "Test Acc:  61.556 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6761046302144544  Time:  1.7683601379394531\n",
      "Test Loss:  0.6673693334569737\n",
      "Test Acc:  59.278 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6770288487420465  Time:  1.7447509765625\n",
      "Test Loss:  0.6690080323997809\n",
      "Test Acc:  59.778 %\n",
      "================================================== 38\n",
      "results 1\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=83, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=83, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=51, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=51, out_features=40, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=40, out_features=128, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=128, out_features=61, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=61, out_features=52, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=52, out_features=15, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=15, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6907784590755937  Time:  1.4017698764801025\n",
      "Test Loss:  0.686754899973772\n",
      "Test Acc:  54.322 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6808073970958264  Time:  1.3947582244873047\n",
      "Test Loss:  0.6712069584398853\n",
      "Test Acc:  57.099999999999994 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6796828504026371  Time:  1.449249029159546\n",
      "Test Loss:  0.6725003205391825\n",
      "Test Acc:  56.666000000000004 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6778060920047064  Time:  1.467513084411621\n",
      "Test Loss:  0.6700480184992965\n",
      "Test Acc:  56.686 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6779615437462382  Time:  1.4701392650604248\n",
      "Test Loss:  0.6671352307407223\n",
      "Test Acc:  60.848 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6782520415574095  Time:  1.4340407848358154\n",
      "Test Loss:  0.6703242410202416\n",
      "Test Acc:  57.733999999999995 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6785888093231368  Time:  1.4447031021118164\n",
      "Test Loss:  0.667990471635546\n",
      "Test Acc:  58.818000000000005 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6767048459418499  Time:  1.4041578769683838\n",
      "Test Loss:  0.6670835288811703\n",
      "Test Acc:  59.47 %\n",
      "================================================== 39\n",
      "Train Loss:  0.677376897665706  Time:  1.4259939193725586\n",
      "Test Loss:  0.6660496045132073\n",
      "Test Acc:  60.136 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6767805353568418  Time:  1.4309217929840088\n",
      "Test Loss:  0.6681263443766808\n",
      "Test Acc:  57.965999999999994 %\n",
      "================================================== 39\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=134, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=134, out_features=100, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=100, out_features=130, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=12, out_features=66, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=66, out_features=17, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=17, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6923519723171735  Time:  1.4433510303497314\n",
      "Test Loss:  0.6868188986364676\n",
      "Test Acc:  53.112 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6871617181892813  Time:  1.4844591617584229\n",
      "Test Loss:  0.6828135759854803\n",
      "Test Acc:  54.068000000000005 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6839364549539385  Time:  1.4843268394470215\n",
      "Test Loss:  0.6830357623343565\n",
      "Test Acc:  55.035999999999994 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6831335708172652  Time:  1.4762592315673828\n",
      "Test Loss:  0.68071470424837\n",
      "Test Acc:  56.12200000000001 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6825779898323282  Time:  1.4571659564971924\n",
      "Test Loss:  0.6789909072068273\n",
      "Test Acc:  55.32 %\n",
      "================================================== 39\n",
      "Train Loss:  0.682466662713211  Time:  1.422227144241333\n",
      "Test Loss:  0.6782986546049312\n",
      "Test Acc:  55.24400000000001 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6829689353921987  Time:  1.416193962097168\n",
      "Test Loss:  0.6780656512294497\n",
      "Test Acc:  54.752 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6819830256221938  Time:  1.4424350261688232\n",
      "Test Loss:  0.6752069936114915\n",
      "Test Acc:  56.438 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6820006422752881  Time:  1.4384920597076416\n",
      "Test Loss:  0.6757746040821075\n",
      "Test Acc:  56.525999999999996 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6809708974222197  Time:  1.4867429733276367\n",
      "Test Loss:  0.6791248783773306\n",
      "Test Acc:  55.806 %\n",
      "================================================== 39\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=83, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=83, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=130, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=12, out_features=66, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=66, out_features=17, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=17, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6914848200161091  Time:  1.4501979351043701\n",
      "Test Loss:  0.6786870658397675\n",
      "Test Acc:  57.544 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6826769998908913  Time:  1.4414043426513672\n",
      "Test Loss:  0.6691355352499047\n",
      "Test Acc:  57.884 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6806333719813911  Time:  1.457996129989624\n",
      "Test Loss:  0.6716125741296884\n",
      "Test Acc:  57.089999999999996 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6797041618911019  Time:  1.4541511535644531\n",
      "Test Loss:  0.6686993505881758\n",
      "Test Acc:  57.38799999999999 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6785611007770482  Time:  1.3754000663757324\n",
      "Test Loss:  0.6676726447684425\n",
      "Test Acc:  57.294 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6764046493237906  Time:  1.3865160942077637\n",
      "Test Loss:  0.6678396691473163\n",
      "Test Acc:  58.372 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6757557607480209  Time:  1.3869860172271729\n",
      "Test Loss:  0.666924096491872\n",
      "Test Acc:  57.918000000000006 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6771567361633273  Time:  1.4193599224090576\n",
      "Test Loss:  0.6653495245442098\n",
      "Test Acc:  60.065999999999995 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6773081750330263  Time:  1.4675068855285645\n",
      "Test Loss:  0.6675148831338299\n",
      "Test Acc:  57.052 %\n",
      "================================================== 39\n",
      "Train Loss:  0.67665599297433  Time:  1.463623046875\n",
      "Test Loss:  0.6658488490751812\n",
      "Test Acc:  58.44200000000001 %\n",
      "================================================== 39\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=57, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=57, out_features=122, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=122, out_features=133, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=133, out_features=65, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=65, out_features=46, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=46, out_features=133, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=133, out_features=116, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=116, out_features=56, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=56, out_features=48, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=48, out_features=96, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=96, out_features=130, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=130, out_features=108, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=108, out_features=95, bias=True)\n",
      "    (25): LeakyReLU(negative_slope=0.01)\n",
      "    (26): Linear(in_features=95, out_features=103, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=103, out_features=36, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=36, out_features=39, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Linear(in_features=39, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932399638377837  Time:  2.584138870239258\n",
      "Test Loss:  0.6931783903618248\n",
      "Test Acc:  50.056 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6931979253779362  Time:  2.6038620471954346\n",
      "Test Loss:  0.693209842151525\n",
      "Test Acc:  49.944 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6931920917364802  Time:  2.67008900642395\n",
      "Test Loss:  0.6932058735769622\n",
      "Test Acc:  49.944 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6931887282072192  Time:  2.625952959060669\n",
      "Test Loss:  0.6931735806319178\n",
      "Test Acc:  50.056 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6932142938140535  Time:  2.624129056930542\n",
      "Test Loss:  0.6931998161028843\n",
      "Test Acc:  49.944 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6932011348922757  Time:  2.560906171798706\n",
      "Test Loss:  0.6931535160663177\n",
      "Test Acc:  49.944 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6932233539375946  Time:  2.5682551860809326\n",
      "Test Loss:  0.6931462412586018\n",
      "Test Acc:  50.056 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6931896100949196  Time:  2.6658241748809814\n",
      "Test Loss:  0.6931827746483744\n",
      "Test Acc:  50.056 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6931780393976364  Time:  2.682337999343872\n",
      "Test Loss:  0.6932944801388955\n",
      "Test Acc:  49.944 %\n",
      "================================================== 39\n",
      "Train Loss:  0.693213627286201  Time:  2.695755958557129\n",
      "Test Loss:  0.693146786215354\n",
      "Test Acc:  49.944 %\n",
      "================================================== 39\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=83, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=83, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=122, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=122, out_features=104, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=104, out_features=93, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=93, out_features=67, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=67, out_features=66, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=66, out_features=12, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=12, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6904883815424285  Time:  1.5034668445587158\n",
      "Test Loss:  0.6810032913879472\n",
      "Test Acc:  54.608000000000004 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6828193105485317  Time:  1.4518859386444092\n",
      "Test Loss:  0.6742649558855562\n",
      "Test Acc:  57.006 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6805984873841279  Time:  1.4775218963623047\n",
      "Test Loss:  0.6729811728000641\n",
      "Test Acc:  59.964 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6799309495156699  Time:  1.503937005996704\n",
      "Test Loss:  0.6712879462509739\n",
      "Test Acc:  58.336 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6790165370398195  Time:  1.501288890838623\n",
      "Test Loss:  0.6688899844884872\n",
      "Test Acc:  56.992 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6780030934044915  Time:  1.5313031673431396\n",
      "Test Loss:  0.6689593825413256\n",
      "Test Acc:  57.26599999999999 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6779895020227362  Time:  1.47316312789917\n",
      "Test Loss:  0.6686130184300092\n",
      "Test Acc:  57.02 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6780564774126903  Time:  1.4978268146514893\n",
      "Test Loss:  0.6709479872061281\n",
      "Test Acc:  58.666 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6778007806652654  Time:  1.5218498706817627\n",
      "Test Loss:  0.6664409795585944\n",
      "Test Acc:  59.128 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6775291179218431  Time:  1.5355250835418701\n",
      "Test Loss:  0.6660251492748455\n",
      "Test Acc:  59.984 %\n",
      "================================================== 40\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=100, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=100, out_features=34, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=34, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=130, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=12, out_features=66, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=66, out_features=17, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=17, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.689642919458612  Time:  1.4179589748382568\n",
      "Test Loss:  0.6773291652908131\n",
      "Test Acc:  57.656 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6824124427172389  Time:  1.415329933166504\n",
      "Test Loss:  0.6741514923621197\n",
      "Test Acc:  58.242000000000004 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6795801814020115  Time:  1.413125991821289\n",
      "Test Loss:  0.6703721737983276\n",
      "Test Acc:  57.068 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6786150162237404  Time:  1.394092082977295\n",
      "Test Loss:  0.6713072794432543\n",
      "Test Acc:  58.32000000000001 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6789890225351292  Time:  1.4483709335327148\n",
      "Test Loss:  0.6690848031822516\n",
      "Test Acc:  58.544 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6780392742940109  Time:  1.4580061435699463\n",
      "Test Loss:  0.6677569917270115\n",
      "Test Acc:  59.736 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6777988942000117  Time:  1.4168407917022705\n",
      "Test Loss:  0.667444035410881\n",
      "Test Acc:  59.565999999999995 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6776067229518055  Time:  1.3952629566192627\n",
      "Test Loss:  0.6690901551319628\n",
      "Test Acc:  59.114 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6779465771069492  Time:  1.4203159809112549\n",
      "Test Loss:  0.6686269172600338\n",
      "Test Acc:  59.354 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6762589481625244  Time:  1.4089610576629639\n",
      "Test Loss:  0.6683010766092612\n",
      "Test Acc:  56.95400000000001 %\n",
      "================================================== 40\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=83, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=83, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=130, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=130, out_features=4, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=12, out_features=66, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=66, out_features=17, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=17, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6938601275430109  Time:  1.394284963607788\n",
      "Test Loss:  0.6931454259522107\n",
      "Test Acc:  50.09 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6879417239314448  Time:  1.3915348052978516\n",
      "Test Loss:  0.676419453353298\n",
      "Test Acc:  57.806000000000004 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6813451769578196  Time:  1.3850529193878174\n",
      "Test Loss:  0.6743323009233085\n",
      "Test Acc:  55.87 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6804555943847572  Time:  1.4723589420318604\n",
      "Test Loss:  0.6709438419463684\n",
      "Test Acc:  58.065999999999995 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6790676530260239  Time:  1.4711389541625977\n",
      "Test Loss:  0.6695594781515549\n",
      "Test Acc:  57.550000000000004 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6780222911034187  Time:  1.4590559005737305\n",
      "Test Loss:  0.668821418467833\n",
      "Test Acc:  57.687999999999995 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6779412483646922  Time:  1.4291019439697266\n",
      "Test Loss:  0.668323530530443\n",
      "Test Acc:  59.516000000000005 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6775867210252442  Time:  1.3984160423278809\n",
      "Test Loss:  0.6665780760195791\n",
      "Test Acc:  56.842000000000006 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6774604592445123  Time:  1.4037749767303467\n",
      "Test Loss:  0.6688189059495926\n",
      "Test Acc:  57.577999999999996 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6769505810998652  Time:  1.4342877864837646\n",
      "Test Loss:  0.6653142905965144\n",
      "Test Acc:  58.262 %\n",
      "================================================== 40\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=49, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=49, out_features=33, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=33, out_features=9, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=9, out_features=43, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=43, out_features=65, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=65, out_features=27, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=27, out_features=61, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=61, out_features=72, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=72, out_features=112, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=112, out_features=113, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=113, out_features=107, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=107, out_features=7, bias=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01)\n",
      "    (24): Linear(in_features=7, out_features=50, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=50, out_features=128, bias=True)\n",
      "    (27): LeakyReLU(negative_slope=0.01)\n",
      "    (28): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6933381161550536  Time:  2.003570079803467\n",
      "Test Loss:  0.6935712266333249\n",
      "Test Acc:  49.944 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6932775180270202  Time:  2.005579948425293\n",
      "Test Loss:  0.6932356853266152\n",
      "Test Acc:  50.056 %\n",
      "================================================== 40\n",
      "Train Loss:  0.693292268871391  Time:  2.1002249717712402\n",
      "Test Loss:  0.6932352826911576\n",
      "Test Acc:  49.944 %\n",
      "================================================== 40\n",
      "Train Loss:  0.693260252910809  Time:  2.0925068855285645\n",
      "Test Loss:  0.6931459030934742\n",
      "Test Acc:  50.056 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6932811504297883  Time:  2.0871002674102783\n",
      "Test Loss:  0.6931760061760338\n",
      "Test Acc:  50.056 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6932342728994189  Time:  2.098533868789673\n",
      "Test Loss:  0.6931522625441454\n",
      "Test Acc:  50.056 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6931950195427359  Time:  2.04431414604187\n",
      "Test Loss:  0.6931802055665425\n",
      "Test Acc:  49.944 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6932344734668732  Time:  2.047853946685791\n",
      "Test Loss:  0.6932768751772083\n",
      "Test Acc:  49.944 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6931882726885107  Time:  2.0074350833892822\n",
      "Test Loss:  0.6931666786573372\n",
      "Test Acc:  50.056 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6931828845591441  Time:  2.0162930488586426\n",
      "Test Loss:  0.6931554392284277\n",
      "Test Acc:  49.944 %\n",
      "================================================== 40\n",
      "results 0\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=83, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=83, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=56, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=56, out_features=131, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=131, out_features=52, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=52, out_features=25, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=25, out_features=96, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=96, out_features=61, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=61, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.69093239242143  Time:  1.470304012298584\n",
      "Test Loss:  0.6797157918312111\n",
      "Test Acc:  58.29600000000001 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6813566758684868  Time:  1.475250005722046\n",
      "Test Loss:  0.6701082246644157\n",
      "Test Acc:  58.089999999999996 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6796260584879966  Time:  1.4900951385498047\n",
      "Test Loss:  0.669181723679815\n",
      "Test Acc:  60.238 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6780026809577524  Time:  1.5098459720611572\n",
      "Test Loss:  0.6688685213424721\n",
      "Test Acc:  57.402 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6784873998513187  Time:  1.5203092098236084\n",
      "Test Loss:  0.6674249965925606\n",
      "Test Acc:  59.584 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6773262900592637  Time:  1.4687919616699219\n",
      "Test Loss:  0.6676657847603973\n",
      "Test Acc:  60.62 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6781380422358966  Time:  1.4675440788269043\n",
      "Test Loss:  0.6684727766075913\n",
      "Test Acc:  60.622 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6771438839661814  Time:  1.4752581119537354\n",
      "Test Loss:  0.666881628486575\n",
      "Test Acc:  58.972 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6763055657383299  Time:  1.432197093963623\n",
      "Test Loss:  0.6694202456547289\n",
      "Test Acc:  57.34 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6767577877009872  Time:  1.4783716201782227\n",
      "Test Loss:  0.6683012919158352\n",
      "Test Acc:  57.908 %\n",
      "================================================== 41\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=90, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=90, out_features=122, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=122, out_features=104, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=104, out_features=93, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=93, out_features=67, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=67, out_features=66, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=66, out_features=12, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=12, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6930280175522296  Time:  1.5057928562164307\n",
      "Test Loss:  0.6876616152573605\n",
      "Test Acc:  52.492000000000004 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6902423187764022  Time:  1.5050420761108398\n",
      "Test Loss:  0.6840467668917715\n",
      "Test Acc:  54.612 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6862772633559513  Time:  1.5190081596374512\n",
      "Test Loss:  0.6834586439084034\n",
      "Test Acc:  53.812000000000005 %\n",
      "================================================== 41\n",
      "Train Loss:  0.684692703894455  Time:  1.478100061416626\n",
      "Test Loss:  0.6834746060930953\n",
      "Test Acc:  55.078 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6847579535776681  Time:  1.4853160381317139\n",
      "Test Loss:  0.6832509357102063\n",
      "Test Acc:  53.976 %\n",
      "================================================== 41\n",
      "Train Loss:  0.685041599900183  Time:  1.5055458545684814\n",
      "Test Loss:  0.6812595500021564\n",
      "Test Acc:  54.428 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6848816467027594  Time:  1.4999101161956787\n",
      "Test Loss:  0.6827905105084789\n",
      "Test Acc:  54.269999999999996 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6837215849952977  Time:  1.48750901222229\n",
      "Test Loss:  0.6807847485250357\n",
      "Test Acc:  55.284 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6839396076915908  Time:  1.547910213470459\n",
      "Test Loss:  0.6796057485804265\n",
      "Test Acc:  55.55 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6829650815385971  Time:  1.542541742324829\n",
      "Test Loss:  0.6802111751570994\n",
      "Test Acc:  54.425999999999995 %\n",
      "================================================== 41\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=83, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=83, out_features=8, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=8, out_features=122, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=122, out_features=104, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=104, out_features=93, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=93, out_features=67, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=67, out_features=66, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=66, out_features=12, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=12, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6910030265359113  Time:  1.5138788223266602\n",
      "Test Loss:  0.6822041616756089\n",
      "Test Acc:  56.08800000000001 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6845953538470024  Time:  1.4664649963378906\n",
      "Test Loss:  0.6787065456108171\n",
      "Test Acc:  56.95400000000001 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6797369072472093  Time:  1.5304620265960693\n",
      "Test Loss:  0.6712067629001579\n",
      "Test Acc:  57.714 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6780457102904355  Time:  1.6296110153198242\n",
      "Test Loss:  0.6712427479880196\n",
      "Test Acc:  57.346 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6791537684680772  Time:  1.5554602146148682\n",
      "Test Loss:  0.6715058653938527\n",
      "Test Acc:  58.91 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6775508055286686  Time:  1.5427429676055908\n",
      "Test Loss:  0.6669415229437302\n",
      "Test Acc:  57.458 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6785695224347776  Time:  1.5849578380584717\n",
      "Test Loss:  0.6680200689909409\n",
      "Test Acc:  57.828 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6776065963463191  Time:  1.5904769897460938\n",
      "Test Loss:  0.6683312605838386\n",
      "Test Acc:  59.230000000000004 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6779285941245782  Time:  1.582672119140625\n",
      "Test Loss:  0.6703748219475454\n",
      "Test Acc:  57.230000000000004 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6783183889232413  Time:  1.584561824798584\n",
      "Test Loss:  0.6719461922742882\n",
      "Test Acc:  56.426 %\n",
      "================================================== 41\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=118, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=118, out_features=6, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=6, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6916977774052724  Time:  1.0240378379821777\n",
      "Test Loss:  0.6733928709005823\n",
      "Test Acc:  59.806000000000004 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6852849506548722  Time:  0.9926121234893799\n",
      "Test Loss:  0.6716649514071795\n",
      "Test Acc:  59.638000000000005 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6836700894101693  Time:  1.004652976989746\n",
      "Test Loss:  0.6713875550396589\n",
      "Test Acc:  59.53000000000001 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6825296091337274  Time:  2074.6566281318665\n",
      "Test Loss:  0.673080039875848\n",
      "Test Acc:  58.962 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6821864510539675  Time:  1.0954368114471436\n",
      "Test Loss:  0.6723635801855399\n",
      "Test Acc:  58.245999999999995 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6805659791413885  Time:  1.14485764503479\n",
      "Test Loss:  0.6688841432332993\n",
      "Test Acc:  59.492 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6809539555633155  Time:  1.1555001735687256\n",
      "Test Loss:  0.6692814182262031\n",
      "Test Acc:  58.114 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6806373443916767  Time:  1.1666350364685059\n",
      "Test Loss:  0.6707059716691777\n",
      "Test Acc:  58.394 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6802829654112349  Time:  1.1702289581298828\n",
      "Test Loss:  0.6697263294944957\n",
      "Test Acc:  58.974000000000004 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6795613554707409  Time:  1.127751111984253\n",
      "Test Loss:  0.6692475059202739\n",
      "Test Acc:  58.398 %\n",
      "================================================== 41\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=135, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=135, out_features=109, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=109, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6892672187655512  Time:  1.2026739120483398\n",
      "Test Loss:  0.6750697271556271\n",
      "Test Acc:  58.818000000000005 %\n",
      "================================================== 42\n",
      "Train Loss:  0.684400625254986  Time:  1.2227458953857422\n",
      "Test Loss:  0.6679803765549952\n",
      "Test Acc:  59.936 %\n",
      "================================================== 42\n",
      "Train Loss:  0.684043769201223  Time:  1.2183551788330078\n",
      "Test Loss:  0.667307931549695\n",
      "Test Acc:  60.616 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6834177237792607  Time:  1.1999192237854004\n",
      "Test Loss:  0.6708094578008262\n",
      "Test Acc:  59.0 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6832442353241635  Time:  1.2225141525268555\n",
      "Test Loss:  0.6710154800390711\n",
      "Test Acc:  59.374 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6809186152298085  Time:  1.2242300510406494\n",
      "Test Loss:  0.6687967284601561\n",
      "Test Acc:  59.156 %\n",
      "================================================== 42\n",
      "Train Loss:  0.679172103857472  Time:  1.21598219871521\n",
      "Test Loss:  0.6680466626979866\n",
      "Test Acc:  59.77 %\n",
      "================================================== 42\n",
      "Train Loss:  0.679543508665405  Time:  1.2196948528289795\n",
      "Test Loss:  0.6723185835444198\n",
      "Test Acc:  58.577999999999996 %\n",
      "================================================== 42\n",
      "Train Loss:  0.678821696417175  Time:  1.177563190460205\n",
      "Test Loss:  0.6731425219652604\n",
      "Test Acc:  57.996 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6791946015653819  Time:  1.1857409477233887\n",
      "Test Loss:  0.6713497827247698\n",
      "Test Acc:  58.464000000000006 %\n",
      "================================================== 42\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=118, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=118, out_features=6, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=6, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6886324836824932  Time:  1.0620322227478027\n",
      "Test Loss:  0.6736527237356925\n",
      "Test Acc:  59.662000000000006 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6842578347146946  Time:  1.0222468376159668\n",
      "Test Loss:  0.6719215980597905\n",
      "Test Acc:  59.230000000000004 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6836324279760793  Time:  1.0507080554962158\n",
      "Test Loss:  0.6724281809767898\n",
      "Test Acc:  58.97599999999999 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6821851797782592  Time:  1.0492618083953857\n",
      "Test Loss:  0.6723581990417169\n",
      "Test Acc:  59.34 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6820374598903377  Time:  1.0531201362609863\n",
      "Test Loss:  0.6718600790719597\n",
      "Test Acc:  58.589999999999996 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6815194322244964  Time:  1.0676653385162354\n",
      "Test Loss:  0.6696663158280509\n",
      "Test Acc:  58.752 %\n",
      "================================================== 42\n",
      "Train Loss:  0.680862225972823  Time:  1.0586130619049072\n",
      "Test Loss:  0.6717392811361624\n",
      "Test Acc:  58.192 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6796096310998402  Time:  1.0118818283081055\n",
      "Test Loss:  0.6708901147453152\n",
      "Test Acc:  58.45 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6799664062305089  Time:  1.005932092666626\n",
      "Test Loss:  0.66834878191656\n",
      "Test Acc:  58.78 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6796967511194466  Time:  1.0175740718841553\n",
      "Test Loss:  0.6687144390782531\n",
      "Test Acc:  58.675999999999995 %\n",
      "================================================== 42\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=118, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=118, out_features=6, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=6, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.689690418922118  Time:  0.9763779640197754\n",
      "Test Loss:  0.6754707840024209\n",
      "Test Acc:  58.94200000000001 %\n",
      "================================================== 42\n",
      "Train Loss:  0.684910837316165  Time:  1.0063459873199463\n",
      "Test Loss:  0.6718581668576415\n",
      "Test Acc:  59.404 %\n",
      "================================================== 42\n",
      "Train Loss:  0.684527133068029  Time:  1.0105299949645996\n",
      "Test Loss:  0.6705064271785774\n",
      "Test Acc:  59.714 %\n",
      "================================================== 42\n",
      "Train Loss:  0.682729883350595  Time:  1.0061781406402588\n",
      "Test Loss:  0.6680210257063106\n",
      "Test Acc:  60.175999999999995 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6817851608252004  Time:  5028.316485881805\n",
      "Test Loss:  0.6676208571511872\n",
      "Test Acc:  59.168 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6794925375576437  Time:  1.0811188220977783\n",
      "Test Loss:  0.6672741925838043\n",
      "Test Acc:  59.44199999999999 %\n",
      "================================================== 42\n",
      "Train Loss:  0.679670761754043  Time:  0.9931378364562988\n",
      "Test Loss:  0.6681775599718094\n",
      "Test Acc:  59.392 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6798565640066662  Time:  0.9740598201751709\n",
      "Test Loss:  0.6673232107138147\n",
      "Test Acc:  59.221999999999994 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6796040711176656  Time:  0.9393160343170166\n",
      "Test Loss:  0.6678849832744015\n",
      "Test Acc:  58.302 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6790827886901633  Time:  0.9760701656341553\n",
      "Test Loss:  0.6668121200435015\n",
      "Test Acc:  59.388 %\n",
      "================================================== 42\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=124, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=124, out_features=41, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=41, out_features=119, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=119, out_features=10, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=10, out_features=27, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=27, out_features=133, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=133, out_features=113, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=113, out_features=51, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=51, out_features=140, bias=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=140, out_features=92, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=92, out_features=92, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=92, out_features=104, bias=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=104, out_features=128, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Linear(in_features=128, out_features=78, bias=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=78, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Loss:  0.6932850419169795  Time:  3.4115829467773438\n",
      "Test Loss:  0.6931833323775506\n",
      "Test Acc:  50.056 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6932272982858393  Time:  2.8388969898223877\n",
      "Test Loss:  0.6932492523777242\n",
      "Test Acc:  49.944 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6932093511967763  Time:  2.343534231185913\n",
      "Test Loss:  0.6931323114098334\n",
      "Test Acc:  50.056 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6923708917885801  Time:  2.619731903076172\n",
      "Test Loss:  0.6891498191624271\n",
      "Test Acc:  52.038 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6876439887241725  Time:  2.4803311824798584\n",
      "Test Loss:  0.6734878889151982\n",
      "Test Acc:  58.544 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6825506049786171  Time:  2.4298601150512695\n",
      "Test Loss:  0.668109439161359\n",
      "Test Acc:  58.5 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6809115342415162  Time:  2.468550205230713\n",
      "Test Loss:  0.6712937248604638\n",
      "Test Acc:  57.508 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6801663249948598  Time:  2.36811900138855\n",
      "Test Loss:  0.6683735412602522\n",
      "Test Acc:  56.657999999999994 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6786732229873211  Time:  2.479428768157959\n",
      "Test Loss:  0.6663259249560687\n",
      "Test Acc:  60.866 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6777849449728527  Time:  2.477548122406006\n",
      "Test Loss:  0.6673274742705482\n",
      "Test Acc:  58.666 %\n",
      "================================================== 42\n",
      "results 2\n",
      "Simple_MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=56, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=126, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=126, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-25a736dfa42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m70000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidate_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b2fcf6e7d176>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print (outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_epoch = 10\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "#model2,optimizer2 = create_model()\n",
    "acc1 =0\n",
    "acc2 =0\n",
    "\n",
    "arx = create_ar(8)\n",
    "best , ar1,ar2 = next_gen(arx)\n",
    "loops = [best,ar1,ar2,arx]\n",
    "\n",
    "results =[]\n",
    "best_score =0\n",
    "for generations in range (99):\n",
    "    for index in range(4):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        model ,optimizer ,ar = create_model(loops[index])\n",
    "        temp_model = model\n",
    "        temp_optimizer = optimizer\n",
    "       \n",
    "        for i in range(n_epoch):\n",
    "            train_loader = create_set(70000,df_train)\n",
    "            train_loss = train_epoch(temp_model,train_loader,criterion,temp_optimizer)\n",
    "            test_loss,test_acc,pre = test_epoch(temp_model,validate_loader,criterion)\n",
    "\n",
    "            Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "            Test_acc.append(test_acc)\n",
    "            print('='*50,generations)\n",
    "        \n",
    "        if (test_acc > best_score):\n",
    "            print (\"updating model ======= \", test_acc)\n",
    "            best_model = temp_model\n",
    "            best_optimizer = temp_optimizer\n",
    "            best_score = test_acc\n",
    "        \n",
    "        results.append(test_acc)\n",
    "    if (results[0]> results[1] and results[0]> results[2] and results[0]> results[3]):\n",
    "        print('results 0')\n",
    "        best\n",
    "        a,b,best = next_gen(loops[0])\n",
    "    \n",
    "    if (results[1]> results[0] and results[1]> results[2] and results[1]> results[3]):\n",
    "        print('results 1')\n",
    "        a,b,best = next_gen(loops[1])\n",
    "    if (results[2]> results[0] and results[2]> results[1] and results[2]> results[3]):\n",
    "        print('results 2')\n",
    "        a,b,best = next_gen(loops[2])\n",
    "\n",
    "    if (results[3]> results[0] and results[3]> results[1] and results[3]> results[2]):\n",
    "        print('results 2')\n",
    "        a,b,best = next_gen(loops[3])\n",
    "\n",
    "\n",
    "    c=create_ar(8)\n",
    "    loops = [a,b,best,c]\n",
    "    results =[]\n",
    "        #print(pre)\n",
    "    #model2 ,optimizer2 = create_model()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.6708108797973517  Time:  1.1604208946228027\n",
      "Test Loss:  0.6688035106172368\n",
      "Test Acc:  58.209999999999994 %\n",
      "================================================== 0\n",
      "Train Loss:  0.6712623156455099  Time:  1.1579952239990234\n",
      "Test Loss:  0.6688442072089837\n",
      "Test Acc:  58.098000000000006 %\n",
      "================================================== 1\n",
      "Train Loss:  0.670500567981175  Time:  1.277318000793457\n",
      "Test Loss:  0.6689721738197365\n",
      "Test Acc:  57.943999999999996 %\n",
      "================================================== 2\n",
      "Train Loss:  0.6716652147623957  Time:  1.1493730545043945\n",
      "Test Loss:  0.6674726772673276\n",
      "Test Acc:  57.07 %\n",
      "================================================== 3\n",
      "Train Loss:  0.6710972326750658  Time:  1.2299470901489258\n",
      "Test Loss:  0.6689005332941912\n",
      "Test Acc:  57.154 %\n",
      "================================================== 4\n",
      "Train Loss:  0.669608107634953  Time:  1.2284560203552246\n",
      "Test Loss:  0.6677745328265794\n",
      "Test Acc:  57.498000000000005 %\n",
      "================================================== 5\n",
      "Train Loss:  0.6722897799039373  Time:  1.1907367706298828\n",
      "Test Loss:  0.6674901864358357\n",
      "Test Acc:  57.354000000000006 %\n",
      "================================================== 6\n",
      "Train Loss:  0.6716757991484233  Time:  1.2380101680755615\n",
      "Test Loss:  0.6678590418732896\n",
      "Test Acc:  58.098000000000006 %\n",
      "================================================== 7\n",
      "Train Loss:  0.6704054018672632  Time:  1.2049942016601562\n",
      "Test Loss:  0.6684851765024419\n",
      "Test Acc:  58.236 %\n",
      "================================================== 8\n",
      "Train Loss:  0.6711768872883855  Time:  1.2989978790283203\n",
      "Test Loss:  0.6660101891172175\n",
      "Test Acc:  57.769999999999996 %\n",
      "================================================== 9\n",
      "Train Loss:  0.6701148949107345  Time:  1.5179283618927002\n",
      "Test Loss:  0.6678040471612191\n",
      "Test Acc:  57.666 %\n",
      "================================================== 10\n",
      "Train Loss:  0.6703547275796229  Time:  1.341926097869873\n",
      "Test Loss:  0.6689212556396212\n",
      "Test Acc:  57.36599999999999 %\n",
      "================================================== 11\n",
      "Train Loss:  0.6713983480419431  Time:  1.4324290752410889\n",
      "Test Loss:  0.6694192338962944\n",
      "Test Acc:  57.58 %\n",
      "================================================== 12\n",
      "Train Loss:  0.6715949417985215  Time:  1.333376169204712\n",
      "Test Loss:  0.6672279551929358\n",
      "Test Acc:  58.806000000000004 %\n",
      "================================================== 13\n",
      "Train Loss:  0.6700443306139537  Time:  1.4105639457702637\n",
      "Test Loss:  0.6679111256891367\n",
      "Test Acc:  58.236 %\n",
      "================================================== 14\n",
      "Train Loss:  0.671514663465169  Time:  1.3856639862060547\n",
      "Test Loss:  0.6666929773530181\n",
      "Test Acc:  57.762 %\n",
      "================================================== 15\n",
      "Train Loss:  0.6681482685463769  Time:  1.3990089893341064\n",
      "Test Loss:  0.6701964824175348\n",
      "Test Acc:  57.855999999999995 %\n",
      "================================================== 16\n",
      "Train Loss:  0.6706355721974859  Time:  1.3626959323883057\n",
      "Test Loss:  0.6686244765106513\n",
      "Test Acc:  57.752 %\n",
      "================================================== 17\n",
      "Train Loss:  0.6701767730469607  Time:  1.420098066329956\n",
      "Test Loss:  0.6690403597087277\n",
      "Test Acc:  56.896 %\n",
      "================================================== 18\n",
      "Train Loss:  0.6703101706748106  Time:  1.4990549087524414\n",
      "Test Loss:  0.6711420383380384\n",
      "Test Acc:  57.623999999999995 %\n",
      "================================================== 19\n",
      "Train Loss:  0.6705085832853707  Time:  1.3369090557098389\n",
      "Test Loss:  0.6680386534758976\n",
      "Test Acc:  57.778 %\n",
      "================================================== 20\n",
      "Train Loss:  0.6723133371192582  Time:  1.3107728958129883\n",
      "Test Loss:  0.6710591389208423\n",
      "Test Acc:  56.922 %\n",
      "================================================== 21\n",
      "Train Loss:  0.6720931390110327  Time:  1.3853256702423096\n",
      "Test Loss:  0.6682957693630335\n",
      "Test Acc:  57.376000000000005 %\n",
      "================================================== 22\n",
      "Train Loss:  0.6705426360879626  Time:  1.3547489643096924\n",
      "Test Loss:  0.6688040646971488\n",
      "Test Acc:  57.572 %\n",
      "================================================== 23\n",
      "Train Loss:  0.6710827025223751  Time:  1.3782830238342285\n",
      "Test Loss:  0.6690108733517783\n",
      "Test Acc:  57.14399999999999 %\n",
      "================================================== 24\n",
      "Train Loss:  0.6705562368947633  Time:  1.3026351928710938\n",
      "Test Loss:  0.6711742516074862\n",
      "Test Acc:  57.891999999999996 %\n",
      "================================================== 25\n",
      "Train Loss:  0.6720217524134383  Time:  1.3309009075164795\n",
      "Test Loss:  0.6672986052474197\n",
      "Test Acc:  57.692 %\n",
      "================================================== 26\n",
      "Train Loss:  0.6708954283777548  Time:  1.3238089084625244\n",
      "Test Loss:  0.6674318045985942\n",
      "Test Acc:  57.85 %\n",
      "================================================== 27\n",
      "Train Loss:  0.67021010572813  Time:  1.32224702835083\n",
      "Test Loss:  0.6682728437744841\n",
      "Test Acc:  57.438 %\n",
      "================================================== 28\n",
      "Train Loss:  0.6702781009430788  Time:  1.3487529754638672\n",
      "Test Loss:  0.668782152691666\n",
      "Test Acc:  57.338 %\n",
      "================================================== 29\n",
      "Train Loss:  0.6699135048048837  Time:  1.3184709548950195\n",
      "Test Loss:  0.6681369564362934\n",
      "Test Acc:  58.065999999999995 %\n",
      "================================================== 30\n",
      "Train Loss:  0.6719416294779096  Time:  1.2809319496154785\n",
      "Test Loss:  0.6707833911083183\n",
      "Test Acc:  57.482 %\n",
      "================================================== 31\n",
      "Train Loss:  0.6701032488929982  Time:  1.3391451835632324\n",
      "Test Loss:  0.6701982076070747\n",
      "Test Acc:  58.489999999999995 %\n",
      "================================================== 32\n",
      "Train Loss:  0.6696992513476586  Time:  1.3721208572387695\n",
      "Test Loss:  0.6683277101541052\n",
      "Test Acc:  57.824 %\n",
      "================================================== 33\n",
      "Train Loss:  0.6695004187676371  Time:  1.2650511264801025\n",
      "Test Loss:  0.668114707177999\n",
      "Test Acc:  57.51199999999999 %\n",
      "================================================== 34\n",
      "Train Loss:  0.6703205573923734  Time:  1.2506000995635986\n",
      "Test Loss:  0.6684187249261506\n",
      "Test Acc:  57.616 %\n",
      "================================================== 35\n",
      "Train Loss:  0.6702966823869821  Time:  1.2877209186553955\n",
      "Test Loss:  0.6683334340246356\n",
      "Test Acc:  58.178 %\n",
      "================================================== 36\n",
      "Train Loss:  0.6695938405333733  Time:  1.4408230781555176\n",
      "Test Loss:  0.6685483559053771\n",
      "Test Acc:  58.001999999999995 %\n",
      "================================================== 37\n",
      "Train Loss:  0.6695566727798812  Time:  1.551048994064331\n",
      "Test Loss:  0.6695609393776679\n",
      "Test Acc:  57.762 %\n",
      "================================================== 38\n",
      "Train Loss:  0.6703944549876817  Time:  1.5874221324920654\n",
      "Test Loss:  0.6702585147351635\n",
      "Test Acc:  57.92 %\n",
      "================================================== 39\n",
      "Train Loss:  0.6708008932824038  Time:  1.5625848770141602\n",
      "Test Loss:  0.6680279444066846\n",
      "Test Acc:  58.104 %\n",
      "================================================== 40\n",
      "Train Loss:  0.6693586655417267  Time:  1.2993130683898926\n",
      "Test Loss:  0.6664510968388343\n",
      "Test Acc:  57.654 %\n",
      "================================================== 41\n",
      "Train Loss:  0.6697062080611988  Time:  1.2280550003051758\n",
      "Test Loss:  0.6692827976479823\n",
      "Test Acc:  57.57600000000001 %\n",
      "================================================== 42\n",
      "Train Loss:  0.6696530027048928  Time:  1.2817680835723877\n",
      "Test Loss:  0.6694421740818997\n",
      "Test Acc:  57.894 %\n",
      "================================================== 43\n",
      "Train Loss:  0.6700797917283311  Time:  1.279054880142212\n",
      "Test Loss:  0.6686195940995703\n",
      "Test Acc:  57.324 %\n",
      "================================================== 44\n",
      "Train Loss:  0.6706284284591675  Time:  1.268561840057373\n",
      "Test Loss:  0.670783265816922\n",
      "Test Acc:  57.214 %\n",
      "================================================== 45\n",
      "Train Loss:  0.6708201540976154  Time:  1.306251049041748\n",
      "Test Loss:  0.6678279875492563\n",
      "Test Acc:  57.66 %\n",
      "================================================== 46\n",
      "Train Loss:  0.6690368935161707  Time:  1.265308141708374\n",
      "Test Loss:  0.6685886443877707\n",
      "Test Acc:  58.056 %\n",
      "================================================== 47\n",
      "Train Loss:  0.6702593510248223  Time:  1.2287609577178955\n",
      "Test Loss:  0.6694656364163574\n",
      "Test Acc:  57.472 %\n",
      "================================================== 48\n",
      "Train Loss:  0.6686017023665565  Time:  1.2479419708251953\n",
      "Test Loss:  0.6669976562261581\n",
      "Test Acc:  58.012 %\n",
      "================================================== 49\n",
      "Train Loss:  0.6694612423984372  Time:  1.2853901386260986\n",
      "Test Loss:  0.6706059249688168\n",
      "Test Acc:  58.02 %\n",
      "================================================== 50\n",
      "Train Loss:  0.6704506916659219  Time:  1.266847848892212\n",
      "Test Loss:  0.6695663381596001\n",
      "Test Acc:  57.048 %\n",
      "================================================== 51\n",
      "Train Loss:  0.6702086873808686  Time:  1.3246948719024658\n",
      "Test Loss:  0.6707126632028696\n",
      "Test Acc:  57.902 %\n",
      "================================================== 52\n",
      "Train Loss:  0.6704998344791179  Time:  1.293379783630371\n",
      "Test Loss:  0.6711520765508924\n",
      "Test Acc:  57.312 %\n",
      "================================================== 53\n",
      "Train Loss:  0.669596720410853  Time:  1.2303800582885742\n",
      "Test Loss:  0.6693543578897204\n",
      "Test Acc:  57.63 %\n",
      "================================================== 54\n",
      "Train Loss:  0.6690239596123598  Time:  1.2164039611816406\n",
      "Test Loss:  0.6691233765105812\n",
      "Test Acc:  57.628 %\n",
      "================================================== 55\n",
      "Train Loss:  0.6691253930330276  Time:  1.2713737487792969\n",
      "Test Loss:  0.6685461870261601\n",
      "Test Acc:  57.611999999999995 %\n",
      "================================================== 56\n",
      "Train Loss:  0.6696709823243472  Time:  1.2816309928894043\n",
      "Test Loss:  0.6676579932777249\n",
      "Test Acc:  57.89 %\n",
      "================================================== 57\n",
      "Train Loss:  0.6685915148379852  Time:  1.2776660919189453\n",
      "Test Loss:  0.6697845176166418\n",
      "Test Acc:  57.467999999999996 %\n",
      "================================================== 58\n",
      "Train Loss:  0.6680902266988948  Time:  1.24161696434021\n",
      "Test Loss:  0.6677357502737824\n",
      "Test Acc:  58.06400000000001 %\n",
      "================================================== 59\n",
      "Train Loss:  0.6687615428652082  Time:  1.3137993812561035\n",
      "Test Loss:  0.6719226688146591\n",
      "Test Acc:  57.544 %\n",
      "================================================== 60\n",
      "Train Loss:  0.6687984235432683  Time:  1.2328290939331055\n",
      "Test Loss:  0.6683322404112134\n",
      "Test Acc:  57.846 %\n",
      "================================================== 61\n",
      "Train Loss:  0.6702345837743915  Time:  1.4511399269104004\n",
      "Test Loss:  0.6706226802602107\n",
      "Test Acc:  57.668 %\n",
      "================================================== 62\n",
      "Train Loss:  0.6699560415379855  Time:  1.4069900512695312\n",
      "Test Loss:  0.6689897501955226\n",
      "Test Acc:  58.008 %\n",
      "================================================== 63\n",
      "Train Loss:  0.669329014967899  Time:  1.3574321269989014\n",
      "Test Loss:  0.667758930094388\n",
      "Test Acc:  58.168 %\n",
      "================================================== 64\n",
      "Train Loss:  0.6694540162475742  Time:  1.2939667701721191\n",
      "Test Loss:  0.6707761293771316\n",
      "Test Acc:  57.718 %\n",
      "================================================== 65\n",
      "Train Loss:  0.671679395802167  Time:  1.2377631664276123\n",
      "Test Loss:  0.6703849343621001\n",
      "Test Acc:  58.012 %\n",
      "================================================== 66\n",
      "Train Loss:  0.6679821324591734  Time:  1.3187410831451416\n",
      "Test Loss:  0.6697972568930411\n",
      "Test Acc:  57.524 %\n",
      "================================================== 67\n",
      "Train Loss:  0.6692206041545284  Time:  1.2716989517211914\n",
      "Test Loss:  0.6655017605849675\n",
      "Test Acc:  58.236 %\n",
      "================================================== 68\n",
      "Train Loss:  0.668753443323836  Time:  1.2391798496246338\n",
      "Test Loss:  0.669757106778573\n",
      "Test Acc:  58.15 %\n",
      "================================================== 69\n",
      "Train Loss:  0.667889477951186  Time:  1.3067350387573242\n",
      "Test Loss:  0.6687568894454411\n",
      "Test Acc:  57.98799999999999 %\n",
      "================================================== 70\n",
      "Train Loss:  0.6682409920862743  Time:  1.2088382244110107\n",
      "Test Loss:  0.6694043467239458\n",
      "Test Acc:  58.238 %\n",
      "================================================== 71\n",
      "Train Loss:  0.6694625150792453  Time:  1.2402002811431885\n",
      "Test Loss:  0.6684824042782491\n",
      "Test Acc:  58.3 %\n",
      "================================================== 72\n",
      "Train Loss:  0.6687596920801668  Time:  1.284933090209961\n",
      "Test Loss:  0.6702704435708572\n",
      "Test Acc:  57.56 %\n",
      "================================================== 73\n",
      "Train Loss:  0.6694059843311504  Time:  1.2764501571655273\n",
      "Test Loss:  0.6683431982385869\n",
      "Test Acc:  57.962 %\n",
      "================================================== 74\n",
      "Train Loss:  0.6684791305843665  Time:  1.2599399089813232\n",
      "Test Loss:  0.6696470249064115\n",
      "Test Acc:  57.333999999999996 %\n",
      "================================================== 75\n",
      "Train Loss:  0.6686542143627089  Time:  1.210927963256836\n",
      "Test Loss:  0.6672140262564834\n",
      "Test Acc:  57.89 %\n",
      "================================================== 76\n",
      "Train Loss:  0.6695253225613613  Time:  1.273029088973999\n",
      "Test Loss:  0.6709343441286866\n",
      "Test Acc:  57.762 %\n",
      "================================================== 77\n",
      "Train Loss:  0.6677517918299656  Time:  1.1961829662322998\n",
      "Test Loss:  0.6707067094287094\n",
      "Test Acc:  57.584 %\n",
      "================================================== 78\n",
      "Train Loss:  0.6707812416918424  Time:  1.317533254623413\n",
      "Test Loss:  0.6705103422306022\n",
      "Test Acc:  57.028 %\n",
      "================================================== 79\n",
      "Train Loss:  0.6683364182102437  Time:  1.3432848453521729\n",
      "Test Loss:  0.6691993815558297\n",
      "Test Acc:  57.44200000000001 %\n",
      "================================================== 80\n",
      "Train Loss:  0.6677041455191008  Time:  1.2227840423583984\n",
      "Test Loss:  0.6695462325397803\n",
      "Test Acc:  57.498000000000005 %\n",
      "================================================== 81\n",
      "Train Loss:  0.6712587390627179  Time:  1.2451848983764648\n",
      "Test Loss:  0.6708467496292931\n",
      "Test Acc:  57.42 %\n",
      "================================================== 82\n",
      "Train Loss:  0.6683001110748369  Time:  1.268996000289917\n",
      "Test Loss:  0.6689019598522965\n",
      "Test Acc:  57.336 %\n",
      "================================================== 83\n",
      "Train Loss:  0.6691474312422226  Time:  1.273021936416626\n",
      "Test Loss:  0.6701537769059746\n",
      "Test Acc:  57.023999999999994 %\n",
      "================================================== 84\n",
      "Train Loss:  0.6683368646368688  Time:  1.2653391361236572\n",
      "Test Loss:  0.6705648409468787\n",
      "Test Acc:  57.846 %\n",
      "================================================== 85\n",
      "Train Loss:  0.6679181991791239  Time:  1.2370340824127197\n",
      "Test Loss:  0.6694044972560844\n",
      "Test Acc:  57.598000000000006 %\n",
      "================================================== 86\n",
      "Train Loss:  0.6688850683217146  Time:  1.34586763381958\n",
      "Test Loss:  0.6715592462189344\n",
      "Test Acc:  57.599999999999994 %\n",
      "================================================== 87\n",
      "Train Loss:  0.6690037980371591  Time:  1.2399790287017822\n",
      "Test Loss:  0.6695214443060816\n",
      "Test Acc:  57.294 %\n",
      "================================================== 88\n",
      "Train Loss:  0.6689757954101173  Time:  1.2272100448608398\n",
      "Test Loss:  0.6696184934401999\n",
      "Test Acc:  57.14 %\n",
      "================================================== 89\n",
      "Train Loss:  0.6706699391408842  Time:  1.2972207069396973\n",
      "Test Loss:  0.6691971837866063\n",
      "Test Acc:  56.984 %\n",
      "================================================== 90\n",
      "Train Loss:  0.6685925779902205  Time:  1.282790184020996\n",
      "Test Loss:  0.6688172643890187\n",
      "Test Acc:  58.077999999999996 %\n",
      "================================================== 91\n",
      "Train Loss:  0.669860947497037  Time:  1.2074859142303467\n",
      "Test Loss:  0.6694874316453934\n",
      "Test Acc:  57.752 %\n",
      "================================================== 92\n",
      "Train Loss:  0.6680217449154172  Time:  1.2265973091125488\n",
      "Test Loss:  0.6684445276552317\n",
      "Test Acc:  57.568 %\n",
      "================================================== 93\n",
      "Train Loss:  0.6680925372911959  Time:  1.3482239246368408\n",
      "Test Loss:  0.6686055435209858\n",
      "Test Acc:  57.44200000000001 %\n",
      "================================================== 94\n",
      "Train Loss:  0.6665051713281748  Time:  1.2556180953979492\n",
      "Test Loss:  0.6677750960296515\n",
      "Test Acc:  57.76 %\n",
      "================================================== 95\n",
      "Train Loss:  0.6692222399371011  Time:  1.2132210731506348\n",
      "Test Loss:  0.6714614830455001\n",
      "Test Acc:  57.38799999999999 %\n",
      "================================================== 96\n",
      "Train Loss:  0.6693221607378551  Time:  1.2702977657318115\n",
      "Test Loss:  0.6697340118033546\n",
      "Test Acc:  57.614 %\n",
      "================================================== 97\n",
      "Train Loss:  0.6682362608155425  Time:  1.2681941986083984\n",
      "Test Loss:  0.6693031502013304\n",
      "Test Acc:  57.486000000000004 %\n",
      "================================================== 98\n"
     ]
    }
   ],
   "source": [
    "for i in range(99):\n",
    "            train_loader = create_set(50000,df_train)\n",
    "            train_loss = train_epoch(best_model,train_loader,criterion,best_optimizer)\n",
    "            test_loss,test_acc,pre = test_epoch(best_model,validate_loader,criterion)\n",
    "\n",
    "            \n",
    "            Train_loss.append(train_loss)\n",
    "            Test_loss.append(test_loss)\n",
    "            Test_acc.append(test_acc)\n",
    "            print('='*50,i)\n",
    "predicted_res = submit_epoch(model,test_data)\n",
    "\n",
    "df = pd.DataFrame(predicted_res)\n",
    "df.to_csv('/Users/bebik/Downloads/data/033363870_10.txt',index=False,header=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(predicted_res)\n",
    "df.to_csv('/Users/bebik/Downloads/data/033363870_11.txt',index=False,header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f94057d5d50>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss/Criterion')\n",
    "plt.plot(Train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f93783a0910>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Loss/Criterion')\n",
    "plt.plot(Test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f93f082fb10>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(Test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
